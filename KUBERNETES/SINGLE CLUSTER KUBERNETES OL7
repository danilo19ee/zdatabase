+++++++++++++++++++++++++++++++++
+ SINGLE CLUSTER KUBERNETES OL7 +
+++++++++++++++++++++++++++++++++

#KUBERNETES IS AN OPEN SOURCE PRODUCT USED TO AUTOMATE THE IMPLEMENTATION, DIMENSIONING AND MANAGEMENT OF CONTAINER APPLICATIONS

CHECK KERNEL VERSION (THE ORACLE LINUX 7 RUN WITH KERNEL VERSION 4.*)
uname -a
Linux localhost.localdomain 4.14.35-1902.9.2.el7uek.x86_64 #2 SMP Mon Dec 23 13:39:16 PST 2019 x86_64 x86_64 x86_64 GNU/Linux

#DISABLE SELINUX AND STOP FIREWALL IN ALL NODES 
sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinux && setenforce 0
systemctl stop firewalld && systemctl disable firewalld

#ADD REPOSITORY ol7_addons
vi /etc/yum.repos.d/oracle-linux-ol7.repo
[ol7_addons]
name=Oracle Linux $releasever Add ons ($basearch)
baseurl=https://yum$ociregion.oracle.com/repo/OracleLinux/OL7/addons/$basearch/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
gpgcheck=1
enabled=1

#LIST REPOSITORIES
yum repolist
Loaded plugins: langpacks, ulninfo
ol7_UEKR5                                                                                                                                                                                                                                                | 2.5 kB  00:00:00     
ol7_addons                                                                                                                                                                                                                                               | 2.5 kB  00:00:00     
ol7_latest                                                                                                                                                                                                                                               | 2.7 kB  00:00:00     
(1/6): ol7_UEKR5/x86_64/updateinfo                                                                                                                                                                                                                       |  28 kB  00:00:00     
(2/6): ol7_addons/x86_64/updateinfo                                                                                                                                                                                                                      |  76 kB  00:00:00     
(3/6): ol7_addons/x86_64/primary_db                                                                                                                                                                                                                      | 147 kB  00:00:00     
(4/6): ol7_UEKR5/x86_64/primary_db                                                                                                                                                                                                                       |  10 MB  00:00:00     
(5/6): ol7_latest/x86_64/updateinfo                                                                                                                                                                                                                      | 2.6 MB  00:00:00     
(6/6): ol7_latest/x86_64/primary_db                                                                                                                                                                                                                      |  26 MB  00:00:01     
repo id                                              repo name                                                                                                                                                            status
ol7_UEKR5/x86_64                                     Latest Unbreakable Enterprise Kernel Release 5 for Oracle Linux 7Server (x86_64)                                                                                        221
ol7_addons/x86_64                                    Oracle Linux 7Server Add ons (x86_64)                                                                                                                                   395
ol7_latest/x86_64                                    Oracle Linux 7Server Latest (x86_64)                                                                                                                                 16,108
repolist: 16,724

#CHANGE HOSTNAME
#FOR SLAVE MACHINES, SET THE HOSTNAME
hostnamectl 
hostnamectl set-hostname KUBERMASTER
hostnamectl --static 

vi /etc/hosts
192.168.1.116  KUBERMASTER
192.168.1.117  KUBERWORKER

#ADD DISK TO CREATE STORAGE AREA FOR DOCKER ENGINE
fdisk -l
Disk /dev/sdb: 107.4 GB, 107374182400 bytes, 209715200 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

#CREATE BTRFS PARTITION FOR DOCKER
mkdir -p /var/lib/docker

parted /dev/sdb -s -- mklabel gpt mkpart DOCKER 1 -1

mkfs.btrfs -L var-lib-docker /dev/sdb1
btrfs-progs v4.9.1
See http://btrfs.wiki.kernel.org for more information.

Label:              var-lib-docker
UUID:               e35d7f7e-b8f2-474d-8ba0-8a75c5c3d30b
Node size:          16384
Sector size:        4096
Filesystem size:    100.00GiB
Block group profiles:
  Data:             single            8.00MiB
  Metadata:         DUP               1.00GiB
  System:           DUP               8.00MiB
SSD detected:       no
Incompat features:  extref
Number of devices:  1
Devices:
   ID        SIZE  PATH
    1   100.00GiB  /dev/sdb1

#MOUNT PARTITION DOCKER
echo "LABEL=var-lib-docker /var/lib/docker btrfs defaults 0 0" >> /etc/fstab
mount /var/lib/docker
df -h
Used Avail Use% Mounted on
devtmpfs        2.0G     0  2.0G   0% /dev
tmpfs           2.0G     0  2.0G   0% /dev/shm
tmpfs           2.0G  9.4M  2.0G   1% /run
tmpfs           2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/sda2        31G  7.2G   22G  25% /
/dev/sda1       347M  284M   64M  82% /boot
tmpfs           394M   20K  394M   1% /run/user/0
/dev/sdb1       100G   17M   98G   1% /var/lib/docker

#INSTALL AND CONFIGURE NTP
yum -y install ntp

systemctl enable ntpd

ntpdate a.ntp.br
 3 Feb 11:42:15 ntpdate[3249]: step time server 200.160.0.8 offset 74.420798 sec

vi /etc/ntp.conf 
server a.ntp.br
server b.ntp.br
server c.ntp.br

systemctl start ntpd

systemctl status ntpd
‚óè ntpd.service - Network Time Service
   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled)
   Active: active (running) since Mon 2020-02-03 11:47:35 -03; 4s ago

#INSTALL DOCKER ENGINE
yum -y install docker-engine
systemctl enable docker
systemctl start docker

## KUBERNETES MASTER ##
#ON KUBERNETES MASTER SERVER
yum -y install kubeadm kubelet kubectl

#LOG IN ORACLE CONTAINER REGISTRY
#IT IS NECESSARY TO HAVE AN ORACLE LOGIN
FIRST OPEN ORACLE CONTAINER REGISTRY ADDRESS IN A BROWSER, LOGIN, GO TO THE "CONTAINER SERVICES" OPTION AND SING IN TO ACCEPT THE TERMS OF USE
https://container-registry.oracle.com/

docker login container-registry.oracle.com/kubernetes
Username: <email> 
Password: <password>
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded

### CONFIGURE MASTER SERVER ####
kubeadm-setup.sh up --pod-network-cidr 10.0.0.0/16
Checking kubelet and kubectl RPM ...
Starting to initialize master node ...
Checking if env is ready ...
Checking whether docker can pull busybox image ...
Checking access to container-registry.oracle.com/kubernetes ...
Trying to pull repository container-registry.oracle.com/kubernetes/kube-proxy ...
v1.12.10: Pulling from container-registry.oracle.com/kubernetes/kube-proxy
Digest: sha256:2340f6d185aaec709f46a878b2b0003615de95c87809fb224eb53818ad5ddda6
Status: Image is up to date for container-registry.oracle.com/kubernetes/kube-proxy:v1.12.10
container-registry.oracle.com/kubernetes/kube-proxy:v1.12.10
Checking whether docker can run container ...
Checking iptables default rule ...
Checking br_netfilter module ...
Checking sysctl variables ...
Enabling kubelet ...
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/                                                                                                                                     kubelet.service.
Check successful, ready to run 'up' command ...
Waiting for kubeadm to setup master cluster...
Please wait ...
| - 80% completed
Waiting for the control plane to become ready ...
..............
100% completed
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds created
Patching CVE ...

Installing kubernetes-dashboard ...

Enabling kubectl-proxy.service ...
Starting kubectl-proxy.service ...
deployment.extensions/coredns patched

[===> PLEASE DO THE FOLLOWING STEPS BELOW: <===]

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config


You can now join any number of machines by running the following on each node
as root:

  kubeadm-setup.sh join 192.168.1.116:6443 --token 06ewe6.5b8l9qorhwzwbmhn --discovery-token-ca-cert-hash sha256:3f786c0d6c581232c65d6c4d7834ce3f123a2493c2cce1ef3d07f82e6e446637                                                                                                                                     h sha256:3f786c0d6c581232c65d6c4d7834ce3f123a2493c2cce1ef3d07f82e6e446637

#COPY USER ENVORIMENTS ROOT FILES TO EXECUTE COMMANDS ON KUBERNETES
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
export KUBECONFIG=$HOME/.kube/config
echo 'export KUBECONFIG=$HOME/.kube/config' >> $HOME/.bashrc

#VERIFY SERVICE MANAGER 

kubectl get pods -n kube-system
NAME                                   READY   STATUS     RESTARTS   AGE
coredns-6d5cc884f4-hqdl8               1/1     Running    0          5m12s
coredns-6d5cc884f4-z2pfg               1/1     Running    0          2m16s
etcd-kubemaster                        1/1     Running    0          5m12s
kube-apiserver-kubemaster              1/1     Running    2          105s
kube-controller-manager-kubemaster     1/1     Running    1          104s
kube-flannel-ds-gq9r4                  0/1     Init:0/1   0          32s
kube-flannel-ds-lnfgr                  1/1     Running    2          5m13s
kube-proxy-k4c8f                       1/1     Running    0          32s
kube-proxy-mvmr8                       1/1     Running    0          5m16s
kube-scheduler-kubemaster              1/1     Running    1          108s
kubernetes-dashboard-f6b58ff9c-fbxql   1/1     Running    0          5m13s


### CONFIGURE WORKER SERVER ###

#DISABLE SELINUX AND STOP FIREWALL IN ALL NODES 
sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinux && setenforce 0
systemctl stop firewalld && systemctl disable firewalld

#ADD REPOSITORY ol7_addons
vi /etc/yum.repos.d/oracle-linux-ol7.repo
[ol7_addons]
name=Oracle Linux $releasever Add ons ($basearch)
baseurl=https://yum$ociregion.oracle.com/repo/OracleLinux/OL7/addons/$basearch/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
gpgcheck=1
enabled=1

#CHANGE HOSTNAME
#FOR SLAVE MACHINES, SET THE HOSTNAME
hostnamectl 
hostnamectl set-hostname KUBERWORKER
hostnamectl --static 

vi /etc/hosts
192.168.1.116  KUBERMASTER
192.168.1.117  KUBERWORKER

#INSTALL AND CONFIGURE NTP
yum -y install ntp

systemctl enable ntpd

ntpdate a.ntp.br
 3 Feb 11:42:15 ntpdate[3249]: step time server 200.160.0.8 offset 74.420798 sec

vi /etc/ntp.conf 
server a.ntp.br
server b.ntp.br
server c.ntp.br

systemctl start ntpd

systemctl status ntpd
‚óè ntpd.service - Network Time Service
   Loaded: loaded (/usr/lib/systemd/system/ntpd.service; enabled; vendor preset: disabled)
   Active: active (running) since Mon 2020-02-03 11:47:35 -03; 4s ago

#INSTALL KUBERNETS PACKAGES ON WORKER
yum -y install kubeadm kubelet kubectl

#INSTALL DOCKER ENGINE
yum -y install docker-engine
systemctl enable docker
systemctl start docker

#LOG IN ORACLE CONTAINER REGISTRY
docker login container-registry.oracle.com/kubernetes
Username: <email>
Password: <password>
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded

#JOIN TO CLUSTER KUBERNETES
#USE TOKEN HASH TO SUBSCRIBE IN CLUSTER MASTER
 kubeadm-setup.sh join 192.168.1.116:6443 --token 06ewe6.5b8l9qorhwzwbmhn --discovery-token-ca-cert-hash sha256:3f786c0d6c581232c65d6c4d7834ce3f123a2493c2cce1ef3d07f82e6e446637                                                                                                                                       covery-token-ca-cert-hash sha256:3f786c0d6c581232c65d6c4d7834ce3f123a2493c2cce1ef3d07f82e6e446637
Checking kubelet and kubectl RPM ...
Starting to initialize worker node ...
Checking if env is ready ...
Checking whether docker can pull busybox image ...
Checking access to container-registry.oracle.com/kubernetes ...
Trying to pull repository container-registry.oracle.com/kubernetes/kube-proxy ...
v1.12.10: Pulling from container-registry.oracle.com/kubernetes/kube-proxy
Digest: sha256:2340f6d185aaec709f46a878b2b0003615de95c87809fb224eb53818ad5ddda6
Status: Image is up to date for container-registry.oracle.com/kubernetes/kube-proxy:v1.12.10
container-registry.oracle.com/kubernetes/kube-proxy:v1.12.10
Checking whether docker can run container ...
Checking iptables default rule ...
Checking br_netfilter module ...
Checking sysctl variables ...
Enabling kubelet ...
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/sys                                                                                                                                         tem/kubelet.service.
Check successful, ready to run 'join' command ...
[validation] WARNING: kubeadm doesn't fully support multiple API Servers yet
[preflight] running pre-flight checks
        [WARNING RequiredIPVSKernelModulesAvailable]: the IPVS proxier will not be used, because the                                                                                                                                          following required kernel modules are not loaded: [ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh] or no builtin                                                                                                                                          kernel ipvs support: map[ip_vs:{} ip_vs_rr:{} ip_vs_sh:{} ip_vs_wrr:{} nf_conntrack_ipv4:{}]
you can solve this problem with following methods:
 1. Run 'modprobe -- ' to load missing kernel modules;
2. Provide the missing builtin kernel ipvs support

        [WARNING Swap]: running with swap on is not supported. Please disable swap
        [WARNING SystemVerification]: this Docker version is not on the list of validated versions:                                                                                                                                          19.03.1-ol. Latest validated version: 18.06
[discovery] Trying to connect to API Server "192.168.1.116:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.1.116:6443"
[discovery] Trying to connect to API Server "192.168.1.116:6443"
[discovery] Created cluster-info discovery client, requesting info from "https://192.168.1.116:6443"
[discovery] Requesting info from "https://192.168.1.116:6443" again to validate TLS against the pinn                                                                                                                                         ed public key
[discovery] Requesting info from "https://192.168.1.116:6443" again to validate TLS against the pinn                                                                                                                                         ed public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinn                                                                                                                                         ed roots, will use API Server "192.168.1.116:6443"
[discovery] Successfully established connection with API Server "192.168.1.116:6443"
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinn                                                                                                                                         ed roots, will use API Server "192.168.1.116:6443"
[discovery] Successfully established connection with API Server "192.168.1.116:6443"
[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.12" ConfigMap in the                                                                                                                                          kube-system namespace
[kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[preflight] Activating the kubelet service
[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "                                                                                                                                         kuberworker" as an annotation

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.

## KUBERNETES MASTER ##
#VERIFY KUBERNETES NODES 
kubectl get nodes
NAME          STATUS   ROLES    AGE     VERSION
kubermaster   Ready    master   48m     v1.12.10+1.0.10.el7
kuberworker   Ready    <none>   3m57s   v1.12.10+1.0.10.el7

kubectl get pods -n kube-system
NAME                                   READY   STATUS    RESTARTS   AGE
coredns-6d5cc884f4-xbqr8               1/1     Running   0          50m
coredns-6d5cc884f4-z4kmq               1/1     Running   0          47m
etcd-kubermaster                       1/1     Running   0          50m
kube-apiserver-kubermaster             1/1     Running   1          46m
kube-controller-manager-kubermaster    1/1     Running   1          47m
kube-flannel-ds-8t6hj                  1/1     Running   2          50m
kube-flannel-ds-lsjfn                  1/1     Running   0          6m19s
kube-proxy-5djhr                       1/1     Running   0          6m19s
kube-proxy-q2vjc                       1/1     Running   0          50m
kube-scheduler-kubermaster             1/1     Running   1          47m
kubernetes-dashboard-f6b58ff9c-r7vmz   1/1     Running   0          50m

#TOKEN LIST
kubeadm token list
TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS
7kd319.bm6eo2n3w8ca8wfo   23h       2020-04-27T18:56:04-03:00   authentication,signing   <none>        system:bootstrappers:kubeadm:default-node-token

#NOTE
By default, tokens expire after 24 hours. 
If you are joining a node to the cluster after the current token has expired, 
you can create a new token by running the following command on the master node:

kubeadm token create
I0426 19:05:08.563508    5600 version.go:236] remote version is much newer: v1.18.2; falling back to: stable-1.12
l3xh3w.r5aitpw45gey52pl

#LIST NODES
kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
kubemaster   Ready    master   10m     v1.12.10+1.0.11.el7
kubework     Ready    <none>   5m17s   v1.12.10+1.0.11.el7

#YOUR CLUSTER IS UP AND RUNNING 
#SEE THAT SOME CLOUD NATIVE SERVICE ARE ALREADY DELIVERED BY DEFAULT BY ORACLE (CoreDNS, Flannel)

#STOP KUBERNETS 
#RUN ON MASTER AND WORKER

kubeadm-setup.sh stop
Stopping kubelet now ...
Stopping containers now ...

#RUN BACKUP SINGLE MASTER KLUSTER

kubeadm-setup.sh backup /backups
Creating backup at directory /backup ...
Using container-registry.oracle.com/kubernetes/etcd:3.2.24-1
Checking if container-registry.oracle.com/kubernetes/etcd:3.2.24-1 is available
Trying to pull repository container-registry.oracle.com/kubernetes/etcd ...
3.2.24-1: Pulling from container-registry.oracle.com/kubernetes/etcd
Digest: sha256:6d802fffc0bbf18d53e718eb860cac9a957b99abd710d9a80634b491b3786516
Status: Image is up to date for container-registry.oracle.com/kubernetes/etcd:3.2.24-1
container-registry.oracle.com/kubernetes/etcd:3.2.24-1
f9b3323562e5236d5963e83a4e76ecb735697d4e1cfb901ca6744d1b0e540c24  /var/run/kubeadm/backup/etcd-backup-1587939009.tar
7a88d4f7fb10eeac506f8b77dba6b9a0d25a0bbdc4934491c48f6cfe26f45966  /var/run/kubeadm/backup/k8s-master-0-1587939009.tar
Backup is successfully stored at /backup/master-backup-v1.12.10-1-0-1587939009.tar ...
You can restart your cluster now by doing:
# kubeadm-setup.sh restart

ls -ltrh /backup/
-rw-r--r-- 1 root root 78M Apr 26 19:10 master-backup-v1.12.10-1-0-1587939009.tar

#RESTORE THE CLUSTER CONFIGURATION AND STATE

yum install kubeadm kubectl kubelet

kubeadm-setup.sh restore /backup/master-backup-v1.12.10-1-0-1587939009.tar

kubeadm-setup.sh restart

sudo cp /etc/kubernetes/admin.conf $HOME/ 
$ sudo chown $(id -u):$(id -g) $HOME/admin.conf
$ export KUBECONFIG=$HOME/admin.conf
echo 'export KUBECONFIG=$HOME/admin.conf' >> $HOME/.bashrc

kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
kubemaster   Ready    master   10m     v1.12.10+1.0.11.el7
kubework     Ready    <none>   5m17s   v1.12.10+1.0.11.el7

#REMOVE WORKER NODE FROM THE CLUSTER
#ON WORKER
kubeadm-setup.sh down
Stopping kubelet now ...
Stopping containers now ...
[root@kubework ~]# kubeadm-setup.sh down
[WARNING] This action will RESET this node !!!!
          Since this is a worker node, please also run the following on the master (if not already done)
          # kubectl delete node kubework
          Please select 1 (continue) or 2 (abort) :
1) continue
2) abort
#? 1
#? 1
[reset] WARNING: changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] are you sure you want to proceed? [y/N]: [preflight] running pre-flight checks
[reset] stopping the kubelet service
[reset] unmounting mounted directories in "/var/lib/kubelet"
[reset] no etcd manifest found in "/etc/kubernetes/manifests/etcd.yaml". Assuming external etcd
[reset] please manually reset etcd to prevent further issues
[reset] deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]
[reset] deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
deleting flannel.1 ip link ...
removing /etc/kubernetes directory ...

#ON MASTER
kubectl delete node kubework
node "kubework" deleted

#RUN APPLICATION IN A POD
kubectl create deployment --image nginx hello-world
deployment.apps/hello-world created
kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
hello-world-5f55779987-kh5dr   0/1     Pending   0          17s

kubectl describe pods
Name:               hello-world-5f55779987-kh5dr
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               <none>
Labels:             app=hello-world
                    pod-template-hash=5f55779987
Annotations:        <none>
Status:             Pending
IP:
Controlled By:      ReplicaSet/hello-world-5f55779987
Containers:
  nginx:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ljsq4 (ro)
Conditions:
  Type           Status
  PodScheduled   False
Volumes:
  default-token-ljsq4:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-ljsq4
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            Age                From               Message
  ----     ------            ----               ----               -------
  Warning  FailedScheduling  52s (x2 over 52s)  default-scheduler  0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate.

#SCALE POD DEPLOYMENT
kubectl scale deployment --replicas=2 hello-world
deployment.extensions/hello-world scaled

#LIST PODS
kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
hello-world-5f55779987-jpxjw   0/1     Pending   0          23s
hello-world-5f55779987-kh5dr   0/1     Pending   0          96s

#EXPOSE A SERVICE OBJECT FOR YOUR APPLICATION

kubectl expose deployment hello-world --port 80 --type=LoadBalancer
service/hello-world exposed

kubectl get services
NAME          TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
hello-world   LoadBalancer   10.99.39.243   <pending>     80:30799/TCP   4s
kubernetes    ClusterIP      10.96.0.1      <none>        443/TCP        28m

#DELETE SERVICE
kubectl delete services hello-world
service "hello-world" deleted

#DELETE DEPLOYMENT
kubectl delete deployment hello-world
deployment.extensions "hello-world" deleted

#PROXY ACCESS
#START PROXY
kubectl proxy
systemctl enable --now kubectl-proxy

#GET TOKEN
kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token:
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi03NGN3ZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjdiMThlMjdhLThlZWQtMTFlYS04MzU1LTA4MDAyNzVkYmU5ZiIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.YIuicBpWi65TXpdHM67oSVRd_qN951E-wMrJSSrMl3eiscl4_5L5oGbaoD_IAZe7HhivAlRZhPI5GfpA-l7_M1vVEJLi_3fepoHdqchk0e0l3Gotc8sboEDNso9X5pXB6Io5HUe8_jvUgo0MgPljJZ5fqeGjT0fMc70nNafncT3kQlKqPlVN2iAKIwTxncNtnANgrRNI3NXAUNVNvx6EwdbBYL3r0kjXjxuz6nk_cYP1rqgnFM45n2pSCPp4HvxPcAl-QNZQJ7q4CXUXKY2rvYrCO-yvx_0t4750GFfFECuslk-B9iUvUVYie-fuKHr20R8y9Wwpl_tqFH7yYr51Ag

#ACCESS HTTP
http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
#NOTE
PAST THE TOKEN ON THE LOGIN PAGE

#CREATE ADMIN USER
kubectl create serviceaccount dashboard-admin-sa
serviceaccount/dashboard-admin-sa created

kubectl create clusterrolebinding dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=default:dashboard-admin-sa
clusterrolebinding.rbac.authorization.k8s.io/dashboard-admin-sa created

kubectl get secrets
NAME                             TYPE                                  DATA   AGE
dashboard-admin-sa-token-rrp67   kubernetes.io/service-account-token   3      10m
default-token-jjwvc              kubernetes.io/service-account-token   3      62m

#GET TOLKEN
kubectl describe secret dashboard-admin-sa-token-rrp67
Name:         dashboard-admin-sa-token-rrp67
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: dashboard-admin-sa
              kubernetes.io/service-account.uid: bb2e6b2c-8ef4-11ea-8355-0800275dbe9f

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRhc2hib2FyZC1hZG1pbi1zYS10b2tlbi1ycnA2NyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tc2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiYjJlNmIyYy04ZWY0LTExZWEtODM1NS0wODAwMjc1ZGJlOWYiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpkYXNoYm9hcmQtYWRtaW4tc2EifQ.bgFAJ1CaF-sue7x5RlIR_3L8tLdn47RCR4wKKmP-TP_eUpYljslkbaecP_GI6bIS-tIuCcRkHSdfblc3rdIqwbFQV5KG2fQBIDqiTV5FBa8QkFtuD4jGT4z8Id3tIGS8VXCLG7T5r_vXGUbVpkF8uBTxtWCuKMmfC8bojZ2cvS8Tgr42uXUKChCcLUFy16zEuMCkxQySoWohi-YHu_-2JlNnT3aj-0dIsDJWsKwFWqMY02GhdnpgjzFk0jBOQQpB7KnFNk1CEcPfv9VfDn-sAA8Jnp0CJGj9z50-eFv0E2Dh29CxvspIyD0fp2kCfx8Uqr_Bhm77bBUaClqCi8vxtg





















