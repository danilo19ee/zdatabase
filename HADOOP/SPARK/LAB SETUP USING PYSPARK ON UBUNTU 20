++++++++++++++++++++++++++++++++++++++++++
+  LAB SETUP USING PYSPARK ON UBUNTU 20  +
++++++++++++++++++++++++++++++++++++++++++

#INSTALL PIP2

apt install python-pip

#INSTALL WGET

pip2 install wget

#INSTALL FINDSPARK

pip2 install findspark

#ENTER ON DIRECTORY

cd /usr/local/spark-3.0.0-bin-hadoop3.2/bin

#RUN PYSPARK

./pyspark

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Python version 2.7.17 (default, Apr 15 2020 17:20:14)
SparkSession available as 'spark'.

#DOWNLOAD LAB DATA

>>> import wget
>>> url = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/BD0211EN/data/LabData.zip'
>>> filename = wget.download(url)

#EXTRACT FILE LabData.zip

>>> import zipfile;
>>> zf = zipfile.ZipFile("LabData.zip")
>>> zf.extractall()

#IMPORT LIBRARY TO SEARCH DATASETS FILES
#PYSPARK ISN’T ON SYS.PATH BY DEFAULT, BUT THAT DOESN’T MEAN IT CAN’T BE USED AS A REGULAR LIBRARY
#USE findspark.init() TO RESOLVE THIS
#pyspark.SparkContext WE CAN SAY APACHE SPARK SPARKCONTEXT IS A HEART OF SPARK APPLICATION. 
#IT IS THE MAIN ENTRY POINT TO SPARK FUNCTIONALITY. 
#GENERATING, SPARKCONTEXT IS A MOST IMPORTANT TASK FOR SPARK DRIVER APPLICATION AND SET UP 
#INTERNAL SERVICES AND ALSO CONSTRUCTS A CONNECTION TO SPARK EXECUTION ENVIRONMENT. 
#THIS IS ESSENTIALLY A CLIENT OF SPARK’S EXECUTION ENVIRONMENT, 
#THAT ACTS AS A MASTER OF SPARK APPLICATION.
#getOrCreate THIS FUNCTION MAY BE USED TO GET OR INSTANTIATE A SPARKCONTEXT AND REGISTER IT AS A SINGLETON OBJECT

>>> import findspark
>>> import pyspark
>>> findspark.init()
>>> sc = pyspark.SparkContext.getOrCreate()
>>> readme = sc.textFile("LabData/README.md")

#VALIDATION OF ACCESS TO FILES
#THE RDD ACTION RETURNED A VALUE OF 98
>>> readme.count()
98

#FIND THE FIRST ITEM IN THE RDD:
>>> readme.first()
u'# Apache Spark'

#NOW LET’S TRY A TRANSFORMATION. 
#USE THE FILTER TRANSFORMATION TO RETURN A NEW RDD WITH A SUBSET OF THE ITEMS IN THE FILE.
#YOU CAN EVEN CHAIN TOGETHER TRANSFORMATIONS AND ACTIONS. 
#TO FIND OUT HOW MANY LINES CONTAINS THE WORD “SPARK”

>>> linesWithSpark = readme.filter(lambda line: "Spark" in line)
>>> readme.filter(lambda line: "Spark" in line).count()
18

#YOU WILL SEE THAT RDD CAN BE USED FOR MORE COMPLEX COMPUTATIONS. 
#YOU WILL FIND THE LINE FROM THAT "README.MD" FILE WITH THE MOST WORDS IN IT
#map() MAP IS DEFINED IN ABSTRACT CLASS RDD IN SPARK AND IT IS A TRANSFORMATION KIND OF OPERATION
#lambda A LAMBDA EXPRESSION IS A FUNCTION DEFINED ON THE LINE AND IS CALLED ON EACH ITEM IN A LIST OR MATRIX. 
#IT IS VERY USEFUL FOR DATA MANIPULATION.
#SPLIT A STRING INTO A LIST WHERE EACH WORD IS A LIST ITEM

>>> readme.map(lambda line: len(line.split())).reduce(lambda a, b: a if (a > b) else b)
14

#reduceByKey MERGE THE VALUES FOR EACH KEY USING AN ASSOCIATIVE AND COMMUTATIVE REDUCE FUNCTION.
#Here we combined the flatMap, map, and the reduceByKey functions to do a word count of each word in the readme file.
To collect the word counts, use the collect action.
#collect (Action)  RETURN ALL THE ELEMENTS OF THE DATASET AS AN ARRAY AT THE DRIVER PROGRAM. 
THIS IS USUALLY USEFUL AFTER A FILTER OR OTHER OPERATION THAT RETURNS A SUFFICIENTLY SMALL SUBSET OF THE DATA.

>>> wordCounts = readme.flatMap(lambda line: line.split()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a+b)
>>> wordCounts.collect()
[('#', 1), ('Apache', 1), ('Spark', 14), ('is', 6), ('It', 2), ('provides', 1), ('high-level', 1), ('APIs', 1), ('in', 5), ('Scala,', 1), ('Java,', 1), 
('an', 3), ('optimized', 1), ('engine', 1), ('supports', 2), ('computation', 1), ('analysis.', 1), ('set', 2), ('of', 5), ('tools', 1), ('SQL', 2), 
('MLlib', 1), ('machine', 1), ('learning,', 1), ('GraphX', 1), ('graph', 1), ('processing,', 1), ('Documentation', 1), ('latest', 1), ('programming', 1), 
('guide,', 1), ('[project', 2), ('README', 1), ('only', 1), ('basic', 1), ('instructions.', 1), ('Building', 1), ('using', 2), ('[Apache', 1), ('run:', 1), 
('do', 2), ('this', 1), ('downloaded', 1), ('documentation', 3), ('project', 1), ('site,', 1), ('at', 2), 
('Spark"](http://spark.apache.org/docs/latest/building-spark.html).', 1), ('Interactive', 2), ('Shell', 2), ('The', 1), ('way', 1), ('start', 1), ('Try', 1), 
('following', 2), ('1000:', 2), ('scala>', 1), ('1000).count()', 1), ('Python', 2), ('Alternatively,', 1), ('use', 3), ('And', 1), ('run', 7), ('Example', 1), 
('several', 1), ('programs', 2), ('them,', 1), ('`./bin/run-example', 1), ('[params]`.', 1), ('example:', 1), ('./bin/run-example', 2), ('SparkPi', 2), 
('variable', 1), ('when', 1), ('examples', 2), ('spark://', 1), ('URL,', 1), ('YARN,', 1), ('"local"', 1), ('locally', 2), ('N', 1), ('abbreviated', 1), 
('class', 2), ('name', 1), ('package.', 1), ('instance:', 1), ('print', 1), ('usage', 1), ('help', 1), ('no', 1), ('params', 1), ('are', 1), ('Testing', 1), 
('Spark](#building-spark).', 1), ('Once', 1), ('built,', 1), ('tests', 2), ('using:', 1), ('./dev/run-tests', 1), ('Please', 3), ('guidance', 3), ('module,', 1), ('individual', 1), ('Note', 1), ('About', 1), ('uses', 1), ('library', 1), ('HDFS', 1), ('other', 1), ('Hadoop-supported', 1), ('storage', 1), ('systems.', 1), ('Because', 1), ('have', 1), ('changed', 1), ('different', 1), ('versions', 1), ('Hadoop,', 2), ('must', 1), ('against', 1), ('version', 1), ('refer', 2), ('particular', 3), ('distribution', 1), ('Hive', 2), ('Thriftserver', 1), ('distributions.', 1), ('["Third', 1), ('distribution.', 1), ('[Configuration', 1), ('Guide](http://spark.apache.org/docs/latest/configuration.html)', 1), ('online', 1), ('overview', 1), ('configure', 1), ('Spark.', 1), ('a', 10), ('fast', 1), ('and', 10), ('general', 2), ('cluster', 2), ('computing', 1), ('system', 1), ('for', 12), ('Big', 1), ('Data.', 1), ('Python,', 2), ('R,', 1), ('that', 3), ('graphs', 1), ('data', 1), ('also', 5), ('rich', 1), ('higher-level', 1), ('including', 3), ('DataFrames,', 1), ('Streaming', 1), ('stream', 1), ('processing.', 1), ('<http://spark.apache.org/>', 1), ('##', 8), ('Online', 1), ('You', 3), ('can', 6), ('find', 1), ('the', 21), ('documentation,', 1), ('on', 6), ('web', 1), ('page](http://spark.apache.org/documentation.html)', 1), ('wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1), ('This', 2), ('file', 1), ('contains', 1), ('setup', 1), ('built', 1), ('Maven](http://maven.apache.org/).', 1), ('To', 2), ('build', 3), ('its', 1), ('example', 3), ('programs,', 1), ('build/mvn', 1), ('-DskipTests', 1), ('clean', 1), ('package', 1), ('(You', 1), ('not', 1), ('need', 1), ('to', 14), ('if', 4), ('you', 4), ('pre-built', 1), ('package.)', 1), ('More', 1), ('detailed', 2), ('available', 1), ('from', 1), ('["Building', 1), ('Scala', 2), ('easiest', 1), ('through', 1), ('shell:', 2), ('./bin/spark-shell', 1), ('command,', 2), ('which', 2), ('should', 2), ('return', 2), ('sc.parallelize(1', 1), ('prefer', 1), ('./bin/pyspark', 1), ('>>>', 1), ('sc.parallelize(range(1000)).count()', 1), ('Programs', 1), ('comes', 1), ('with', 4), ('sample', 1), ('`examples`', 2), ('directory.', 1), ('one', 2), ('<class>', 1), ('For', 2), ('will', 1), ('Pi', 1), ('locally.', 1), ('MASTER', 1), ('environment', 1), ('running', 1), ('submit', 1), ('cluster.', 1), ('be', 2), ('mesos://', 1), ('or', 3), ('"yarn"', 1), ('thread,', 1), ('"local[N]"', 1), ('threads.', 1), ('MASTER=spark://host:7077', 1), ('Many', 1), ('given.', 1), ('Running', 1), ('Tests', 1), ('first', 1), ('requires', 1), ('[building', 1), ('see', 1), ('how', 2), ('[run', 1), ('tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).', 1), ('A', 1), ('Hadoop', 4), ('Versions', 1), ('core', 1), ('talk', 1), ('protocols', 1), ('same', 1), ('your', 1), ('runs.', 1), ('["Specifying', 1), ('Version"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1), ('building', 3), ('See', 1), ('Party', 1), ('Distributions"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)', 1), ('application', 1), ('works', 1), ('Configuration', 1)]

#USE REDUCE TO DETERMINE WHATS IS THE MOST FREQUENT WORD IN THE README, AND HOW MANY TIMES WAS IT USED

wordCounts.reduce(lambda a, b: a if (a[1] > b[1]) else b)
('the', 21)

#cache() SPARK ALSO SUPPORTS PULLING DATA SETS INTO A CLUSTER-WIDE IN-MEMORY CACHE. 
THIS IS VERY USEFUL WHEN DATA IS ACCESSED REPEATEDLY, SUCH AS WHEN QUERYING A SMALL DATASET OR WHEN RUNNING AN ITERATIVE ALGORITHM LIKE RANDOM FORESTS. 
it is useful for accessing repeated data much faster. Python and Scala use the same commands
let's use our RDD linesWithSpark and cache it
Now remember that a transformation action, like the cache is not processed
until some action like the count is called

>>> print(linesWithSpark.count())
18
>>> from timeit import Timer
>>> def count():
...       return linesWithSpark.count()
...       t = Timer(lambda: count()) 
... 
>>> print(t.timeit(number=50))
2.358324580000044

>>> linesWithSpark.cache()
PythonRDD[54] at RDD at PythonRDD.scala:53
>>> print(t.timeit(number=50))
3.0295938620001834










