+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+  WORKING WITH SCALA LIBRARIES SPARK-SHELL ON UBUNTU 20  +
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_265)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import sys.process._
import sys.process._

scala> val lines = scala.io.Source.fromFile("LabData/nycweather.csv").mkString
lines: String =
""2013-01-01",1,0
"2013-01-02",-2,0
"2013-01-03",-2,0
"2013-01-04",1,0
"2013-01-05",3,0
"2013-01-06",4,0
"2013-01-07",5,0
"2013-01-08",6,0
"2013-01-09",7,0
"2013-01-10",7,0
"2013-01-11",6,13.97
"2013-01-12",7,0.51
"2013-01-13",8,0
"2013-01-14",8,2.29
"2013-01-15",3,3.05
"2013-01-16",2,17.53
"2013-01-17",4,0
"2013-01-18",-1,0
"2013-01-19",5,0
"2013-01-20",6,0
"2013-01-21",-2,0
"2013-01-22",-7,0
"2013-01-23",-9,0
"2013-01-24",-8,0
"2013-01-25",-7,1.78
"2013-01-26",-6,0
"2013-01-27",-3,0
"2013-01-28",1,5.59
"2013-01-29",6,1.52
"2013-01-30",9,1.02
"2013-01-31",8,22.86
"2013-02-01",-2,0
"2013-02-02",-4,0.51
"2013-02-03",-3,0.51
"2013-02-04",-3,0
"2013-02-05",-1,0.51
"2013-02-06",1,0
"2013-02-07",-2,0
"2013-02-08",-1,29.21
"2013-02-09",-3,9.65
"2013-0...

scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@773236a7

scala> import sqlContext.implicits._
import sqlContext.implicits._

scala> case class Weather(date: String, temp: Int, precipitation: Double)
defined class Weather

scala> val weather = sc.textFile("LabData/nycweather.csv").map(_.split(",")). map(w => Weather(w(0), w(1).trim.toInt, w(2).trim.toDouble)).toDF()
weather: org.apache.spark.sql.DataFrame = [date: string, temp: int ... 1 more field]

scala> weather.registerTempTable("weather")
warning: there was one deprecation warning (since 2.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'

scala> val hottest_with_precip = sqlContext.sql("SELECT * FROM weather WHERE precipitation > 0.0 ORDER BY temp DESC")
hottest_with_precip: org.apache.spark.sql.DataFrame = [date: string, temp: int ... 1 more field]

scala> hottest_with_precip.collect()
res1: Array[org.apache.spark.sql.Row] = Array(["2013-06-26",27,1.27], ["2013-06-27",27,6.1], ["2013-07-08",27,5.59], ["2013-07-09",27,5.84], ["2013-07-22",27,1.52], ["2013-07-23",27,7.87], ["2013-08-09",27,1.27], ["2013-06-02",26,21.59], ["2013-07-03",26,13.46], ["2013-08-27",26,0.25], ["2013-08-28",26,10.92], ["2013-09-02",26,1.27], ["2013-09-10",26,0.25], ["2013-09-12",26,40.64], ["2013-06-17",25,0.25], ["2013-07-02",25,2.03], ["2013-07-29",25,0.25], ["2013-07-01",24,21.34], ["2013-08-08",24,11.68], ["2013-08-12",24,1.27], ["2013-08-22",24,6.35], ["2013-08-26",24,1.02], ["2013-09-03",24,0.76], ["2013-06-18",23,4.83], ["2013-07-12",23,6.35], ["2013-07-13",23,1.52], ["2013-07-28",23,6.1], ["2013-08-03",23,1.52], ["2013-08-13",23,21.59], ["2013-05-23",22,45.97],...

scala> :quit

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

CREATING A SPARK APPLICATION USING MLLIB

./spark-shell

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_265)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.clustering.KMeans

scala> import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.Vectors

scala> val taxiFile = sc.textFile("LabData/nyctaxisub.csv")
taxiFile: org.apache.spark.rdd.RDD[String] = LabData/nyctaxisub.csv MapPartitionsRDD[1] at textFile at <console>:26

scala> taxiFile.count()
res0: Long = 250000                                                             

scala> val taxiData=taxiFile.filter(_.contains("2013")).filter(_.split(",")(3)!="" ).filter(_.split(",")(4)!="")
taxiData: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at <console>:27

scala> taxiData.count()
res1: Long = 249999

scala> val taxiFence=taxiData.filter(_.split(",")(3).toDouble>40.70).filter(_.split(",")(3).toDouble<40.86).filter(_.split(",")(4).toDouble>(-74.02)).filter(_.split(",")(4).toDouble<(-73.93))
taxiFence: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at filter at <console>:27

scala> taxiFence.count()
res2: Long = 206646                                                             

scala> val taxi=taxiFence.
     |     map{
     |         line=>Vectors.dense(
     |             line.split(',').slice(3,5).map(_ .toDouble)
     |         )
     |     }
taxi: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[9] at map at <console>:28

scala> val iterationCount=10
iterationCount: Int = 10

scala> val clusterCount=3
clusterCount: Int = 3

scala> val model=KMeans.train(taxi,clusterCount,iterationCount)
20/09/09 16:50:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
20/09/09 16:50:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
model: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@45c2fa2

scala> val clusterCenters=model.clusterCenters.map(_.toArray)
clusterCenters: Array[Array[Double]] = Array(Array(40.7870920457465, -73.95708700772364), Array(40.72481642093873, -73.99586622167347), Array(40.7570053759507, -73.98085806937306))

scala> clusterCenters.foreach(lines=>println(lines(0),lines(1)))
(40.7870920457465,-73.95708700772364)
(40.72481642093873,-73.99586622167347)
(40.7570053759507,-73.98085806937306)

https://www.google.com/maps/place/40%C2%B047'13.5%22N+73%C2%B057'25.5%22W/@40.787092,-73.9592757,17z/data=!3m1!4b1!4m5!3m4!1s0x0:0x0!8m2!3d40.787092!4d-73.957087
https://www.google.com/maps/place/40%C2%B043'29.3%22N+73%C2%B059'45.1%22W/@40.7248164,-73.9980549,17z/data=!3m1!4b1!4m5!3m4!1s0x0:0x0!8m2!3d40.7248164!4d-73.9958662
https://www.google.com/maps/place/40%C2%B045'25.2%22N+73%C2%B058'51.1%22W/@40.7570054,-73.9830468,17z/data=!3m1!4b1!4m5!3m4!1s0x0:0x0!8m2!3d40.7570054!4d-73.9808581

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

CREATING A SPARK APPLICATION USING SPARK STREAMING

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_265)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.log4j.Logger
import org.apache.log4j.Logger

scala> import org.apache.log4j.Level
import org.apache.log4j.Level

scala> Logger.getLogger("org").setLevel(Level.OFF)

scala> Logger.getLogger("akka").setLevel(Level.OFF)

scala> import org.apache.spark._
import org.apache.spark._

scala> import org.apache.spark.streaming._
import org.apache.spark.streaming._

scala> import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.StreamingContext._

scala> val ssc = new StreamingContext(sc, Seconds(1))
ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@4da86d09

scala> val lines = ssc.socketTextStream("localhost", 7777)
lines: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] = org.apache.spark.streaming.dstream.SocketInputDStream@9be2619

scala> val pass = lines.map(_.split(",")).map(pass=>(pass(15), pass(7).toInt)).reduceByKey(_+_)
pass: org.apache.spark.streaming.dstream.DStream[(String, Int)] = org.apache.spark.streaming.dstream.ShuffledDStream@6788168c

scala> pass.print()

scala> ssc.start()

scala> -------------------------------------------
Time: 1599747323000 ms
-------------------------------------------

-------------------------------------------
Time: 1599747324000 ms
-------------------------------------------

-------------------------------------------                                     
Time: 1599747325000 ms
-------------------------------------------






























