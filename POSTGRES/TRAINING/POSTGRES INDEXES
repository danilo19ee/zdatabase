####################
# POSTGRES INDEXES #
####################

Any question can be viewed from different points of view. 
We will talk about what an application developer using a DBMS should be interested in: what indexes exist, 
why there are so many different indexes in PostgreSQL, and how to use them to speed up queries. 
Perhaps, the topic could be revealed with fewer words, 
but we secretly hope for an inquisitive developer who is also interested in the details of the internal structure, 
especially since understanding such details allows not only listening to someone else's opinion, but also drawing our own conclusions.

The questions of developing new types of indexes will remain outside the discussion. 
This requires knowledge of the C language and belongs to the competence of a systems programmer rather than an application developer. 
For the same reason, we will practically not consider programming interfaces, but will focus only on what matters for using ready-to-use indexes.

In this part, we will talk about the division of responsibilities between the general indexing mechanism related to the database engine and the individual 
index access methods that can be added as extensions to PostgreSQL. In the next part, 
we will look at the accessor interface and important concepts such as classes and operator families. 
After such a long but necessary introduction, we will take a closer look at the structure and use of various types of indexes: 
Hash, B-tree, GiST, SP-GiST, GIN and RUM, BRIN and Bloom.

-----------
- Indexes -
-----------

Indexes in PostgreSQL are special database objects designed primarily to speed up data access. 
These are auxiliary structures: any index can be dropped and restored from the information in the table. 
Sometimes you hear that a DBMS can work without indexes, just slowly. However, 
this is not the case because indexes also serve to maintain some integrity constraints.

There are currently six different kinds of indexes built into PostgreSQL 9.6, and one more is available as an extension, 
made possible by important changes in version 9.6. So we should expect other types of indexes to appear in the near future.

Despite all the differences between the types of indexes (also called accessor methods), 
ultimately any of them establishes a correspondence between a key (for example, the value of an indexed column) and the table rows in which that key occurs. 
Lines are identified by a TID (tuple id), which consists of the file block number and the line position within the block. 
Then, knowing the key or some information about it, you can quickly read those lines in which the information of interest to us may be located, 
without looking through the entire table.

It is important to understand that the index, while speeding up data access, instead requires certain costs for its maintenance. 
For any operation on indexed data — whether it is inserting, deleting, or updating rows in a table — the indexes created on that table must be rebuilt, 
and within the same transaction. Note that updating table fields that have not been indexed does not rebuild the indexes; 
this mechanism is called HOT (Heap-Only Tuples).

Extensibility has several implications. To make the new access method easy to integrate into the system, 
PostgreSQL has a generic indexing mechanism. Its main task is to get the TID from the accessor and work with them:

  * reading data from corresponding versions of table rows;
  * sampling by a separate TID, or immediately by a set of TIDs (with building a bitmap);
  * checking the visibility of row versions for the current transaction, taking into account the isolation level.

The indexing engine is involved in the execution of queries; it is called according to the plan built during the optimization phase. 
The optimizer, when iterating and evaluating the various paths to execute a query, 
must understand the capabilities of all accessors that can potentially be applied. 
Will the access method be able to send data immediately in the desired order, 
or should we provide for sorting separately? is it possible to apply an accessor to search for null? - such questions are constantly solved by the optimizer.

Accessor information isn't just needed by the optimizer. When creating an index, 
the system needs to decide: can the index be built over multiple columns? can this index ensure uniqueness?

So, each access method must provide all the necessary information about itself. 
Before version 9.6, the pg_am table was used for this, and since 9.6 the data has moved deeper, inside special functions. 
We will get acquainted with this interface a little later.

The tasks of the accessor itself include everything else:

  * implementation of the index building algorithm and data paging (so that any index is processed in the same way by the buffer cache manager);
  * search for information in the index by the expression "indexed-field operator expression";
  * estimating the cost of using the index;
  * work with locks necessary for correct parallel execution of processes;
  * creation of a write-ahead log (WAL).

We'll first look at the capabilities of the general indexing mechanism, and then move on to looking at the various accessors.

-------------------
- Indexing Engine -
-------------------

The indexing mechanism allows PostgreSQL to work in the same way with a wide variety of accessors, given their capabilities.

Basic scanning methods

--------------
- Index scan -
--------------

You can work differently with the TIDs supplied by the index. Let's consider an example:

postgres = # create table t (a integer, b text, c boolean);
CREATE TABLE
postgres = # insert into t (a, b, c)
   select s.id, chr ((32 + random () * 94) :: integer), random () <0.01
   from generate_series (1,100000) as s (id)
   order by random ();
INSERT 0 100000
postgres = # create index on t (a);
CREATE INDEX
postgres = # analyze t;
ANALYZE

We have created a table with three fields. The first field contains numbers from 1 to 100000, 
and an index has been created on it (we don't care which one yet). The second field contains various ASCII characters besides non-printable ones. 
Finally, the third field contains a Boolean value that is true for about 1% of the rows and false for the rest. Rows are inserted into the table in random order.

Let's try to select a value according to the condition "a = 1". Note that the condition has the form "indexed-field operator expression", 
where the operator is "equal" and the expression (search key) is "1". In most cases, the condition must be exactly this kind for the index to be used.

postgres = # explain (costs off) select * from t where a = 1;
          QUERY PLAN
-------------------------------
 Index Scan using t_a_idx on t
   Index Cond: (a = 1)
(2 rows)

In this case, the optimizer has decided to use an Index Scan. When indexed, the accessor returns the TID values one at a time, 
until there are no matching rows. The indexing engine takes turns looking at the pages of the table pointed to by TIDs, 
getting the version of the row, checking its visibility in accordance with the multiversion rules, and returning the resulting data.

---------------
- Bitmap Scan -
---------------

Index scan works well when it comes to just a few values. However, as the sample grows, 
the chances of having to go back to the same table page multiple times increase. Therefore, in this case, the optimizer switches to a bitmap scan:

postgres = # explain (costs off) select * from t where a <= 100;
             QUERY PLAN
------------------------------------
 Bitmap Heap Scan on t
   Recheck Cond: (a <= 100)
   -> Bitmap Index Scan on t_a_idx
         Index Cond: (a <= 100)
(4 rows)

First, the accessor returns all TIDs that match the condition (the Bitmap Index Scan node), and a bitmap of row versions is built from them. 
The row versions are then read from the table (Bitmap Heap Scan) - each page will only be read once.

Note that in the second step, the condition can be rechecked (Recheck Cond). 
The sample may be too large for the row version bitmap to fit entirely into RAM (limited by the work_mem parameter). 
In this case, only a bitmap of pages containing at least one valid version of the string is built. Such a "rough" map takes up less space, 
but when reading a page, you have to double-check the conditions for each line stored there. Note that even in the case of a small sample (as in our example), 
the "Recheck Cond" step is still displayed in the plan, although it is not actually executed.

If conditions are imposed on multiple table fields and those fields are indexed, ,
a bitmap scan allows (if the optimizer deems it beneficial) multiple indexes to be used simultaneously. For each index, bitmaps of row versions are built, 
which are then bitwise logically multiplied (if the expressions are connected with the AND condition), 
or logically added (if the expressions are connected with the OR condition). For instance:

postgres = # create index on t (b);
CREATE INDEX
postgres = # analyze t;
ANALYZE
postgres = # explain (costs off) select * from t where a <= 100 and b = 'a';
                    QUERY PLAN
--------------------------------------------------
 Bitmap Heap Scan on t
   Recheck Cond: ((a <= 100) AND (b = 'a' :: text))
   -> BitmapAnd
         -> Bitmap Index Scan on t_a_idx
               Index Cond: (a <= 100)
         -> Bitmap Index Scan on t_b_idx
               Index Cond: (b = 'a' :: text)
(7 rows)

Here the BitmapAnd node concatenates the two bitmaps using the bitwise operation "and".

Bitmap scanning avoids repeated accesses to the same data page. 
But what if the data in the pages of the table is physically ordered in the same way as the index entries? Of course, 
you cannot completely rely on the physical order of the data in the pages - if you want sorted data, 
you must explicitly specify the ORDER BY clause in the query. But situations are quite possible in which, in fact, 
"almost all" of the data is ordered: for example, if rows are added in the desired order and do not change after that, 
or after the CLUSTER command has been executed. Then building a bitmap is an extra step, 
a regular index scan will be no worse (if you do not take into account the possibility of combining several indices). 
Therefore, when choosing an access method, the planner looks into special statistics that show the degree of data ordering:

postgres = # select attname, correlation from pg_stats where tablename = 't';
 attname  | correlation
--------- + -------------
 b        | 0.533512
 c        | 0.942365
 a        | -0.00768816
(3 rows)

Values close in absolute value to one indicate a high ordering (as for column c), and close to zero, on the contrary, 
indicate a chaotic distribution (column a).

-------------------
- Sequential scan -
-------------------

For the sake of completeness, it should be said that under the non-selective condition, 
the optimizer will prefer to use the index to scan the entire table sequentially:

postgres = # explain (costs off) select * from t where a <= 40000;
       QUERY PLAN
------------------------
 Seq Scan on t
   Filter: (a <= 40000)
(2 rows)

And he will be right. The point is that indexes work better, the higher the selectivity of the condition, that is, the fewer rows that satisfy it. 
As the sample grows, so does the overhead of reading the index pages.

The situation is aggravated by the fact that sequential reading is faster than reading pages "out of order". 
This is especially true for hard drives, where the mechanical operation of bringing the head to the track takes significantly longer than reading the data itself;
this effect is less pronounced with SSDs. To take into account the difference in the cost of access, 
there are two parameters seq_page_cost and random_page_cost, which can be set not only globally, but also at the table space level, 
thus taking into account the characteristics of different disk subsystems.

--------------------
- Covering indices -
--------------------

Typically, the main purpose of an accessor is to return matching table row identifiers so that the indexing engine can read the data it needs. 
But what if the index already contains all the data it needs to query? Such an index is called covering, 
in which case the optimizer can use an Index Only Scan:

postgres = # vacuum t;
VACUUM
postgres = # explain (costs off) select a from t where a <100;
             QUERY PLAN
------------------------------------
 Index Only Scan using t_a_idx on t
   Index Cond: (a <100)
(2 rows)

The name might suggest that the indexing mechanism does not access the table at all, getting all the information it needs solely from the accessor. 
This is not entirely true, because indexes in PostgreSQL do not contain information to judge the visibility of rows. 
Therefore, the accessor returns all the row versions that match the search condition, whether they are visible to the current transaction or not.

However, if the indexing engine had to look at the table each time to determine visibility, this scan method would be no different from a regular index scan.

The problem is solved by the fact that PostgreSQL maintains a so-called visibility map for tables, 
in which the vacuum process marks pages in which data has not changed long enough for all transactions to see them, 
regardless of the start time and isolation level. If the identifier of the row returned by the index refers to such a page, then visibility can be omitted.

Therefore, regular cleaning improves the performance of the covering indexes. Moreover, 
the optimizer takes into account the number of uncleaned rows and may opt out of using an index-only scan if it predicts large visibility overhead.

The number of table accesses that were forced can be found using the explain analyze command:

postgres = # explain (analyze, costs off) select a from t where a <100;
                                  QUERY PLAN
-------------------------------------------------- -----------------------------
 Index Only Scan using t_a_idx on t (actual time = 0.025..0.036 rows = 99 loops = 1)
   Index Cond: (a <100)
   Heap Fetches: 0
 Planning time: 0.092 ms
 Execution time: 0.059 ms
(5 rows)

In this case, it was not necessary to access the table (Heap Fetches: 0), since cleanup has just been performed. 
In general, the closer this number is to zero, the better.

Not all indexes store the indexed values themselves along with row identifiers. If an accessor cannot return data, it cannot be used for index-only scans.

--------
- Null -
--------

Undefined values play an important role in relational databases as a convenient way to represent the fact that a value does not exist or is not known.

But special importance also requires a special attitude to itself. Normal Boolean logic turns into three-valued logic; 
it is not clear whether the undefined value should be less than normal values or more (hence the special constructions for sorting NULLS FIRST and NULLS LAST); 
it is not obvious whether it is necessary to take into account undefined values in aggregate functions or not; special statistics required for the scheduler ...

From the point of view of indexing support with undefined values, there is also ambiguity: should such values be indexed or not? If you do not index null, 
the index can be more compact. But if you index, then it becomes possible to use the index for conditions of the form "indexed-field IS [NOT] NULL", 
as well as as a covering index in the absence of conditions on the table (since in this case the index must return the data of all rows of the table, 
including number and with undefined values).

For each accessor, its developers make their own decision whether to index nulls or not. But, as a rule, they are still indexed.

--------------------------
- Multiple field indexes -
--------------------------

Conditions on multiple fields can be supported using multi-column indexes. For example, we could create an index on two fields of our table:

postgres = # create index on t (a, b);
CREATE INDEX
postgres = # analyze t;
ANALYZE

The optimizer will most likely prefer this index over bitmap concatenation, since here we immediately get the desired TIDs without any additional actions:

postgres = # explain (costs off) select * from t where a <= 100 and b = 'a';
                   QUERY PLAN
------------------------------------------------
 Index Scan using t_a_b_idx on t
   Index Cond: ((a <= 100) AND (b = 'a' :: text))
(2 rows)

A multi-column index can also be used to speed up selection by condition for some of the fields - starting from the first:

postgres = # explain (costs off) select * from t where a <= 100;
              QUERY PLAN
--------------------------------------
 Bitmap Heap Scan on t
   Recheck Cond: (a <= 100)
   -> Bitmap Index Scan on t_a_b_idx
         Index Cond: (a <= 100)
(4 rows)

Typically, if no condition is imposed on the first field, the index will not be used. However, in some cases, 
the optimizer may find it more beneficial than sequential scans. We'll cover this topic in more detail when we look at btree indexes.

Not all accessors support creating indexes on multiple columns.

----------------------
- Expression indexes -
----------------------

We talked about the search condition being "indexed-field operator expression". In the example below, 
the index will not be used because an expression with it is used instead of the field name:

postgres = # explain (costs off) select * from t where lower (b) = 'a';
                QUERY PLAN
------------------------------------------
 Seq Scan on t
   Filter: (lower ((b) :: text) = 'a' :: text)
(2 rows)

This particular query can be easily rewritten so that only the field name appears to the left of the operator. 
But if this is not possible, expression indexes (functional indexes) come to the rescue:

postgres = # create index on t (lower (b));
CREATE INDEX
postgres = # analyze t;
ANALYZE
postgres = # explain (costs off) select * from t where lower (b) = 'a';
                     QUERY PLAN
-------------------------------------------------- -
 Bitmap Heap Scan on t
   Recheck Cond: (lower ((b) :: text) = 'a' :: text)
   -> Bitmap Index Scan on t_lower_idx
         Index Cond: (lower ((b) :: text) = 'a' :: text)
(4 rows)

A functional index is created not by a table field, but by an arbitrary expression; 
the optimizer will take such an index into account for conditions of the form "indexed-expression operator expression". 
If the calculation of the indexed expression is a costly operation, then updating the index will also require significant computing resources.

It should also be borne in mind that separate statistics are collected for the indexed expression. It can be seen in the pg_stats view by the index name:

postgres = # \ d t
       Table "public.t"
 Column | Type | Modifiers
-------- + --------- + -----------
 a | integer |
 b | text |
 c | boolean |
Indexes:
    "t_a_b_idx" btree (a, b)
    "t_a_idx" btree (a)
    "t_b_idx" btree (b)
    "t_lower_idx" btree (lower (b))

postgres = # select * from pg_stats where tablename = 't_lower_idx';
...

If necessary, you can control the number of histogram buckets in the same way as for regular table fields (bearing in mind that the column name 
can be different depending on the indexed expression):

postgres=# \d t_lower_idx
 Index "public.t_lower_idx"
 Column | Type | Definition
--------+------+------------
 lower  | text | lower(b)
btree, for table "public.t"

postgres=# alter index t_lower_idx alter column "lower" set statistics 69;
ALTER INDEX

-------------------
- Partial Indexes -
-------------------

Sometimes it becomes necessary to index only part of the table rows. 
This is usually due to a strong uneven distribution: a rare value makes sense to search by index, 
but a frequent value is easier to find by a full scan of the table.

Of course, you can build a regular index on column "c" and it will work as we expect:

postgres = # create index on t (c);
CREATE INDEX
postgres = # analyze t;
ANALYZE
postgres = # explain (costs off) select * from t where c;
          QUERY PLAN
-------------------------------
 Index Scan using t_c_idx on t
   Index Cond: (c = true)
   Filter: c
(3 rows)

postgres = # explain (costs off) select * from t where not c;
    QUERY PLAN
-------------------
 Seq Scan on t
   Filter: (NOT c)
(2 rows)

Moreover, the index is 276 pages:

postgres = # select relpages from pg_class where relname = 't_c_idx';
 relpages
----------
      276
(1 row)

But since column "c" is true for only one percent of the rows, 99% of the index is simply never used. In this case, a partial index can be built:

postgres = # create index on t (c) where c;
CREATE INDEX
postgres = # analyze t;
ANALYZE

The size of such an index was reduced to 5 pages:

postgres = # select relpages from pg_class where relname = 't_c_idx1';
 relpages
----------
     five
(1 row)

In some cases, the difference in volume and performance can be quite significant.

-----------
- Sorting -
-----------

If the accessor returns row ids in sort order, this gives the optimizer additional options for executing the query.

You can scan the table and then sort the data:

postgres = # set enable_indexscan = off;
SET
postgres = # explain (costs off) select * from t order by a;
     QUERY PLAN
---------------------
 Sort
   Sort Key: a
   -> Seq Scan on t
(3 rows)

And you can read the data using the index immediately in the sort order:

postgres = # set enable_indexscan = on;
SET
postgres = # explain (costs off) select * from t order by a;
          QUERY PLAN
-------------------------------
 Index Scan using t_a_idx on t
(1 row)

Of all the accessors, only btree can return sorted data, so let's postpone a more detailed discussion until we look at this type of index.

-------------------------
- Parallel construction -
-------------------------

Typically building an index requires a SHARE lock on the table. This lock allows you to read data from the table, 
but disallows any changes while the index is being built.

You can verify this if, at the time of creating the index, say, on table t, in another session, execute the query:

postgres = # select mode, granted from pg_locks where relation = 't' :: regclass;
   mode     | granted
----------- + ---------
 ShareLock  | t
(1 row)

If the table is large enough and is actively used in insert, update, or delete mode, 
this may be invalid - modifying sessions will wait a long time to release the lock.

In this case, you can use parallel index creation:

postgres = # create index concurrently on t (a);
CREATE INDEX

Such a command sets a SHARE UPDATE EXCLUSIVE lock, which allows both reading and modifying data (only changing the structure of the table is prohibited, 
as well as simultaneously performing cleanup, analysis, or building another index on the same table).

However, there is a downside. First, the index will build more slowly than usual, since instead of one pass through the table, two are performed, 
and you still need to wait for the completion of parallel transactions that modify the data.

Second, building an index in parallel can result in a deadlock or unique constraint violation. The index is nevertheless created, 
but in a "non-working" state; in this case, it must be deleted and recreated again. 
Broken indexes are marked with the word INVALID in the output of the psql \ d command, and the complete list can be obtained with the query:

postgres = # select indexrelid :: regclass index_name, indrelid :: regclass table_name from pg_index where not indisvalid;
 index_name  | table_name
------------ + ------------
 t_a_idx     | t
(1 row)

--------------
- Properties -
--------------

All properties of access methods are presented in the pg_am table (am - access method). From this table, you can get the list of available methods itself:

postgres = # select amname from pg_am;
amname
--------
btree
hash
gist
gin
spgist
brin
(6 rows)

Although sequential scanning can rightfully be classified as access method, historically it has not been included in this list.

In PostgreSQL 9.5 and older, each property was represented by a separate field in the pg_am table. Since version 9.6, 
properties are polled by special functions and are divided into several levels:

accessor properties - pg_indexam_has_property,
properties of a particular index - pg_index_has_property,
properties of the individual columns of the index - pg_index_column_has_property.

The division into accessor and index levels is made for the future: at present, 
all indexes created based on the same accessor will always have the same properties.


Accessor properties include the following four (for example, btree):

postgres = # select a.amname, p.name, pg_indexam_has_property (a.oid, p.name)
from pg_am a,
unnest (array ['can_order', 'can_unique', 'can_multi_col', 'can_exclude']) p (name)
where a.amname = 'btree' order by a.amname;
amname   | name            | pg_indexam_has_property
-------- + --------------- + -------------------------
btree    | can_order       | t
btree    | can_unique      | t
btree    | can_multi_col   | t
btree    | can_exclude     | t
(4 rows)

 * can_order 
   The accessor allows you to specify the sort order of values when the index is created (currently only applicable for btree);
 * can_unique
   Support for unique constraint and primary key (only applicable for btree);
 * can_multi_col
   An index can be built on multiple columns;
 * can_exclude
   Support for limiting the EXCLUDE exception.

Index related properties (let's take an existing one for example):

postgres = # select p.name, pg_index_has_property ('t_a_idx' :: regclass, p.name)
from unnest (array ['clusterable', 'index_scan', 'bitmap_scan', 'backward_scan']) p (name);
name            | pg_index_has_property
--------------- + -----------------------
clusterable     | t
index_scan      | t
bitmap_scan     | t
backward_scan   | t
(4 rows)

 * clusterable
   Possibility of reordering table rows in accordance with the given index (clustering by the CLUSTER command of the same name);
 * index_scan
   Index scan support. This property may seem strange, but not all indexes can return TIDs one at a time - some give all results at once and only support bitmap scanning;
 * bitmap_scan
   Bitmap scanning support;
 * backward_scan
   Returns the result in the reverse order of that specified when the index was created.

Finally, the column properties:

postgres = # select p.name, pg_index_column_has_property ('t_a_idx' :: regclass, 1, p.name)
from unnest (array ['asc', 'desc', 'nulls_first', 'nulls_last', 'orderable', 'distance_orderable', 'returnable', 'search_array', 'search_nulls']) p (name);
name                 | pg_index_column_has_property
-------------------- + ----------------------------- -
asc                  | t
desc                 | f
nulls_first          | f
nulls_last           | t
orderable            | t
distance_orderable   | f
returnable           | t
search_array         | t
search_nulls         | t
(9 rows)

 * asc, desc, nulls_first, nulls_last, orderable
   These properties are related to the ordering of values ​​(we'll talk about them when we get to btree indexes);
 * distance_orderable
   Returns the result in the sort order by operation (currently only applicable for gist and rum indexes);
 * returnable
   The ability to use an index without accessing the table, that is, support for only index access;
 * search_array
   Support for searching multiple values for the "indexed-field IN (list_constants)" or, equivalently, "indexed-field = ANY (array_constants)";
 * search_nulls
   The ability to search for is null and is not null conditions.

We have already discussed some of the properties in detail earlier. Some of the properties are currently implemented by only one method. 
We will consider such possibilities when we talk about this particular method.

---------------------------------
- Operator Classes and Families -
---------------------------------

In addition to the set of "skills", you also need to know what types of data and with what operators the access method works. 
For this, PostgreSQL has the concepts of an operator class and an operator family.

The operator class contains a minimal set of operators (and possibly helper functions) for working with an index on some data type.

A class is always part of a family of operators. In this case, several classes can be included in one common family if they have the same semantics. 
For example, the integer_ops family includes classes int8_ops, int4_ops, and int2_ops for different sized but identical bigint, integer, and smallint types:

postgres = # select opfname, opcname, opcintype :: regtype
from pg_opclass opc, pg_opfamily opf
where opf.opfname = 'integer_ops'
and opc.opcfamily = opf.oid
and opf.opfmethod = (select oid from pg_am where amname = 'btree');
opfname       | opcname    | opcintype
------------- + ---------- + -----------
integer_ops   | int2_ops   | smallint
integer_ops   | int4_ops   | integer
integer_ops   | int8_ops   | bigint
(3 rows)

Another example: the datetime_ops family includes operator classes for working with dates (both without time, and with time):

postgres = # select opfname, opcname, opcintype :: regtype
from pg_opclass opc, pg_opfamily opf
where opf.opfname = 'datetime_ops'
and opc.opcfamily = opf.oid
and opf.opfmethod = (select oid from pg_am where amname = 'btree');
opfname        | opcname           | opcintype
-------------- + ----------------- + ----------------- ------------
datetime_ops   | date_ops          | date
datetime_ops   | timestamptz_ops   | timestamp with time zone
datetime_ops   | timestamp_ops     | timestamp without time zone
(3 rows)

The family can also include additional operators for comparing values of different types. 
By grouping into families, the planner can use an index for predicates with values of different types. The family may also contain other helper functions.

In most cases, you don't need to know anything about operator families and classes. 
Usually we just create an index and use some default operator class.

However, you can specify the operator class explicitly. A simple example when needed: in a database with a non-C collation, 
a normal index on a text field does not support LIKE:

postgres = # show lc_collate;
lc_collate
-------------
en_US.UTF-8
(1 row)
postgres = # explain (costs off) select * from t where b like 'A%';
QUERY PLAN
-----------------------------
Seq Scan on t
Filter: (b ~~ 'A%' :: text)
(2 rows)

You can overcome this limitation by creating an index with the operator class text_pattern_ops (note how the condition has changed in the plan):

postgres = # create index on t (b text_pattern_ops);
CREATE INDEX
postgres = # explain (costs off) select * from t where b like 'A%';
QUERY PLAN
-------------------------------------------------- --------------
Bitmap Heap Scan on t
Filter: (b ~~ 'A%' :: text)
-> Bitmap Index Scan on t_b_idx1
Index Cond: ((b ~> = ~ 'A' :: text) AND (b ~ <~ 'B' :: text))
(4 rows)

--------------------
- System directory -
--------------------

To conclude this part, we present a small diagram of the system catalog tables that directly relate to operator classes and families.
https://mega.nz/file/xo0l1K5I#0JNL6ibUVgBLSNxvG7F0JKULOGHB6YsYlrfr1EXD-yU

All these tables are, of course, detailed.
https://www.postgresql.org/docs/13/catalogs.html

Using the system catalog, you can find the answer to a number of questions without even looking at the documentation. 
For example, what types of data can such and such an accessor work with?

postgres = # select opcname, opcintype :: regtype
from pg_opclass
where opcmethod = (select oid from pg_am where amname = 'btree')
order by opcintype :: regtype :: text;
opcname               | opcintype
--------------------- + ---------------------------- -
abstime_ops           | abstime
array_ops             | anyarray
enum_ops              | anyenum
...

What operators are included in the class (and therefore the index can be used for conditional access that includes such an operator)?

postgres = # select amop.amopopr :: regoperator
from pg_opclass opc, pg_opfamily opf, pg_am am, pg_amop amop
where opc.opcname = 'array_ops'
and opf.oid = opc.opcfamily
and am.oid = opf.opfmethod
and amop.amopfamily = opc.opcfamily
and am.amname = 'btree'
and amop.amoplefttype = opc.opcintype;
amopopr
-----------------------
<(anyarray, anyarray)
<= (anyarray, anyarray)
= (anyarray, anyarray)
> = (anyarray, anyarray)
> (anyarray, anyarray)
(5 rows)

---------------
- Index Types -
---------------

--------
- Hash -
--------

General theory

Many modern programming languages include hash tables as their underlying data type. Outwardly, it looks like a regular array, but not an integer, 
but any data type (for example, a string) is used as an index. The hash index is similar in PostgreSQL. How it works?

Typically, data types have very large ranges of valid values: how many different rows can theoretically be represented in a column of type text? 
At the same time, how many different values are actually stored in the text column of some table? Usually not much.

The idea of hashing is to associate a small number with a value of any data type (from 0 to N − 1, N values in total). 
This mapping is called a hash function. The resulting number can be used as an index of a regular array, where you can add references to table rows (TID). 
The elements of such an array are called hash table buckets - several TIDs can be in one bucket if the same indexed value occurs in different rows.

The hash function is better, the more evenly it distributes the original values across the buckets. 
But even a good function will sometimes give the same result for different input values - this is called collision. 
So, one bucket may contain TIDs corresponding to different keys, and therefore the TIDs obtained from the index must be rechecked.

Just as an example: what hash function can you think of for strings? Let the number of baskets be 256. 
Then the code of the first character can be taken as the basket number (for example, we have a one-byte encoding). 
Is this a good hash function? Obviously not: if all lines start with the same character, they all end up in the same bucket; 
there is no question of uniformity, you will have to recheck all the values ​​and the whole meaning of hashing will be lost.
What if you sum the codes of all characters modulo 256? It will be much better, although not perfect either. 
If you are wondering how such a hash function actually works in PostgreSQL, see the definition of hash_any () in hashfunc.c.

Index device

Let's go back to the hash index. Our task is to quickly find the corresponding TID by the value of some data type (indexing key).

When inserting into the index, calculate the hash function for the key. Hash functions in PostgreSQL always return integer, 
which corresponds to a range of 232 ≈ 4 billion values. The number of buckets is initially equal to two and increases dynamically, 
adjusting to the amount of data; the bucket number can be calculated from the hash code using bit arithmetic. Let's put our TID in this basket.

But this is not enough, because TIDs corresponding to different keys can get into one basket. 
How to be? It would be possible to write the original key value along with the TID to the bucket, but this would greatly increase the size of the index. 
So to save space, not the key itself is saved in the basket, but its hash code.

When searching the index, we compute the hash function for the key and get the bucket number. 
All that remains is to iterate through the entire contents of the basket and return only the matching TIDs with the required hash codes. 
This is done efficiently because hash-TID pairs are stored in order.

But it may so happen that two different keys will not just end up in the same bucket, 
but will also have the same 4-byte hash codes - no one canceled collisions. Therefore, 
the access method asks the general indexing mechanism to control each TID, 
rechecking the condition against the table row (the mechanism can do this along with the visibility check).

Page organization

If you look at the index not from the point of view of scheduling and executing a query, but through the eyes of a buffer cache manager, 
it turns out that all information, all index records must be packed into pages. 
Such index pages are buffered and flushed out in the same way as table pages.

https://mega.nz/file/V99GSa5b#imu61Yt23noh2sKw8AzJI_9KpJKLOrkFsySkLRCWXCw

The hash index, as you can see in the picture, uses pages (gray rectangles) of four types:

 * Meta page - zero page, contains information about what is inside the index;
 * Bucket pages - the main pages of the index, store data in the form of pairs "hash code - TID";
 * Overflow pages - are arranged in the same way as cart pages and are used when one page is not enough for the cart;
 * Bitmap pages - These are the overflow pages that have been vacated and can be used for other buckets.
 
Down arrows from index page items symbolize TIDs - links to table rows.
With the next increase in the index, twice as many buckets (and, accordingly, pages) are created at one time than the last time. 
In order not to select at once such a potentially large number of pages, in version 10 they made a smoother increase in size. Well, 
overflow pages are just allocated as needed and tracked in bitmap pages, which are also allocated as needed.
Note that the hash index cannot be reduced in size. If you delete some of the indexed rows, 
the once allocated pages are no longer returned to the operating system, but only reused for new data after cleaning (VACUUM). 
The only way to reduce the physical size of the index is to rebuild it from scratch with the REINDEX or VACUUM FULL command. 

Example

Let's give an example of creating a hash index. In order not to invent our own tables, 
here and further we will use the demo database on air transportation, and this time we will take the table of flights.

demo = # create index on flights using hash (flight_no);
WARNING: hash indexes are not WAL-logged and their use is discouraged
CREATE INDEX

demo = # explain (costs off) select * from flights where flight_no = 'PG0001';
QUERY PLAN
-------------------------------------------------- -
Bitmap Heap Scan on flights
Recheck Cond: (flight_no = 'PG0001' :: bpchar)
-> Bitmap Index Scan on flights_flight_no_idx
Index Cond: (flight_no = 'PG0001' :: bpchar)
(4 rows)

An unpleasant feature of the current implementation of a hash index is that actions with it are not written to the write-ahead log 
(which is what PostgreSQL warns us about when creating an index). As a consequence, 
hash indexes cannot be recovered from a failure and do not participate in replication. In addition, 
the hash index is significantly inferior to the B-tree in terms of versatility, and its effectiveness also raises questions. 
That is, there is no practical sense in using such indexes now.

However, the situation will change this fall with the release of the tenth version of PostgreSQL. In it, 
the hash index was finally provided with log support and additionally optimized memory allocation and the efficiency of concurrent work.

Hashing semantics

Why has the hash index survived almost from the very birth of PostgreSQL to the present day in a state in which it cannot be used? 
The fact is that the hashing algorithm is used very widely in the DBMS (in particular, for hash joins and groupings), 
and the system needs to know which hash function to apply to which data types. But this mapping is not static, 
it cannot be set once and for all - PostgreSQL allows you to add new types on the fly. Here in the hash access method such a match is contained, 
and it is presented in the form of binding auxiliary functions to operator families:

demo = # select opf.opfname as opfamily_name,
amproc.amproc :: regproc AS opfamily_procedure
from pg_am am,
pg_opfamily opf,
pg_amproc amproc
where opf.opfmethod = am.oid
and amproc.amprocfamily = opf.oid
and am.amname = 'hash'
order by opfamily_name,
opfamily_procedure;

opfamily_name | opfamily_procedure
-------------------- + --------------------
abstime_ops | hashint4
aclitem_ops | hash_aclitem
array_ops | hash_array
bool_ops | hashchar
...

Although not documented, these functions can be used to compute the hash code of a value of the appropriate type. For example, 
the hashtext function is used for the text_ops family:

demo = # select hashtext ('times');
hashtext
-----------
127722028
(1 row)

demo = # select hashtext ('two');
hashtext
-----------
345620034
(1 row)

Properties

Let's see the properties of the hash index that this accessor tells the system about itself. 
We gave requests last time; for now we will limit ourselves only to the results:

name            | pg_indexam_has_property
--------------- + -------------------------
can_order       | f
can_unique      | f
can_multi_col   | f
can_exclude     | t

name            | pg_index_has_property
--------------- + -----------------------
clusterable     | f
index_scan      | t
bitmap_scan     | t
backward_scan   | t

name                 | pg_index_column_has_property
-------------------- + ----------------------------- -
asc                  | f
desc                 | f
nulls_first          | f
nulls_last           | f
orderable            | f
distance_orderable   | f
returnable           | f
search_array         | f
search_nulls         | f

The hash function does not preserve the ordering relation: 
from the fact that the value of the hash function of one key is less than the value of the function of the other key, 
no conclusions can be drawn about how the keys themselves are ordered. Therefore, a hash index, in principle, can support a single "equal" operation:

demo = # select opf.opfname AS opfamily_name,
amop.amopopr :: regoperator AS opfamily_operator
from pg_am am,
pg_opfamily opf,
pg_amop amop
where opf.opfmethod = am.oid
and amop.amopfamily = opf.oid
and am.amname = 'hash'
order by opfamily_name,
opfamily_operator;

opfamily_name   | opfamily_operator
--------------- + ----------------------
abstime_ops     | = (abstime, abstime)
aclitem_ops     | = (aclitem, aclitem)
array_ops       | = (anyarray, anyarray)
bool_ops        | = (boolean, boolean)
...

Accordingly, the hash index cannot produce ordered data (can_order, orderable). For the same reason, 
the hash index does not work with null values: the "equal" operation does not make sense for null (search_nulls).
Since the hash index does not store the keys (but only the hash codes of the keys), it cannot be used for index-only access (returnable).
Multi-column indexes (can_multi_col) are not supported by this accessor.

Insides

Starting with version 10, it will be possible to look into the internal structure of the hash index using the pageinspect extension. This is how it will look:
https://www.postgresql.org/docs/devel/pageinspect.html

demo = # create extension pageinspect;
CREATE EXTENSION

Meta page (get the number of rows in the index and the maximum used cart number):

demo = # select hash_page_type (get_raw_page ('flights_flight_no_idx', 0));
hash_page_type
----------------
metapage
(1 row)
demo = # select ntuples, maxbucket from hash_metapage_info (get_raw_page ('flights_flight_no_idx', 0));
ntuples   | maxbucket
--------- + -----------
33121     | 127
(1 row)

Cart page (get the number of actual rows and rows that can be cleared):

demo = # select hash_page_type (get_raw_page ('flights_flight_no_idx', 1));
hash_page_type
----------------
bucket
(1 row)
demo = # select live_items, dead_items from hash_page_stats (get_raw_page ('flights_flight_no_idx', 1));
live_items   | dead_items
------------ + ------------
407          | 0
(1 row)

And so on. But you can hardly understand the meaning of all the available fields without studying the source code.

---------
- Btree -
---------

Device

The btree index, aka B-tree, is suitable for sorted data. In other words, the greater than, greater than or equal, less than, less than or equal, 
and equal operators must be defined for the data type. Note that the same data can sometimes be sorted in different ways, 
which brings us back to the concept of an operator family.

As always, the B-tree index records are packed into pages. In leaf pages, these records contain indexed data (keys) and table row references (TIDs); 
in internal pages, each record refers to a child index page and contains the minimum key value in that page.

B-trees have several important properties:

 * They are balanced, that is, any leaf page is separated from the root by the same number of internal pages. 
   Therefore, the search for any value takes the same time.
 * They are highly branched, that is, each page (usually 8 KB) contains many (hundreds) of TIDs at once. As a result, the depth of B-trees is shallow; 
   in practice, up to 4–5 for very large tables.
 * The data in the index is sorted in non-decreasing order (both between pages and within each page), 
   and pages at the same level are linked in a bidirectional list. Therefore, 
   we can get an ordered data set by simply traversing the list in one direction or the other, without going back to the root each time.

Here is a schematic example of an index on a single field with integer keys.
https://mega.nz/file/AwkGBYia#EzRFw9sdSBHF7iDZ44tw_t1hEr-_bLXB7GLeYWYN1sk

At the very beginning of the file is a meta page that refers to the root of the index. 
Below the root are internal nodes; the bottom row is leaf pages. Down arrows symbolize links from leaf nodes to table rows (TIDs).

Equality search

Consider finding a value in a tree using the "indexed-field = expression" condition. Let's say we are interested in key 49.
https://mega.nz/file/Y4k2GYZL#YG9rdbyE9V_TWzdfpKGV26RL_VsvgwS7UCBsGQECVRg

The search starts from the root node, and we need to determine which of the child nodes to go down to. Knowing the keys (4, 32, 64) in the root node, 
we thereby understand the ranges of values in the child nodes. Since 32 ≤ 49 <64, we need to descend to the second child node. 
Then the same procedure is repeated recursively until a leaf node is reached, from which the necessary TIDs can already be obtained.

In reality, this seemingly simple procedure is complicated by a number of circumstances. For example, an index may contain non-unique keys, 
and there may be enough identical values that they do not fit on one page. Continuing our example, 
it seems that from the internal node we should go down the link that leads from the value 49. But, as you can see in the picture, 
this way we will skip one of the keys 49 in the previous leaf page. Therefore, when we find an exact key equality on the inner page, 
we have to go down one position to the left, and then look through the index records of the lower level from left to right in search of the key of interest.

(Another difficulty is caused by the fact that during the search, other processes can change the data: the tree can be rebuilt, 
pages can be split into two, etc. All algorithms are built in such a way that these simultaneous actions do not interfere with each other and 
do not require unnecessary we will not go into these details.)

Inequality search

When searching for "indexed-field ≤ expression" (or "indexed-field ≥ expression"), 
first find the value in the index by the equality condition "indexed-field = expression" (if any), 
and then move through the leaf pages to end in the desired direction.
The figure illustrates this process for the condition n ≤ 35:
https://mega.nz/file/goMH0KYa#4LBkWOJoTHqPi7VEaFmkUr-4wBnU7si80TqX1l29_K0

The operators "greater than" and "less" are supported in the same way, you just need to exclude the originally found value.

Range search

When searching in the range "expression1 ≤ indexed-field ≤ expression2", we find the value by the condition "indexed-field = expression1", 
and then move through the leaf pages while the condition "indexed-field ≤ expression2" is satisfied. Or vice versa: 
we start with the second expression and move in the other direction until we reach the first.

The figure shows the process for condition 23 ≤ n ≤ 64:
https://mega.nz/file/4oFVnaCS#6myiocGQ5M4wT4P8FSO8r-Odm-4OnUxAXJembC-XNgY

Example

Let's see how query plans look like with an example. As usual, we will use the demo database and this time we will take the aircraft table. 
There are only nine rows in it, and the planner will not use the index of its own accord - after all, the entire table fits into one page. 
But we are interested in it because of its clarity.

demo = # select * from aircrafts;
aircraft_code   | model                 | range
--------------- + --------------------- + -------
773             | Boeing 777-300        | 11100
763             | Boeing 767-300        | 7900
SU9             | Sukhoi SuperJet-100   | 3000
320             | Airbus A320-200       | 5700
321             | Airbus A321-200       | 5600
319             | Airbus A319-100       | 6700
733             | Boeing 737-300        | 4200
CN1             | Cessna 208 Caravan    | 1200
CR2             | Bombardier CRJ-200    | 2700
(9 rows)

demo = # create index on aircrafts (range);
CREATE INDEX

demo = # set enable_seqscan = off;
SET

(Or explicitly create index on aircrafts using btree (range), but the B-tree is built by default.)

Equality search:

demo = # explain (costs off) select * from aircrafts where range = 3000;
QUERY PLAN
-------------------------------------------------- -
Index Scan using aircrafts_range_idx on aircrafts
Index Cond: (range = 3000)
(2 rows)

Inequality search:

demo = # explain (costs off) select * from aircrafts where range <3000;
QUERY PLAN
-------------------------------------------------- -
Index Scan using aircrafts_range_idx on aircrafts
Index Cond: (range <3000)
(2 rows)

And by range:

demo = # explain (costs off) select * from aircrafts where range between 3000 and 5000;
QUERY PLAN
-------------------------------------------------- ---
Index Scan using aircrafts_range_idx on aircrafts
Index Cond: ((range> = 3000) AND (range <= 5000))
(2 rows)

Sorting

It is worth emphasizing again that for any scan method (index, index-only, bitmap), the btree accessor returns ordered data, 
as can be clearly seen in the above figures.
Therefore, if there is an index on a table by a sort condition, the optimizer will take into account both possibilities: 
accessing the table by index and automatically retrieving sorted data, or sequential reading of the table and then sorting the result.

The sort order

The sort order can be explicitly specified when creating an index. For example, a flight range index could be created like this:

demo = # create index on aircrafts (range desc);

In this case, large values would appear on the left in the tree, and smaller values on the right. 
Why would you need it if you can walk through the indexed values both in one direction and in the other?

The reason is multi-column indices. Let's create a view that will show the aircraft models and the conventional division into short, 
medium and long haul vessels:

demo = # create view aircrafts_v as
select model,
case
when range <4000 then 1
when range <10000 then 2
else 3
end as class
from aircrafts;
CREATE VIEW

demo = # select * from aircrafts_v;
model                 | class
--------------------- + -------
Boeing 777-300        | 3
Boeing 767-300        | 2
Sukhoi SuperJet-100   | 1
Airbus A320-200       | 2
Airbus A321-200       | 2
Airbus A319-100       | 2
Boeing 737-300        | 2
Cessna 208 Caravan    | 1
Bombardier CRJ-200    | 1
(9 rows)

And create an index (using an expression):

demo = # create index on aircrafts (
(case when range <4000 then 1 when range <10000 then 2 else 3 end), model);
CREATE INDEX

We can now use this index to get the data sorted by both columns in ascending order:

demo = # select class, model from aircrafts_v order by class, model;
class   | model
------- + ---------------------
1       | Bombardier CRJ-200
1       | Cessna 208 caravan
1       | Sukhoi SuperJet-100
2       | Airbus A319-100
2       | Airbus A320-200
2       | Airbus A321-200
2       | Boeing 737-300
2       | Boeing 767-300
3       | Boeing 777-300
(9 rows)

demo = # explain (costs off) select class, model from aircrafts_v order by class, model;
QUERY PLAN
-------------------------------------------------- ------
Index Scan using aircrafts_case_model_idx on aircrafts
(1 row)

Similarly, you can execute a query sorted in descending order:

demo = # select class, model from aircrafts_v order by class desc, model desc;
class   | model
------- + ---------------------
3       | Boeing 777-300
2       | Boeing 767-300
2       | Boeing 737-300
2       | Airbus A321-200
2       | Airbus A320-200
2       | Airbus A319-100
1       | Sukhoi SuperJet-100
1       | Cessna 208 caravan
1       | Bombardier CRJ-200
(9 rows)

demo = # explain (costs off)
select class, model from aircrafts_v order by class desc, model desc;
QUERY PLAN
-------------------------------------------------- ---------------
Index Scan Backward using aircrafts_case_model_idx on aircrafts
(1 row)

But it is not possible to retrieve data from this index sorted in descending order by one column and ascending by the other. 
This requires a separate sorting:

demo = # explain (costs off)
select class, model from aircrafts_v order by class asc, model desc;
QUERY PLAN
-------------------------------------------------
Sort
Sort Key: (CASE ... END), aircrafts.model DESC
-> Seq Scan on aircrafts
(3 rows)

(Note that out of grief, the planner chose to scan the table, even though the enable_seqscan = off setting was made earlier. 
This is because it does not actually disable table scans, but only sets it at a prohibitive cost - see the "costs on" plan. )

For such a query to be executed using an index, the index must be built with sorting in the desired order:

demo = # create index aircrafts_case_asc_model_desc_idx on aircrafts (
(case when range <4000 then 1 when range <10000 then 2 else 3 end) asc, model desc);
CREATE INDEX

demo = # explain (costs off)
select class, model from aircrafts_v order by class asc, model desc;
QUERY PLAN
-------------------------------------------------- ---------------
Index Scan using aircrafts_case_asc_model_desc_idx on aircrafts
(1 row)

Column order

Another issue that arises when using multi-column indexes is the order in which the columns are listed in the index. 
In the case of a B-tree, this order is of great importance: the data within the pages will be sorted first by the first field, then by the second, and so on.
The index that we have built on the basis of range intervals and models can be roughly represented as follows:
https://mega.nz/file/18kjDACJ#-XW86IeTpistZ6NpktznoL6_XZOb4BBBAnMHeuk51Ko

Of course, such a small index will actually fit into one root page; in the figure, it is artificially distributed over several pages for clarity.

It is clear from this diagram that search will work efficiently with, for example, 
predicates such as "class = 3" (search only in the first field) or "class = 3 and model = 'Boeing 777-300'" (search in both fields ).

But the search by the predicate "model = 'Boeing 777-300'" will be much less efficient: starting from the root, 
we cannot determine which of the child nodes to go down to, so we will have to go down into all of them. 
This does not mean that such an index cannot be used in principle - the only question is efficiency. 
For example, if we had three classes of aircraft and very many models in each class, then we would have to scan about a third of the index, 
and this might be more efficient than a full scan of the table. It might not have happened.

But if you create an index like this:

demo = # create index on aircrafts (
model, (case when range <4000 then 1 when range <10000 then 2 else 3 end));
CREATE INDEX

then the order of the fields will change:
https://mega.nz/file/t8tj0SiC#Q35bEXEb5bDPS-dhzpa_wHj2S8IwEfugAaELi4SGppw
And with such an index, the search by the predicate "model = 'Boeing 777-300'" will be performed efficiently, but by the predicate "class = 3" it will not.

Undefined values

The btree accessor indexes undefined values ​​and supports is null and is not null searches.

Let's take a table of flights with undefined values:

demo = # create index on flights (actual_arrival);
CREATE INDEX
demo = # explain (costs off) select * from flights where actual_arrival is null;
QUERY PLAN
-------------------------------------------------- -----
Bitmap Heap Scan on flights
Recheck Cond: (actual_arrival IS NULL)
-> Bitmap Index Scan on flights_actual_arrival_idx
Index Cond: (actual_arrival IS NULL)
(4 rows)

Undefined values are positioned at one end or the other of leaf nodes, depending on how the index was created (nulls first or nulls last). 
This is important if the query involves sorting: the order of undefined values ​​in the index and in the sort order must match for the index to be usable.
In this example, the orders are the same, so the index can be used:

demo = # explain (costs off) select * from flights order by actual_arrival nulls last;
QUERY PLAN
-------------------------------------------------- ------
Index Scan using flights_actual_arrival_idx on flights
(1 row)

But here the orders are different, and the optimizer chooses to scan the table and sort:

demo = # explain (costs off) select * from flights order by actual_arrival nulls first;
QUERY PLAN
----------------------------------------
Sort
Sort Key: actual_arrival NULLS FIRST
-> Seq Scan on flights
(3 rows)

For the index to be used, you need to create it so that undefined values come at the beginning:

demo = # create index flights_nulls_first_idx on flights (actual_arrival nulls first);
CREATE INDEX
demo = # explain (costs off) select * from flights order by actual_arrival nulls first;
QUERY PLAN
-------------------------------------------------- ---
Index Scan using flights_nulls_first_idx on flights
(1 row)

The reason for such discrepancies, of course, is that undefined values are not sortable: 
the result of comparing an undefined value with any other is undefined:

demo = # \ pset null NULL
Null display is "NULL".
demo = # select null <42;
? column?
----------
NULL
(1 row)

This goes against the essence of the B-tree and does not fit into the general scheme. 
But undefined values play such an important role in databases that exceptions must be made for them all the time.

The consequence of the fact that undefined values are indexed is the ability to use the index even if no conditions are imposed on the table at all 
(since the index is guaranteed to contain information about all the rows of the table). 
This can make sense if you want to order the data in your query and the index provides the desired order. 
Then the planner may prefer index access to save on separate sorting.



