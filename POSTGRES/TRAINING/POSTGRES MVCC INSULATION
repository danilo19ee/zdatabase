#############################
# POSTGRES MVCC INSULATION  #
#############################

* Isolation as understood by the standard and PostgreSQL (this article);
* Layers, files, pages - what's going on at the physical level;
* Row Versions, Virtual and Nested Transactions;
* Snapshots and row version visibility, event horizon;
* In-page cleaning and HOT updates;
* Routine vacuum;
* Automatic cleaning (autovacuum);
* Transaction counter overflow and freeze;

What Is Insulation And Why It Matters

Probably, everyone at least knows about the existence of transactions, met the abbreviation ACID and heard about isolation levels. 
But we still have to meet the opinion that this is de theory, which is not necessary in practice. So I'll spend some time trying to explain why this really matters.
You are unlikely to be happy if the application receives incorrect data from the database, or if the application writes incorrect data to the database.
But what is “correct” data? It is known that at the database level can create integrity constraints (integrity constraints, such as NOT NULL or UNIQUE). 
If the data always satisfies the integrity constraints (and this is so, because the DBMS guarantees it), then it is complete.

Correct and holistic- same? Not really. Not all constraints can be formulated at the database level. Some of the restrictions are too complex, for example, 
they cover several tables at once. And even if the constraint could in principle be defined in the database, but for some reason they did not do it, 
this does not mean that it can be violated.

So, correctness is stricter than integrity , but what it is, we do not know exactly. It remains to admit that the standard of correctness is an application that, 
as we would like to believe, is written correctly and never makes mistakes. In any case, if the application does not violate the integrity, but violates the correctness, 
the DBMS will not know about it and will not catch its hand.

In what follows, we will call correctness by the termcoherence (consistency).
Let's assume, however, that the application only executes the correct sequences of statements. What then is the role of the DBMS if the application is already correct?
First, it turns out that the correct sequence of statements can temporarily break the data consistency, 
and this - oddly enough - is normal. A hackneyed but understandable example is the transfer of funds from one account to another. 
The consistency rule might sound like this: the transfer never changes the total amount of money in the accounts
(Such a rule is rather difficult to write in SQL as an integrity constraint, so it exists at the application level and is invisible to the DBMS). 
The transfer consists of two operations: the first decreases funds on one account, the second increases on another. The first operation breaks the data consistency, 
the second restores.

A good exercise is to implement the above rule at the integrity constraint level. Are you weak?
What if the first operation succeeds and the second fails? After all, it's easy: during the second operation, electricity may disappear, 
the server may fall, division by zero may occur - but you never know. It is clear that consistency will be violated, and this cannot be allowed. 
In principle, it is possible to resolve such situations at the application level at the cost of incredible efforts, but, fortunately, it is not necessary: 
the DBMS takes care of it. But for this she must know that the two operations make up an indivisible whole. That is, a transaction .
It turns out to be interesting: knowing that operations constitute a transaction, the DBMS helps maintain consistency by guaranteeing the atomicity of transactions, 
while not knowing anything about specific consistency rules.
But there is also a second, more subtle point. As soon as several simultaneously working transactions appear in the system, which are absolutely correct one at a time, 
together they may work incorrectly. This is due to the fact that the order of operations is mixed: 
it cannot be assumed that all the operations of one transaction are performed first, and only then all the operations of another.
A note about simultaneity. Indeed, transactions can work simultaneously on a system with a multi-core processor, with a disk array, etc. 
But all the same reasoning is also true for a server that executes commands sequentially, in a time-sharing mode: one transaction is executed so many cycles, 
another one ... The term concurrent execution is sometimes used to summarize .
Situations where valid transactions do not work together correctly are called concurrency anomalies .
A simple example: if an application wants to get the correct data from the database, then at least it should not see changes in other uncommitted transactions. 
Otherwise, you can not only get inconsistent data, but also see something that has never been in the database (if the transaction is canceled). 
This anomaly is called dirty reading .

If there are other, more complex anomalies, which we will deal with a little later.
Of course, you cannot refuse concurrent execution: otherwise, what kind of performance can we talk about? But you can’t work with incorrect data either.
And again the DBMS comes to the rescue. You can make transactions run as if sequentially, as if one after the other. In other words, it is isolated from each other. 
In reality, the DBMS can perform intermixing operations, 
but at the same time guarantee that the result of the simultaneous execution will match the result of any of the possible sequential executions. 
This removes any possible anomalies.

So, we come to the definition:

A transaction is a set of operations performed by an application that moves a database from one correct state to another correct state (consistency), 
provided that the transaction is complete (atomicity) and without interference from other transactions (isolation).

This definition combines the first three letters of ACID. They are so closely related to each other that there is simply no point in considering one without the other. 
In fact, it is difficult to tear off the letter D (durability). After all, when a system crashes, changes in uncommitted transactions remain in it, 
with which you have to do something to restore data consistency.

Everything would be fine, but the implementation of complete isolation is a technically difficult task, associated with a decrease in system throughput. 
Therefore, in practice, very often (not always, but almost always) weakened isolation is used, which prevents some, but not all, anomalies. 
This means that part of the work to ensure the correctness of the data falls on the application. 
That is why it is very important to understand what isolation level is used in the system, what guarantees it gives and what it does not, 
and how to write correct code under such conditions.

------------------------------------------------------
- Isolation levels and anomalies in the SQL standard -
------------------------------------------------------

The SQL standard has long described four isolation levels. 
These levels are determined by enumerating anomalies that are allowed or not allowed when transactions are being executed at this level at the same time. 
Therefore, to talk about these levels, you need to get acquainted with anomalies.

Let me emphasize that in this part we are talking about a standard, that is, about a certain theory on which practice relies heavily, 
but from which at the same time it is quite at odds with. Therefore, all examples here are speculative. 
They will use the same operations on customer accounts: this is quite obvious, although, admittedly, 
it has nothing to do with how banking operations are arranged in reality

---------------
- Lost update -
---------------

Let's start with the lost update . This anomaly occurs when two transactions read the same row in a table, then one transaction updates that row, 
and then the second transaction also updates the same row, ignoring the changes made by the first transaction.

For example, two transactions are going to increase the amount on the same account by 100 ₽. The first transaction reads the current value (1000 ₽), 
then the second transaction reads the same value. The first transaction increases the amount (it turns out 1100 ₽) and writes this value. 
The second transaction does the same - it receives the same 1100 ₽ and writes them down. As a result, the client lost 100 ₽.

A lost update is not allowed by the standard at any isolation level.

-----------------------------------
- Dirty Read and Read Uncommitted -
-----------------------------------

We already got acquainted with dirty reading above. This anomaly occurs when a transaction reads uncommitted changes made by another transaction.
For example, the first transaction transfers all the money from the customer's account to another account, but does not commit the change. 
Another transaction reads the account balance, receives $ 0, 
and refuses to withdraw cash from the client - even though the first transaction is aborted and reverses its changes, so the value 0 never existed in the database.
Dirty reads are allowed by the standard at the Read Uncommitted level.

------------------------------------------
- Non-repetitive Read and Read Committed -
------------------------------------------

A nonrepeatable read anomaly occurs when a transaction reads the same row twice, and between reads, a second transaction modifies (or deletes) that row and commits the changes. Then the first transaction will get different results.
For example, suppose a consistency rule prohibits negative amounts in customer accounts. 
The first transaction is going to reduce the amount on the account by 100 ₽. She checks the current value, gets 1000 ₽ and decides that a decrease is possible. 
At this time, the second transaction reduces the amount on the account to zero and commits the changes. If now the first transaction re-checked the amount, 
it would receive 0 ₽ (but it has already decided to decrease the value, and the account “goes into negative territory”).
Non-repetitive reads are allowed by the standard at the Read Uncommitted and Read Committed levels. But Read Committed does not allow dirty reading.

------------------------------------
- Phantom Read and Repeatable Read -
------------------------------------

A phantom read occurs when a transaction reads a set of rows twice for the same condition, and between reads, 
the second transaction adds rows that satisfy this condition (and commits the changes). Then the first transaction will receive different sets of rows.
For example, suppose a consistency rule prohibits a customer from having more than 3 accounts . The first transaction is about to open a new account, 
checks their current number (say 2) and decides that opening is possible. At this time, the second transaction also opens a new account for the client and fixes the changes. 
If now the first transaction rechecked the quantity, it would receive 3 (but it is already opening another account and the client has 4).
Phantom reading is allowed by the standard at the Read Uncommitted, Read Committed and Repeatable Read levels. But at the Repeatable Read level, 
non-repeated reads are not allowed.

--------------------------------------
- Lack of anomalies and Serializable -
--------------------------------------

The standard also defines another level - Serializable - at which no anomalies are allowed. This is not at all the same as prohibiting lost updates and dirty, 
non-repetitive and phantom reading.
The fact is that there are significantly more known anomalies than are listed in the standard, and an unknown number are still unknown.
The Serializable layer should prevent all anomalies in general . This means that at this level, the application developer does not need to think about concurrency. 
If the transactions execute the correct sequence of statements, working alone, the data will be consistent even when these transactions are running simultaneously.

-----------------
- Summary plate -
-----------------

Now you can give a well-known table. But here, for the sake of clarity, the last column is added to it, which is not in the standard.

------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+
	                |   lost changes  |   dirty reading   |   non-repetitive reading    | 	phantom reading	  |   other anomalies |
------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+                  
Read Uncommitted	|     -           |     yes           |     yes                     |     yes             |     yes           |
Read Committed    |     -           |       -           |     yes                     |     yes             |     yes           |
Repeatable Read   |     -           |       -           |       -                     |     yes             |     yes           |
Serializable	    |     -           |       -           |       -                     |     -               |     yes           |
------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+

---------------------------------
- Why exactly these anomalies?  -
---------------------------------

Why, of the many possible anomalies in the standard, only a few are listed and why exactly such?
Probably, nobody knows for certain. But practice here definitely outstripped theory, 
so it is possible that other anomalies were not thought of at that time (talking about the SQL: 92 standard).
In addition, it was assumed that the isolation should be built on locks. 
The idea behind a widely used two-phase blocking protocol(2PL) is that during execution the transaction locks the rows it is working with, 
and upon completion it releases the locks. To simplify a lot, the more locks a transaction acquires, the better it is isolated from other transactions. 
But the performance of the system suffers the more, because instead of working together, transactions begin to line up in a queue for the same lines.
It seems to me that the difference between the isolation levels of the standard is due precisely to the number of required locks.
If the transaction locks the modified rows from being modified, but not from being read, we get the Read Uncommitted level: no lost changes are allowed, 
but uncommitted data can be read.
If a transaction locks mutable rows from both read and change, we get the Read Committed level: uncommitted data cannot be read, but when the row is re-accessed, 
a different value can be obtained (non-repeated read).
If a transaction locks both readable and modified rows from both reading and modification, we get the Repeatable Read level: rereading the row will return the same value.
But there is a problem with Serializable: it is impossible to lock a row that does not exist yet. Because of this, 
the possibility of a phantom reading remains: another transaction can add (but not delete) a row that matches the conditions of a previously executed query, 
and this row will be resampled.

Therefore, to implement the Serializable level, ordinary locks are not enough - you need to lock not rows, but conditions (predicates). 
These locks were called predicate locks . They were proposed back in 1976, 
but their practical applicability is limited to rather simple conditions for which it is clear how to combine two different predicates. As far as I know, 
it did not come to the implementation of such locks in any system.

-----------------------------------
- Isolation levels in PostgreSQL  -
-----------------------------------

Over time, replace the locking transaction management protocols came isolation protocol based on images (Snapshot Isolation). 
Its idea is that each transaction operates with a consistent snapshot of data at a specific point in time, 
which includes only those changes that were committed before the moment the snapshot was taken.

This isolation automatically prevents dirty reads. Formally, in PostgreSQL you can specify the Read Uncommitted level, 
but it will work exactly the same as Read Committed. Therefore, we will not talk about the Read Uncommitted level further.

PostgreSQL implements multiversiona variant of such a protocol. The idea of multiversion is that several versions of the same row can coexist in a DBMS. 
This allows you to build a snapshot of the data using the existing versions and get by with a minimum of blocking. In fact, 
only repeated changes to the same row are blocked. All other operations are performed concurrently: writers never block read transactions, and readers never block anyone.

Due to the use of snapshots of data, isolation in PostgreSQL is stricter than required by the standard: the Repeatable Read level does not allow not only non-repeatable, 
but also phantom reads (although it does not provide complete isolation). And this is achieved without loss of efficiency.

------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+
	                |   lost changes  |   dirty reading   |   non-repetitive reading    | 	phantom reading	  |   other anomalies |
------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+
Read Uncommitted	|     -           |       -           |     yes                     |     yes             |     yes           |
Read Committed    |     -           |       -           |     yes                     |     yes             |     yes           |
Repeatable Read   |     -           |       -           |       -                     |     -               |     yes           |
Serializable	    |     -           |       -           |       -                     |     -               |     -             |
------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+

We will talk about how multiversion is implemented “under the hood” in the following articles, 
and now we will take a closer look at each of the three levels through the eyes of the user (as you understand, the most interesting is hidden behind “other anomalies”). 
To do this, we will create a table of accounts. Alice and Bob have 1,000 rubles each, but Bob has two accounts:

=> CREATE TABLE accounts(
  id integer PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
  number text UNIQUE,
  client text,
  amount numeric
);
=> INSERT INTO accounts VALUES
  (1, '1001', 'alice', 1000.00),
  (2, '2001', 'bob', 100.00),
  (3, '2002', 'bob', 900.00);

-------------------
- Read Committed  -
-------------------

No dirty reading

It is easy to verify that dirty data cannot be read. Let's start a transaction. By default, it will use the Read Committed isolation level:

=> BEGIN;
=> SHOW transaction_isolation;
 transaction_isolation 
-----------------------
 read committed
(1 row)

More precisely, the default level is set by a parameter, it can be changed if necessary:

=> SHOW default_transaction_isolation;
 default_transaction_isolation 
-------------------------------
 read committed
(1 row)

So, in an open transaction, we withdraw funds from the account, but we do not commit the changes. The transaction sees its own changes:

=> UPDATE accounts SET amount = amount - 200 WHERE id = 1;
=> SELECT * FROM accounts WHERE client = 'alice';
 id | number | client | amount 
----+--------+--------+--------
  1 | 1001   | alice  | 800.00
(1 row)

In the second session, let's start another transaction with the same Read Committed level. To distinguish between different transactions, 
the commands for the second transaction will be shown indented and strikethrough.

In order to repeat the above commands (which is useful), you need to open two terminals and run psql in each. In the first one, 
you can enter commands from one transaction, and in the second, commands from another.

|  => BEGIN;
|  => SELECT * FROM accounts WHERE client = 'alice';
|   id | number | client | amount  
|  ----+--------+--------+---------
|    1 | 1001   | alice  | 1000.00
|  (1 row)

As expected, the other transaction does not see uncommitted changes — dirty reads are not allowed.

---------------------------
- Non-repetitive reading  -
---------------------------

Now let the first transaction commit the changes, and the second will re-execute the same query.

=> COMMIT;

|  => SELECT * FROM accounts WHERE client = 'alice';
|   id | number | client | amount 
|  ----+--------+--------+--------
|    1 | 1001   | alice  | 800.00
|  (1 row)
|  => COMMIT;

The request already receives new data - this is the non-repeatable read anomaly , which is allowed at the Read Committed level.
Practical conclusion : in a transaction, 
you cannot make decisions based on the data read by the previous operator - because everything can change during the time between the execution of the operators. 
Here's an example, the variations of which are so common in application code that it is a classic anti-pattern:

IF (SELECT amount FROM accounts WHERE id = 1) >= 1000 THEN
        UPDATE accounts SET amount = amount - 1000 WHERE id = 1;
      END IF;
      
During the time that elapses between the check and the update, other transactions can change the state of the account in any way, 
so that such a "check" does not save you from anything. 
It is convenient to imagine that between operators of one transaction any other operators of other transactions can "wedge in", for example, like this:      

IF (SELECT amount FROM accounts WHERE id = 1) >= 1000 THEN
       -----
      |   UPDATE accounts SET amount = amount - 200 WHERE id = 1;
      |   COMMIT;
       -----
        UPDATE accounts SET amount = amount - 1000 WHERE id = 1;
      END IF;

If, by rearranging the operators, you can spoil everything, then the code is written incorrectly. 
And do not deceive yourself that such a combination of circumstances will not happen - it will definitely happen.

How to write the code correctly? The possibilities usually boil down to the following:

  * Don't write code.
    It is not joke. For example, in this case, the check easily turns into an integrity constraint:
    ALTER TABLE accounts ADD CHECK amount >= 0;
    Now no checks are needed: you just need to perform an action and, if necessary, handle the exception that will be thrown in the event of an attempt to violate integrity.

  * Use one SQL statement.
    Consistency issues arise because in between statements another transaction can complete and the visible data changes. 
    And if there is only one operator, then there are no gaps.
    PostgreSQL has enough tools to solve complex problems with a single SQL statement. Note the general table expressions (CTE), in which, 
    among other things, you can use the INSERT / UPDATE / DELETE statements, as well as the INSERT ON CONFLICT statement, which implements the "insert, 
    and if the row already exists, update" logic in one statement.

  * Custom locks.
    The last resort is to manually set an exclusive lock either on all required rows (SELECT FOR UPDATE), or on the entire table (LOCK TABLE). 
    This always works, but negates the advantages of multi-versioning: instead of being executed simultaneously, some of the operations will be performed sequentially.

---------------------
- Inconsistent read -
---------------------

Before moving on to the next level of isolation, you have to admit that it is not so simple. PostgreSQL's implementation is such that it allows for other, 
lesser known anomalies that are not specified in the standard.

Let's say the first transaction starts transferring funds from one of Bob's accounts to another:

=> BEGIN;
=> UPDATE accounts SET amount = amount - 100 WHERE id = 2;

At this time, another transaction calculates Bob's balance, and the calculation is performed in a loop through all of Bob's accounts. 
In fact, the transaction starts from the first account (and obviously sees the previous state):

|  => BEGIN;
|  => SELECT amount FROM accounts WHERE id = 2;
|   amount 
|  --------
|   100.00
|  (1 row)

At this point, the first transaction completes successfully:

=> UPDATE accounts SET amount = amount + 100 WHERE id = 3;
=> COMMIT;

And the other reads the state of the second account (and already sees the new value):

|  => SELECT amount FROM accounts WHERE id = 3;
|   amount  
|  ---------
|   1000.00
|  (1 row)
|  => COMMIT;

Thus, the second transaction received a total of 1100 ₽, that is, incorrect data. This is the inconsistent read anomaly .

How to avoid such an anomaly while staying at the Read Committed level? Of course, use one operator. For example like this:

      SELECT sum(amount) FROM accounts WHERE client = 'bob';

So far I have argued that data visibility can only change between operators, but is that obvious? And if the request takes a long time, 
can it see some of the data in one state, and some in another?

Let's check. A convenient way to do this is to insert an artificial delay into the statement by calling the pg_sleep function. 
Its parameter sets the delay time in seconds.

=> SELECT amount, pg_sleep(2) FROM accounts WHERE client = 'bob';

While this construction is being executed, in another transaction we transfer funds back:

|  => BEGIN;
|  => UPDATE accounts SET amount = amount + 100 WHERE id = 2;
|  => UPDATE accounts SET amount = amount - 100 WHERE id = 3;
|  => COMMIT;

The result shows that the operator sees the data in the same state in which it was at the time of the start of its execution. This is definitely correct.

 amount  | pg_sleep 
---------+----------
    0.00 | 
 1000.00 | 
(2 rows)

But even here it is not so simple. PostgreSQL allows you to define functions, and functions have the concept of a mutability category. 
If a volatile function is called in a request (with the VOLATILE category), and another request is executed in this function, 
then this request inside the function will see data that is not consistent with the data of the main request.

=> CREATE FUNCTION get_amount(id integer) RETURNS numeric AS $$
  SELECT amount FROM accounts a WHERE a.id = get_amount.id;
$$ VOLATILE LANGUAGE sql;

=> SELECT get_amount(id), pg_sleep(2)
FROM accounts WHERE client = 'bob';

|  => BEGIN;
|  => UPDATE accounts SET amount = amount + 100 WHERE id = 2;
|  => UPDATE accounts SET amount = amount - 100 WHERE id = 3;
|  => COMMIT;

In this case, we will receive incorrect data - 100 ₽ lost:

 get_amount | pg_sleep 
------------+----------
     100.00 | 
     800.00 | 
(2 rows)

Let me emphasize that this effect is possible only at the Read Committed isolation level, and only with the VOLATILE volatility category. 
The trouble is that this is the isolation level and this particular category of variability is used by default, so I must admit that the rake is very good. Don't step on!

---------------------------------------------------
- Inconsistent read in exchange for lost changes  -
---------------------------------------------------

Inconsistent reads within a single statement can — in a somewhat unexpected way — also occur during an update.
Let's see what happens when two transactions try to update the same row. Now Bob has 1,000 rubles on two accounts:

=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount 
----+--------+--------+--------
  2 | 2001   | bob    | 200.00
  3 | 2002   | bob    | 800.00
(2 rows)

Beginning a transaction that reduces Bob's balance:

=> BEGIN;
=> UPDATE accounts SET amount = amount - 100 WHERE id = 3;

At the same time, another transaction charges interest on all customer accounts with a total balance equal to or greater than RUB 1,000:

|  => UPDATE accounts SET amount = amount * 1.01
|  WHERE client IN (
|    SELECT client
|    FROM accounts
|    GROUP BY client
|    HAVING sum(amount) >= 1000
|  );

There are two parts to executing an UPDATE statement. First, a SELECT is actually executed, which selects the rows that match the condition for updating. 
Since the change in the first transaction is not committed, 
the second transaction cannot see it and it does not in any way affect the selection of rows for interest calculation. 
So, Bob's accounts fall under the condition and after the update is completed, his balance should increase by 10 ₽.

The second stage of execution - the selected rows are updated one after another. 
Here the second transaction is forced to "hang" because the id = 3 row is already locked by the first transaction.

Meanwhile, the first transaction commits the changes:

=> COMMIT;

What will be the result?

=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client |  amount  
----+--------+--------+----------
  2 | 2001   | bob    | 202.0000
  3 | 2002   | bob    | 707.0000
(2 rows)

Yes, on the one hand, the UPDATE command should not see the changes in the second transaction. 
But on the other hand, it should not lose the changes committed in the second transaction.

After the lock is released, UPDATE rereads the row it is trying to update (but only one!). As a result, it turns out that Bob was credited with 9 ₽, 
based on the amount of 900 ₽. But if Bob had 900 rubles, his accounts should not have been included in the sample at all.

So, the transaction receives incorrect data: some of the rows are visible at one point in time, some - at another. In return for the lost update, 
we again get an inconsistent read anomaly .

Astute readers will note that with some help from the application at the Read Committed level, it is possible to get a lost update. For example like this:

      x := (SELECT amount FROM accounts WHERE id = 1);
      UPDATE accounts SET amount = x + 100 WHERE id = 1;

The database is not at fault: it receives two SQL statements and does not know anything about the fact that the value x + 100 has something to do with accounts.amount. 
Don't write your code this way.

-------------------
- Repeatable Read -
-------------------

No non-repetitive and phantom readings

The very name of the isolation level indicates that the read is repeatable. 
Let's check this, and at the same time make sure that there are no phantom readings. 
To do this, in the first transaction, return Bob's accounts to their previous state and create a new account for Charlie:

=> BEGIN;
=> UPDATE accounts SET amount = 200.00 WHERE id = 2;
=> UPDATE accounts SET amount = 800.00 WHERE id = 3;
=> INSERT INTO accounts VALUES
  (4, '3001', 'charlie', 100.00);
=> SELECT * FROM accounts ORDER BY id;
 id | number | client  | amount 
----+--------+---------+--------
  1 | 1001   | alice   | 800.00
  2 | 2001   | bob     | 200.00
  3 | 2002   | bob     | 800.00
  4 | 3001   | charlie | 100.00
(4 rows)

In the second session, start a transaction with the Repeatable Read level, specifying it in the BEGIN command (the level of the first transaction is not important).

|  => BEGIN ISOLATION LEVEL REPEATABLE READ;
|  => SELECT * FROM accounts ORDER BY id;
|   id | number | client |  amount  
|  ----+--------+--------+----------
|    1 | 1001   | alice  |   800.00
|    2 | 2001   | bob    | 202.0000
|    3 | 2002   | bob    | 707.0000
|  (3 rows)

Now the first transaction commits the changes, and the second reruns the same request.

=> COMMIT;

|  => SELECT * FROM accounts ORDER BY id;
|   id | number | client |  amount  
|  ----+--------+--------+----------
|    1 | 1001   | alice  |   800.00
|    2 | 2001   | bob    | 202.0000
|    3 | 2002   | bob    | 707.0000
|  (3 rows)
|  => COMMIT;

The second transaction continues to see exactly the same data as at the beginning: neither changes in existing rows nor new rows are visible.
At this level, you don't have to worry about changing something between the two operators.

----------------------------------------------------
- Serialization error in exchange for lost changes -
----------------------------------------------------

Above, we talked about the fact that when two transactions update the same row at the Read Committed level, an inconsistent read anomaly may occur. 
This is because a pending transaction reads a locked row and thus sees it at a different point in time than the rest of the rows.

At the Repeatable Read level, such an anomaly is not allowed, but if it does occur, nothing can be done - so the transaction is aborted with a serialization error. 
Let's check by repeating the same scenario with percentages:

=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount 
----+--------+--------+--------
  2 | 2001   | bob    | 200.00
  3 | 2002   | bob    | 800.00
(2 rows)
=> BEGIN;
=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;

|  => BEGIN ISOLATION LEVEL REPEATABLE READ;
|  => UPDATE accounts SET amount = amount * 1.01
|  WHERE client IN (
|    SELECT client
|    FROM accounts
|    GROUP BY client
|    HAVING sum(amount) >= 1000
|  );

=> COMMIT;

|  ERROR:  could not serialize access due to concurrent update
|  => ROLLBACK;

The data remained consistent:

=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount 
----+--------+--------+--------
  2 | 2001   | bob    | 200.00
  3 | 2002   | bob    | 700.00
(2 rows)

The same error will occur in the case of any other concurrent row change, even if the columns of interest to us have not actually changed.

The practical takeaway is that if an application uses the Repeatable Read isolation level for writing transactions, 
it must be prepared to repeat transactions that failed serialization. This is not possible for read-only transactions.

-----------------------
- Inconsistent record -
-----------------------

So, PostgreSQL prevents all anomalies described in the standard at the Repeatable Read isolation level. 
But not all at all. It turns out there are exactly two anomalies that remain possible.
(This is true not only of PostgreSQL, but also of other snapshot-based isolation implementations.)

The first of these anomalies is an inconsistent write .

Let the following rule of consistency apply: negative amounts on the accounts of a client are allowed if the total amount on all accounts of this client remains non-negative .

The first transaction receives the amount on Bob's accounts: 900 ₽.

=> BEGIN ISOLATION LEVEL REPEATABLE READ;
=> SELECT sum(amount) FROM accounts WHERE client = 'bob';
  sum   
--------
 900.00
(1 row)

The second transaction receives the same amount.

|  => BEGIN ISOLATION LEVEL REPEATABLE READ;
|  => SELECT sum(amount) FROM accounts WHERE client = 'bob';
|    sum   
|  --------
|   900.00
|  (1 row)

The first transaction rightly believes that the amount of one of the accounts can be reduced by 600 ₽.

=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 2;

And the second transaction comes to the same conclusion. But reduces another score:

|  => UPDATE accounts SET amount = amount - 600.00 WHERE id = 3;
|  => COMMIT;

=> COMMIT;
=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount  
----+--------+--------+---------
  2 | 2001   | bob    | -400.00
  3 | 2002   | bob    |  100.00
(2 rows)

We managed to take Bob's balance into the negative, although each of the transactions works correctly one by one.

---------------------------------
- Read-only transaction anomaly -
---------------------------------

This is the second and final anomaly possible at the Repeatable Read level. To demonstrate it, three transactions are required, two of which will modify data and the third will only read.

But first, let's restore Bob's account state:

=> UPDATE accounts SET amount = 900.00 WHERE id = 2;
=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount
----+--------+--------+--------
  3 | 2002   | bob    | 100.00
  2 | 2001   | bob    | 900.00
(2 rows)

The first transaction charges Bob interest on the amount of funds in all accounts. Interest is credited to one of his accounts:

=> BEGIN ISOLATION LEVEL REPEATABLE READ; -- 1
=> UPDATE accounts SET amount = amount + (
  SELECT sum(amount) FROM accounts WHERE client = 'bob'
) * 0.01
WHERE id = 2;

Then another transaction withdraws money from Bob's other account and commits its changes:

|  => BEGIN ISOLATION LEVEL REPEATABLE READ; -- 2
|  => UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;
|  => COMMIT;

If at this moment the first transaction is committed, no anomaly will arise: we could assume that the first transaction was performed first, 
and then the second (but not vice versa, because the first transaction saw the state of the account id = 3 before this account was changed by the second transaction).

But let's imagine that at this moment the third (read-only) transaction begins, which reads the state of some account not affected by the first two transactions:

|  => BEGIN ISOLATION LEVEL REPEATABLE READ; -- 3
|  => SELECT * FROM accounts WHERE client = 'alice';
|   id | number | client | amount 
|  ----+--------+--------+--------
|    1 | 1001   | alice  | 800.00
|  (1 row)

And only after that, the first transaction is completed:

=> COMMIT;

What state should the third transaction see now?

|    SELECT * FROM accounts WHERE client = ‘bob’;

Once started, the third transaction could see changes to the second transaction (which had already been committed), 
but not the first (which had not yet been committed). On the other hand, 
we have already established above that the second transaction should be considered to have started after the first. 
Whatever state the third transaction sees will be inconsistent - this is a read-only transaction anomaly. But at the Repeatable Read level, it is allowed:

|    id | number | client | amount
|   ----+--------+--------+--------
|     2 | 2001   | bob    | 900.00
|     3 | 2002   | bob    |   0.00
|   (2 rows)
|   => COMMIT;

-----------------
- Serializable  -
-----------------

All possible anomalies are prevented at the Serializable level. In fact, Serializable is implemented as an add-on over snapshot-based isolation. 
Anomalies that do not occur with Repeatable Read (such as dirty, non-repeatable, phantom reads) do not occur at the Serializable level either. 
And those anomalies that arise (inconsistent write and anomaly of only a read transaction) 
are detected and the transaction is aborted - the already familiar could not serialize access serialization error occurs.

Inconsistent record

To illustrate, let's repeat the scenario with the inconsistent write anomaly:

=> BEGIN ISOLATION LEVEL SERIALIZABLE;
=> SELECT sum(amount) FROM accounts WHERE client = 'bob';
   sum    
----------
 910.0000
(1 row)

|   => BEGIN ISOLATION LEVEL SERIALIZABLE;
|   => SELECT sum(amount) FROM accounts WHERE client = 'bob';
|      sum    
|   ----------
|    910.0000
|   (1 row)

=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 2;

|   => UPDATE accounts SET amount = amount - 600.00 WHERE id = 3;
|   => COMMIT;

=> COMMIT;
ERROR:  could not serialize access due to read/write dependencies among transactions
DETAIL:  Reason code: Canceled on identification as a pivot, during commit attempt.
HINT:  The transaction might succeed if retried.

As with the Repeatable Read level, an application using the Serializable isolation level must repeat transactions that failed serialization, 
which is also indicated by the hint in the error message.

We gain programming simplicity, but the price for it is the forced termination of a certain proportion of transactions and the need to repeat them. 
The whole question, of course, is how large this share is. If only those transactions were terminated, which really incompatible data intersect with other transactions, 
everything would be fine. But such an implementation would inevitably turn out to be resource-intensive and inefficient, 
since it would have to keep track of operations with each row.

In reality, PostgreSQL's implementation is such that it allows false negatives: some perfectly normal transactions that are simply "out of luck" will also be aborted. 
As we will see later, this depends on many reasons, such as the availability of suitable indexes or the amount of RAM available. In addition, 
there are some other (rather serious) implementation restrictions, for example, queries at the Serializable level will not work on replicas, 
and parallel execution plans will not be used for them. And while work continues to improve the implementation, 
the existing limitations make this isolation level less attractive.
Parallel plans will appear in PostgreSQL 12 ( patch ). And queries on replicas can work in PostgreSQL 13 ( another patch ).

---------------------------------
- Read-only transaction anomaly -
---------------------------------

To prevent a read-only transaction from causing an anomaly and not being harmed by it, 
PostgreSQL offers an interesting mechanism: such a transaction can be blocked until it is safe to execute. 
This is the only case where a SELECT statement might be blocked by row updates. This is how it looks:

=> UPDATE accounts SET amount = 900.00 WHERE id = 2;
=> UPDATE accounts SET amount = 100.00 WHERE id = 3;
=> SELECT * FROM accounts WHERE client = 'bob' ORDER BY id;
 id | number | client | amount 
----+--------+--------+--------
  2 | 2001   | bob    | 900.00
  3 | 2002   | bob    | 100.00
(2 rows)

=> BEGIN ISOLATION LEVEL SERIALIZABLE; -- 1
=> UPDATE accounts SET amount = amount + (
  SELECT sum(amount) FROM accounts WHERE client = 'bob'
) * 0.01
WHERE id = 2;

|  => BEGIN ISOLATION LEVEL SERIALIZABLE; -- 2
|  => UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;
|  => COMMIT;

We explicitly declare the third transaction to be read only (READ ONLY) and deferred (DEFERRABLE):

|   => BEGIN ISOLATION LEVEL SERIALIZABLE READ ONLY DEFERRABLE; -- 3
|   => SELECT * FROM accounts WHERE client = 'alice';

When you try to execute the request, the transaction is blocked, because otherwise its execution will lead to an anomaly.

=> COMMIT;

And only after the first transaction is committed, the third continues execution:

|    id | number | client | amount
|   ----+--------+--------+--------
|     1 | 1001   | alice  | 800.00
|   (1 row)
|   => SELECT * FROM accounts WHERE client = 'bob';
|    id | number | client |  amount  
|   ----+--------+--------+----------
|     2 | 2001   | bob    | 910.0000
|     3 | 2002   | bob    |     0.00
|   (2 rows)
|   => COMMIT;

Another important note: if Serializable isolation is used, then all transactions in the application must use this layer. 
You cannot mix Read Committed (or Repeatable Read) transactions with Serializable. That is, 
you can mix something, but then Serializable will behave like a Repeatable Read without any warnings. 
Why this happens will be discussed later when we talk about implementation.

So if you decide to use Serializble, it is best to globally set the default level (although this, of course, 
does not prevent you from specifying the wrong level explicitly):

      ALTER SYSTEM SET default_transaction_isolation = 'serializable';

You will find a more rigorous presentation of issues related to transactions, 
consistency and anomalies in the book and lecture course by Boris Asenovich Novikov "Fundamentals of Database Technologies".

---------------------------------------
- What isolation level should I use?  -
---------------------------------------

The Read Committed isolation level is used by default in PostgreSQL and, apparently, this level is used in the vast majority of applications. 
It is convenient in that a transaction can be broken on it only in case of a failure, but not to prevent inconsistency. In other words, a serialization error cannot occur.

The flip side of the coin is a large number of possible anomalies, which were discussed in detail above. 
The developer has to constantly keep them in mind and write code so as not to allow them to appear. 
If you cannot formulate the necessary actions in a single SQL statement, you have to resort to explicitly setting locks. 
The most annoying thing is that the code is difficult to test for errors associated with receiving inconsistent data, 
and the errors themselves can occur in an unpredictable and non-reproducible way and therefore are difficult to fix.

The Repeatable Read isolation level removes some inconsistency problems, but, alas, not all. Therefore, you have to not only remember about the remaining anomalies,
but also modify the application so that it correctly handles serialization errors. This is, of course, inconvenient. But for read-only transactions, 
this level perfectly complements Read Committed and is very convenient, for example, for building reports that use several SQL queries.

Finally, the Serializable layer allows you not to worry about inconsistencies at all, which greatly simplifies writing code. 
The only thing that is required of the application is to be able to repeat any transaction when it receives a serialization error. 
But the proportion of interrupted transactions, additional overhead, and the inability to parallelize queries can significantly reduce system throughput. 
It should also be borne in mind that the Serializable level is not applicable on replicas, and that it cannot be mixed with other isolation levels.

-------------
- Relations -
-------------

If you look inside tables and indexes, you will find that they are arranged in a similar way. Both are base objects that contain some data consisting of strings.

There is no doubt that the table consists of rows; this is less obvious for the index. However, 
imagine a B-tree: it consists of nodes that contain indexed values and references to other nodes or table rows. These nodes can be considered index lines - in fact, they are.

In fact, there are still a number of objects that are arranged in a similar way: sequences (in fact, single-row tables), 
materialized views (in fact, tables that remember a query). And then there are ordinary views, which by themselves do not store data, 
but in all other respects are similar to tables.

All these objects in PostgreSQL are called the general word relation (in English relation ). The word is extremely unfortunate, because it is a term from relational theory. 
You can draw a parallel between a relation and a table (view), but certainly not between a relation and an index. 
But it just so happened: PostgreSQL's academic roots are making themselves felt. I think that at first it was tables and views that were called that, 
and the rest has grown over time.

Further, for simplicity, we will only talk about tables and indexes, but the rest of the relationships are arranged in the same way.

Layers (forks) and files
Usually several layers (forks) correspond to each relation. 
There are several types of layers, and each of them contains a certain kind of data.

If there is a layer, then it is initially represented by a single file . The file name consists of a numeric identifier, 
to which an ending can be appended to match the layer name.

The file gradually grows and when its size reaches 1 GB, the next file of the same layer is created (such files are sometimes called segments ). 
The segment sequence number is appended to the end of the file name.

The 1 GB file size limitation has arisen historically to support various file systems, some of which cannot handle large files. 
The limitation can be changed when building PostgreSQL ( ./configure --with-segsize).

Thus, several files can correspond to one relation on the disk. For example, for a small table there will be 3 of them.
All object files belonging to one table space and one database will be placed in one directory. This must be considered because filesystems usually 
do not work well with a large number of files in a directory.
Let's notice right away that the files, in turn, are divided into pages (or blocks ), usually 8 KB each. Let's talk about the internal structure of pages below.
https://mega.nz/file/xwcQzTpZ#oTJ8XGpycrLbX8OL8WfczAjgapfb9LXmhFqa4TNOQyA

Now let's look at the types of layers.
The main layer is the data itself: those same table or index rows. The base layer exists for any relationship (except for views that do not contain data).
The file names of the main layer consist only of a numeric identifier. For example, here is the path to the file of the table that we created last time:

=> SELECT pg_relation_filepath('accounts');
 pg_relation_filepath 
----------------------
 base/41493/41496
(1 row)

Where do these identifiers come from? The base directory corresponds to the pg_default tablespace, the next subdirectory corresponds to the database, 
and the file we are interested in is already in it:

=> SELECT oid FROM pg_database WHERE datname = 'test';
  oid  
-------
 41493
(1 row)

=> SELECT relfilenode FROM pg_class WHERE relname = 'accounts';
 relfilenode 
-------------
       41496
(1 row)

The path is relative, it is counted from the data directory (PGDATA). Moreover, almost all paths in PostgreSQL are PGDATA-based. 
Thanks to this, you can painlessly transfer PGDATA to another place - nothing holds it (unless you may need to configure the path to the libraries in LD_LIBRARY_PATH).

Then we look in the file system:

postgres$ ls -l --time-style=+ /var/lib/postgresql/11/main/base/41493/41496
-rw------- 1 postgres postgres 8192  /var/lib/postgresql/11/main/base/41493/41496

An initialization layer exists only for unlogged tables (created with UNLOGGED) and their indexes. Such objects are no different from ordinary objects, 
except that actions with them are not recorded in the write-ahead log. Due to this, work with them is faster, but in the event of a failure, 
it is impossible to restore the data to a consistent state. Therefore, when restoring, 
PostgreSQL simply deletes all layers of such objects and writes the initialization layer in place of the main layer. The result is a "dummy". 
We will talk about logging in detail, but in a different cycle.

The accounts table is journaled, so there is no initialization layer for it. But for the experiment, you can turn off logging:

=> ALTER TABLE accounts SET UNLOGGED;
=> SELECT pg_relation_filepath('accounts');
 pg_relation_filepath 
----------------------
 base/41493/41507
(1 row)

The ability to turn on and off logging on the fly, as you can see from the example, is associated with overwriting data into files with different names.

The initialization layer has the same name as the main layer, but with the "_init" suffix:

postgres$ ls -l --time-style=+ /var/lib/postgresql/11/main/base/41493/41507_init
-rw------- 1 postgres postgres 0  /var/lib/postgresql/11/main/base/41493/41507_init

Map free space (free space map) - layer in which noted the presence of empty space within the pages. 
This place is constantly changing: when new versions of lines are added, it decreases, when cleaned, it increases. 
The free space map is used when inserting new versions of rows to quickly find a suitable page to accommodate the added data.

Free space map is suffixed with "_fsm". But the file does not appear immediately, but only when necessary. 
The easiest way to achieve this is to clean up the table (we'll talk about why in due time):

=> VACUUM accounts;

postgres$ ls -l --time-style=+ /var/lib/postgresql/11/main/base/41493/41507_fsm
-rw------- 1 postgres postgres 24576  /var/lib/postgresql/11/main/base/41493/41507_fsm

A visibility map is a layer in which pages are marked with one bit, which contain only the current version of the lines. 
Roughly speaking, this means that when a transaction tries to read a row from such a page, the row can be displayed without checking its visibility. 
We will look in detail at how this happens in future articles.

postgres$ ls -l --time-style=+ /var/lib/postgresql/11/main/base/41493/41507_vm
-rw------- 1 postgres postgres 8192  /var/lib/postgresql/11/main/base/41493/41507_vm

---------
- Pages -
---------

As we said, the files are logically divided into pages.

Typically the page is 8KB in size. The size can be changed within some limits (16 KB or 32 KB), but only during assembly ( ./configure --with-blocksize). 
The assembled and launched instance can only work with pages of one size.

Regardless of which layer the files belong to, they are used by the server in about the same way. Pages are first read into the buffer cache, 
where they can be read and modified by processes; then the pages are preempted back to disk as needed.

Each page has internal markup and generally contains the following sections:

       0 + ----------------------------------- +
          | heading |
      24 + ----------------------------------- +
          | array of pointers to string versions |
   lower + ----------------------------------- +
          | free space |
   upper + ----------------------------------- +
          | row versions |
 special + ----------------------------------- +
          | special area |
pagesize + ----------------------------------- +

The size of these sections is easy to find out with the pageinspect "exploratory" extension:

=> CREATE EXTENSION pageinspect;
=> SELECT lower, upper, special, pagesize FROM page_header(get_raw_page('accounts',0));
 lower | upper | special | pagesize 
-------+-------+---------+----------
    40 |  8016 |    8192 |     8192
(1 row)

Here we are looking at the title of the very first (zero) page of the table. In addition to the size of the other areas, 
the header contains other information about the page, but we are not interested in it yet.

There is a special area at the bottom of the page , in our case it is empty. It is used only for indexes, and even then not for all. 
"Below" here corresponds to the picture; perhaps it would be more correct to say "in senior addresses".

Following the special area are the row versions - the very data that we store in the table, plus some service information.
At the top of the page, right after the heading, is the table of contents: an array of pointers to the line versions available on the page.
Free space can remain between line versions and pointers (which is marked in the free space map). Note that there is no fragmentation inside the page, 
all free space is always represented by one fragment.

------------
- Pointers -
------------

Why are pointers to row versions needed? The point is that index rows must somehow refer to the row versions in the table. 
It is clear that the link must contain the file number, the page number in the file, and some indication of the line version. 
An offset relative to the beginning of the page could be used as such an indication, but this is inconvenient. 
We would not be able to move the version of the line within the page because that would break existing links. 
This would lead to fragmentation of space within pages and other unpleasant consequences. 
Therefore, the index refers to the pointer number, and the pointer refers to the current position of the row version in the page. Indirect addressing turns out.

Each pointer takes exactly 4 bytes and contains:

link to the line version;
the length of this version of the string;
several bits defining the status of the row version.

---------------
- Data format -
---------------

The format of the data on the disk completely coincides with the representation of data in the RAM. 
The page is read into the buffer cache "as is", without any conversion. Therefore, data files from one platform are not compatible with other platforms.
For example, the x86 architecture uses little-endian byte ordering, z / Architecture uses big-endian ordering, and ARM uses switchable ordering.
Many architectures provide for data alignment on word boundaries. For example, on a 32-bit x86 system, integers (integer, occupies 4 bytes) 
will be aligned on 4-byte word boundaries, just like double precision floating point numbers (double precision, 8 bytes). On a 64-bit system, 
double values will be aligned on the boundary of 8-byte words. This is another reason for incompatibility.
Due to alignment, the size of the table row depends on the order of the fields. Usually this effect is not very noticeable, 
but in some cases it can lead to a significant increase in size. For example, if you mix char (1) and integer fields, there will usually be 3 bytes wasted between them. 
More details can be found in the presentation of Nikolai Shaplov " What's inside him".

--------------------------
- Row Versions and TOAST -
--------------------------

We will talk in detail about how the string versions work from the inside next time. For now, 
the only important thing for us is that each version should fit entirely on one page: PostgreSQL does not provide a way to "continue" a line on the next page. 
Instead, a technology called TOAST (The Oversized Attributes Storage Technique) is used. The name itself suggests that the string can be sliced into toast.
Seriously speaking, TOAST involves several strategies. "Long" attribute values can be sent to a separate service table, having previously cut them into small toast fragments. 
Another option is to shrink the value so that the row version still fits on a regular table page. And you can do both: first squeeze, and only then cut and send.
For each main table, if necessary, a separate, but one for all attributes, TOAST table (and a special index to it) is created. 
The need is determined by the presence of potentially long attributes in the table. For example, if the table has a column of type numeric or text, 
the TOAST table will be created immediately, even if long values are not used.

Since a TOAST table is essentially a regular table, it still has the same set of layers. And this doubles the number of files that "serve" the table.
Strategies are initially determined by the data types of the columns. You can view them with a command \d+in psql, but since it displays 
a lot of other information at the same time, we will use a query to the system catalog:

=> SELECT attname, atttypid::regtype, CASE attstorage
  WHEN 'p' THEN 'plain'
  WHEN 'e' THEN 'external'
  WHEN 'm' THEN 'main'
  WHEN 'x' THEN 'extended'
END AS storage
FROM pg_attribute
WHERE attrelid = 'accounts'::regclass AND attnum > 0;
 attname | atttypid | storage  
---------+----------+----------
 id      | integer  | plain
 number  | text     | extended
 client  | text     | extended
 amount  | numeric  | main
(4 rows)

Strategy names have the following meanings:

plain - TOAST is not used (it is used for obviously “short” data types, like integer);
extended - both compression and storage in a separate TOAST table are allowed;
external - long values are stored uncompressed in the TOAST table;
main - long values are first compressed, and only get into the TOAST table if compression did not help.

In general terms, the algorithm looks like this. PostgreSQL strives to have at least 4 lines per page. Therefore, if the line size exceeds a fourth of the page, 
taking into account the header (for a normal 8K page, this is 2040 bytes), you must apply TOAST to some of the values. 
We proceed in the order described below and stop as soon as the line stops exceeding the threshold:

First, we iterate over the attributes with the external and extended strategies, moving from the longest to the shortest. 
Extended attributes are compressed (if it has an effect) and if the value itself exceeds a quarter of a page, it is immediately sent to the TOAST table. 
External attributes are handled the same way, but not compressed.
If after the first pass the row version still does not fit, we send the remaining attributes with the external and extended strategies to the TOAST table.
If this does not help, we try to compress the attributes with the main strategy, while leaving them in the table page.
And only if after that the string is still not short enough, the main attributes are sent to the TOAST table.

Sometimes it can be helpful to change the strategy for some columns. For example, if you know in advance that the data in a column is not compressed, 
you can set the external strategy for it - this will save you on unnecessary compression attempts. This is done as follows:

=> ALTER TABLE accounts ALTER COLUMN number SET STORAGE external;

Repeating the request, we get:

 attname | atttypid | storage  
---------+----------+----------
 id      | integer  | plain
 number  | text     | external
 client  | text     | extended
 amount  | numeric  | main

TOAST tables and indexes are located in a separate pg_toast schema and are therefore usually not visible. For temporary tables used pg_toast_temp_ 
circuit N is similar to a conventional pg_temp_ N .
Of course, if you wish, no one bothers to spy on the internal mechanics of the process. Let's say there are three potentially long attributes in the accounts table, 
so there must be a TOAST table. Here she is:

=> SELECT relnamespace::regnamespace, relname
FROM pg_class WHERE oid = (
  SELECT reltoastrelid FROM pg_class WHERE relname = 'accounts'
);
 relnamespace |    relname     
--------------+----------------
 pg_toast     | pg_toast_33953
(1 row)

=> \d+ pg_toast.pg_toast_33953
TOAST table "pg_toast.pg_toast_33953"
   Column   |  Type   | Storage 
------------+---------+---------
 chunk_id   | oid     | plain
 chunk_seq  | integer | plain
 chunk_data | bytea   | plain

It is logical that for "toasts", into which a string is cut, the plain strategy is used: there is no second level TOAST.
PostgreSQL hides the index more carefully, but it is not difficult to find it either:

=> SELECT indexrelid::regclass FROM pg_index
WHERE indrelid = (
  SELECT oid FROM pg_class WHERE relname = 'pg_toast_33953'
);
          indexrelid           
-------------------------------
 pg_toast.pg_toast_33953_index
(1 row)

=> \d pg_toast.pg_toast_33953_index
Unlogged index "pg_toast.pg_toast_33953_index"
  Column   |  Type   | Key? | Definition 
-----------+---------+------+------------
 chunk_id  | oid     | yes  | chunk_id
 chunk_seq | integer | yes  | chunk_seq
primary key, btree, for table "pg_toast.pg_toast_33953"

The client column uses the extended strategy: the values ​​in it will be compressed. Let's check:

=> UPDATE accounts SET client = repeat('A',3000) WHERE id = 1;
=> SELECT * FROM pg_toast.pg_toast_33953;
 chunk_id | chunk_seq | chunk_data 
----------+-----------+------------
(0 rows)

There is nothing in the TOAST table: duplicate characters are perfectly compressed and after that the value fits into a regular table page.
Now let's say the customer's name consists of random characters:

=> UPDATE accounts SET client = (
  SELECT string_agg( chr(trunc(65+random()*26)::integer), '') FROM generate_series(1,3000)
)
WHERE id = 1
RETURNING left(client,10) || '...' || right(client,10);
        ?column?         
-------------------------
 TCKGKZZSLI...RHQIOLWRRX
(1 row)

This sequence cannot be compressed, and it ends up in the TOAST table:

=> SELECT chunk_id,
  chunk_seq,
  length(chunk_data),
  left(encode(chunk_data,'escape')::text, 10) ||
  '...' ||
  right(encode(chunk_data,'escape')::text, 10) 
FROM pg_toast.pg_toast_33953;
 chunk_id | chunk_seq | length |        ?column?         
----------+-----------+--------+-------------------------
    34000 |         0 |   2000 | TCKGKZZSLI...ZIPFLOXDIW
    34000 |         1 |   1000 | DDXNNBQQYH...RHQIOLWRRX
(2 rows)

As you can see, the data is cut into fragments of 2000 bytes.
When accessing a "long" value, PostgreSQL automatically, transparently to the application, restores the original value and returns it to the client.
Of course, both sliced compression and subsequent recovery are a lot of resources. Therefore, storing bulk data in PostgreSQL is not a good idea, 
especially if it is actively used and does not require transactional logic (for example: scanned originals of accounting documents). 
A more profitable alternative may be storing such data on the file system, and in the DBMS - the names of the corresponding files.
The TOAST table is used only when accessing a "long" value. 
In addition, the toast table maintains its own versioning: if the data update does not affect the "long" value, 
the new version of the row will refer to the same value in the TOAST table - this saves space.
Note that TOAST only works for tables, not indexes. This imposes a limit on the size of the indexed keys.

-----------
- Heading -
-----------

As we said, each row can be present in the database in several versions at the same time. One version from another must be somehow distinguished. 
For this purpose, each version has two marks that determine the "time" of the given version (xmin and xmax). 
In quotes - because not time is used as such, but a special incremental counter. And this counter is the transaction number.

(As usual, in fact, everything is more complicated: the number of transactions cannot increase all the time due to the limited capacity of the counter. 
But we will consider these details in detail when we get to freezing.)

When the row is created, xmin is set to the transaction number that issued the INSERT command, and xmax is not populated.

When a row is deleted, the xmax value of the current version is marked with the transaction number that performed the DELETE.

When a row is modified with an UPDATE command, two operations are actually performed: DELETE and INSERT. 
The current version of the line sets xmax to the number of the transaction that performed the UPDATE. Then a new version of the same string is created; 
its xmin value is the same as the xmax value of the previous version.

The xmin and xmax fields are included in the row version header. In addition to these fields, the header contains others, for example:

infomask is a set of bits that define the properties of this version. There are quite a few of them; we will gradually consider the main ones.
ctid is a link to the next, newer, version of the same line. For the most recent, current version of the line, the ctid refers to that version itself. 
The number has the form (x, y), where x is the page number, y is the ordinal number of the pointer in the array.
Nulls bitmap - marks those columns of this version that contain NULL. NULL is not one of the usual values of data types, so the attribute has to be stored separately.

As a result, the header is quite large - at least 23 bytes per row version, and usually more due to the NULL bitmap. 
If the table is "narrow" (that is, contains few columns), the overhead can take up more than useful information.

----------
- Insert -
----------

Let's take a closer look at how low-level row operations are performed and start with insertion.
To experiment, let's create a new table with two columns and an index on one of them:

=> CREATE TABLE t(
  id serial,
  s text
);
=> CREATE INDEX ON t(s);

Let's insert one row after starting the transaction.

=> BEGIN;
=> INSERT INTO t(s) VALUES ('FOO');

Here is our current transaction number:

=> SELECT txid_current();
 txid_current 
--------------
         3664
(1 row)

Let's take a look at the page content. The heap_page_items function of the pageinspect extension allows you to get information about pointers and row versions:

=> SELECT * FROM heap_page_items(get_raw_page('t',0)) \gx
-[ RECORD 1 ]-------------------
lp          | 1
lp_off      | 8160
lp_flags    | 1
lp_len      | 32
t_xmin      | 3664
t_xmax      | 0
t_field3    | 0
t_ctid      | (0,1)
t_infomask2 | 2
t_infomask  | 2050
t_hoff      | 24
t_bits      | 
t_oid       | 
t_data      | \x0100000009464f4f

Note that heap in PostgreSQL refers to tables. This is another strange use of the term - the heap is a well - known data structure that has nothing to do with a table. 
Here the word is used in the sense of "heaped everything", in contrast to ordered indices.
The function shows the data “as is”, in a format that is difficult to understand. To understand, we will leave only part of the information and decipher it:

=> SELECT '(0,'||lp||')' AS ctid,
       CASE lp_flags
         WHEN 0 THEN 'unused'
         WHEN 1 THEN 'normal'
         WHEN 2 THEN 'redirect to '||lp_off
         WHEN 3 THEN 'dead'
       END AS state,
       t_xmin as xmin,
       t_xmax as xmax,
       (t_infomask & 256) > 0  AS xmin_commited,
       (t_infomask & 512) > 0  AS xmin_aborted,
       (t_infomask & 1024) > 0 AS xmax_commited,
       (t_infomask & 2048) > 0 AS xmax_aborted,
       t_ctid
FROM heap_page_items(get_raw_page('t',0)) \gx
-[ RECORD 1 ]-+-------
ctid          | (0,1)
state         | normal
xmin          | 3664
xmax          | 0
xmin_commited | f
xmin_aborted  | f
xmax_commited | f
xmax_aborted  | t
t_ctid        | (0,1)

Here's what we did:

 * Added a zero to the pointer number to make it look the same as t_ctid: (page number, pointer number).
 * Decrypted the state of the lp_flags pointer. Here it is “normal”, which means that the pointer is actually referring to the line version. 
   We will consider other values later.
 * Of all the information bits, only two pairs have been allocated so far. 
   The xmin_committed and xmin_aborted bits indicate whether transaction number xmin is committed (canceled). 
   Two similar bits refer to transaction numbered xmax.

What do we see? When you insert a row, an index number 1 appears in the table page, referencing the first and only version of the row.
In the row version, the xmin field is filled with the current transaction number. The transaction is still active, so both xmin_committed and xmin_aborted are not set.
The row version ctid field refers to the same row. This means that there is no newer version.
The xmax field is filled with a dummy number 0, since this version of the line has not been deleted and is current. 
Transactions will ignore this number because the xmax_aborted bit is set.
Let's take another step towards improving readability by adding information bits to the transaction numbers. And let's create a function, 
since we will need the request more than once:

=> CREATE FUNCTION heap_page(relname text, pageno integer)
RETURNS TABLE(ctid tid, state text, xmin text, xmax text, t_ctid tid)
AS $$
SELECT (pageno,lp)::text::tid AS ctid,
       CASE lp_flags
         WHEN 0 THEN 'unused'
         WHEN 1 THEN 'normal'
         WHEN 2 THEN 'redirect to '||lp_off
         WHEN 3 THEN 'dead'
       END AS state,
       t_xmin || CASE
         WHEN (t_infomask & 256) > 0 THEN ' (c)'
         WHEN (t_infomask & 512) > 0 THEN ' (a)'
         ELSE ''
       END AS xmin,
       t_xmax || CASE
         WHEN (t_infomask & 1024) > 0 THEN ' (c)'
         WHEN (t_infomask & 2048) > 0 THEN ' (a)'
         ELSE ''
       END AS xmax,
       t_ctid
FROM heap_page_items(get_raw_page(relname,pageno))
ORDER BY lp;
$$ LANGUAGE SQL;

In this form, it is much clearer what is going on in the header of the line version:

=> SELECT * FROM heap_page('t',0);
 ctid  | state  | xmin | xmax  | t_ctid 
-------+--------+------+-------+--------
 (0,1) | normal | 3664 | 0 (a) | (0,1)
(1 row)

Similar, but much less detailed, information can be obtained from the table itself using the pseudo-columns xmin and xmax:

=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3664 |    0 |  1 | FOO
(1 row)

------------
- Fixation -
------------

Upon successful completion of the transaction, you need to remember its status - note that it is committed. 
For this, a structure called XACT is used (and before version 10 it was called CLOG (commit log) and this name can still be found in different places).

XACT is not a system catalog table; these are the files in the PGDATA / pg_xact directory. 
They have two bits allocated for each transaction: committed and aborted - just like in the row version header. 
This information is split into several files solely for convenience, we will come back to this issue when we consider freezing. 
And work with these files is carried out page by page, as with all others.

So, when a transaction is committed, the committed bit is set in XACT for this transaction. 
And that's all that happens on commit (although we're not talking about the write-ahead log yet).

When any other transaction accesses the tabular page we just looked at, it will have to answer a few questions.

Has transaction xmin been completed? If not, then the generated row version should not be visible.
This check is done by looking at another structure that resides in the shared memory of the instance and is called ProcArray. 
It contains a list of all active processes, and for each the number of its current (active) transaction is indicated.
If it ended, how - by committing or canceling? If canceled, then the line version should not be visible either.
That's what XACT is for. But, although the last XACT pages are stored in buffers in RAM, it is still expensive to check XACT every time. 
Therefore, once the status of a transaction is clarified, it is written into the xmin_committed and xmin_aborted bits of the row version. 
If one of these bits is set, then the state of the transaction xmin is considered known and the next transaction will no longer need to contact XACT.

Why aren't these bits set by the insert transaction itself? When an insert occurs, the transaction does not yet know if it will succeed. 
And at the moment of committing, it is no longer clear which lines in which pages were changed. There may be many such pages, 
and it is not profitable to memorize them. In addition, some pages can be flushed out of the buffer cache to disk; 
reading them again to change the bits would slow down the commit significantly.

The downside to saving is that after changes, any transaction (even performing a simple read - SELECT) can start changing the data pages in the buffer cache.

So let's commit the change.

=> COMMIT;

Nothing has changed on the page (but we know that the status of the transaction has already been recorded in XACT):

=> SELECT * FROM heap_page('t',0);
 ctid  | state  | xmin | xmax  | t_ctid 
-------+--------+------+-------+--------
 (0,1) | normal | 3664 | 0 (a) | (0,1)
(1 row)

Now the transaction that first accesses the page will have to determine the status of the transaction xmin and write it to the information bits:

=> SELECT * FROM t;
 id |  s  
----+-----
  1 | FOO
(1 row)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3664 (c) | 0 (a) | (0,1)
(1 row)

------------
- Deleting -
------------

When a row is deleted, the number of the current deleting transaction is written in the xmax field of the current version, and the xmax_aborted bit is cleared.

Note that the set xmax value corresponding to an active transaction acts as a row lock. If another transaction is about to update or delete this row, 
it will have to wait for the xmax transaction to complete. We will talk more about blocking later. For now, let's just note that the number of row locks is unlimited. 
They do not take up space in RAM and system performance does not suffer from their number. True, “long” transactions have other disadvantages, but more on that later.

Let's delete the line.

=> BEGIN;
=> DELETE FROM t;
=> SELECT txid_current();
 txid_current 
--------------
         3665
(1 row)

We see that the transaction number was written in the xmax field, but the information bits are not set:

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax | t_ctid 
-------+--------+----------+------+--------
 (0,1) | normal | 3664 (c) | 3665 | (0,1)
(1 row)

----------------
- Cancellation -
----------------

Undoing works in the same way as committing, except that the aborted bit is set for the transaction in XACT. The undo is as fast as the commit. 
Although the command is called ROLLBACK, changes are not rolled back: everything that the transaction has changed in the data pages remains unchanged.

=> ROLLBACK;
=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax | t_ctid 
-------+--------+----------+------+--------
 (0,1) | normal | 3664 (c) | 3665 | (0,1)
(1 row)

When the page is accessed, the status will be checked and the hint bit xmax_aborted will be set in the string version. 
The number xmax itself remains in the page, but no one will look at it anymore.

=> SELECT * FROM t;
 id |  s  
----+-----
  1 | FOO
(1 row)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   |   xmax   | t_ctid 
-------+--------+----------+----------+--------
 (0,1) | normal | 3664 (c) | 3665 (a) | (0,1)
(1 row)

----------
- Update -
----------

Updating works as if first deleting the current version of the row and then inserting a new one.

=> BEGIN;
=> UPDATE t SET s = 'BAR';
=> SELECT txid_current();
 txid_current 
--------------
         3666
(1 row)

The query throws out one line (new version):

=> SELECT * FROM t;
 id |  s  
----+-----
  1 | BAR
(1 row)

But in the page we see both versions:

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3664 (c) | 3666  | (0,2)
 (0,2) | normal | 3666     | 0 (a) | (0,2)
(2 rows)

The deleted version is marked with the current transaction number in the xmax field. Moreover, this value is written over the old one, 
since the previous transaction was canceled. And the xmax_aborted bit is cleared because the status of the current transaction is still unknown.

The first version of the line now refers to the second (t_ctid field) as newer.
A second index appears in the index page, and a second row references the second version in the table page.
As with the deletion, the xmax value in the first version of the row indicates that the row is locked.
Well, let's complete the transaction.

=> COMMIT;

-----------
- Indexes -
-----------

So far, we've only talked about table pages. What happens inside the indexes?

The information in index pages is highly dependent on the specific type of index. And even one type of index has different types of pages. 
For example, a B-tree has a metadata page and "regular" pages.

However, usually a page has an array of pointers to strings and the strings themselves (just like a table page). In addition, 
there is space at the end of the page for special data.

Rows in indexes can also have very different structures depending on the type of index. For example, for a B-tree, 
the leaf page rows contain the index key value and a reference (ctid) to the corresponding table row. In general, the index can be arranged in a completely different way.
The most important point is that there are no row versions in any type of index. Or, 
you can assume that each line is represented by exactly one version. In other words, there are no xmin and xmax fields in the index line header. 
You can assume that links from the index lead to all table row versions - so you can figure out which version a transaction will see only by looking at the table. 
(As usual, this is not the whole truth. In some cases, the visibility map allows you to optimize the process, but we will consider this in more detail later.)
At the same time, in the index page, we find pointers to both versions, both the current and the old:

=> SELECT itemoffset, ctid FROM bt_page_items('t_s_idx',1);
 itemoffset | ctid  
------------+-------
          1 | (0,2)
          2 | (0,1)
(2 rows)

------------------------
- Virtual transactions -
------------------------

In practice, PostgreSQL uses optimizations to "save" transaction numbers.

If a transaction only reads data, then it has no effect on the visibility of row versions. 
Therefore, at the beginning, the service process issues a virtual xid to the transaction. The number consists of a process ID and a sequential number.

The issue of this number does not require synchronization between all processes and is therefore very fast. 
We'll get to know another reason for using virtual numbers when we talk about freezing.

Virtual numbers are not counted in any way in data snapshots.

At different points in time, the system may well contain virtual transactions with numbers that have already been used, and this is normal. 
But such a number cannot be written into data pages, because the next time the page is accessed, it can lose all meaning.

=> BEGIN;
=> SELECT txid_current_if_assigned();
 txid_current_if_assigned 
--------------------------
                         
(1 row)

If a transaction starts changing data, it is given a real, unique transaction number.

=> UPDATE accounts SET amount = amount - 1.00;
=> SELECT txid_current_if_assigned();
 txid_current_if_assigned 
--------------------------
                     3667
(1 row)

=> COMMIT;

-----------------------
- Nested transactions -
-----------------------

---------------
- Save points -
---------------

In SQL are defined in terms of conservation (savepoint), that allow you to cancel the operation of the transaction, without interrupting it completely. 
But this does not fit into the above scheme, since the status of a transaction is one for all its changes, and physically no data is rolled back.

To implement such functionality, a transaction with the point of saving is divided into several separate nested transactions (subtransaction), 
the status of which can be controlled separately.

Nested transactions have their own number (greater than the number of the main transaction). The status of nested transactions is recorded in the usual way in XACT, 
but the final status depends on the status of the main transaction: if it is canceled, then all nested transactions are canceled as well.

Transaction nesting information is stored in files in the PGDATA / pg_subtrans directory. The files are accessed through buffers in the instance's shared memory, 
organized in the same way as XACT buffers.

Don't confuse nested transactions and autonomous transactions. Autonomous transactions are independent of each other, while nested transactions are. 
There are no autonomous transactions in regular PostgreSQL, and perhaps for the best: they are very, 
very rarely needed for business, and their presence in other DBMSs provokes abuse, from which everyone then suffers.

Let's clear the table, start a transaction and insert a row:

=> TRUNCATE TABLE t;
=> BEGIN;
=> INSERT INTO t(s) VALUES ('FOO');
=> SELECT txid_current();
 txid_current 
--------------
         3669
(1 row)

=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3669 |    0 |  2 | FOO
(1 row)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  | xmin | xmax  | t_ctid 
-------+--------+------+-------+--------
 (0,1) | normal | 3669 | 0 (a) | (0,1)
(1 row)

Now let's put a savepoint and insert another line.

=> SAVEPOINT sp;
=> INSERT INTO t(s) VALUES ('XYZ');
=> SELECT txid_current();
 txid_current 
--------------
         3669
(1 row)

Note that the txid_current () function returns the number of the main, not the nested, transaction.

=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3669 |    0 |  2 | FOO
 3670 |    0 |  3 | XYZ
(2 rows)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  | xmin | xmax  | t_ctid 
-------+--------+------+-------+--------
 (0,1) | normal | 3669 | 0 (a) | (0,1)
 (0,2) | normal | 3670 | 0 (a) | (0,2)
(2 rows)

Let's roll back to the savepoint and insert the third line.

=> ROLLBACK TO sp;
=> INSERT INTO t(s) VALUES ('BAR');
=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3669 |    0 |  2 | FOO
 3671 |    0 |  4 | BAR
(2 rows)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3669     | 0 (a) | (0,1)
 (0,2) | normal | 3670 (a) | 0 (a) | (0,2)
 (0,3) | normal | 3671     | 0 (a) | (0,3)
(3 rows)

In the page, we continue to see the row added by the canceled nested transaction.

We commit the changes.

=> COMMIT;
=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3669 |    0 |  2 | FOO
 3671 |    0 |  4 | BAR
(2 rows)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3669 (c) | 0 (a) | (0,1)
 (0,2) | normal | 3670 (a) | 0 (a) | (0,2)
 (0,3) | normal | 3671 (c) | 0 (a) | (0,3)
(3 rows)

Now you can clearly see that each nested transaction has its own status.

Note that nested transactions cannot be used explicitly in SQL, that is, you cannot start a new transaction without completing the current one. 
This mechanism is used implicitly when using savepoints, as well as when handling PL / pgSQL exceptions and in a number of other, more exotic cases.

=> BEGIN;
BEGIN
=> BEGIN;
WARNING:  there is already a transaction in progress
BEGIN
=> COMMIT;
COMMIT
=> COMMIT;
WARNING:  there is no transaction in progress
COMMIT

--------------------------------------
- Errors and atomicity of operations -
--------------------------------------

What happens if an error occurs during the operation? For example, like this:

=> BEGIN;
=> SELECT * FROM t;
 id |  s  
----+-----
  2 | FOO
  4 | BAR
(2 rows)

=> UPDATE t SET s = repeat('X', 1/(id-4));
ERROR:  division by zero

An error has occurred. Now the transaction is considered interrupted and no operation is allowed in it:

=> SELECT * FROM t;
ERROR:  current transaction is aborted, commands ignored until end of transaction block

And even if you try to commit the changes, PostgreSQL will report the cancellation:

=> COMMIT;
ROLLBACK

Why can't the transaction continue after a crash? 
The fact is that an error could arise in such a way that we would get access to some of the changes - the atomicity of not even a transaction, 
but an operator would be violated. As in our example, where the operator managed to update one line before the error:

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3669 (c) | 3672  | (0,4)
 (0,2) | normal | 3670 (a) | 0 (a) | (0,2)
 (0,3) | normal | 3671 (c) | 0 (a) | (0,3)
 (0,4) | normal | 3672     | 0 (a) | (0,4)
(4 rows)

I must say that psql has a mode that nevertheless allows the transaction to continue working after a failure, 
as if the actions of the erroneous statement are being rolled back.

=> \set ON_ERROR_ROLLBACK on
=> BEGIN;
=> SELECT * FROM t;
 id |  s  
----+-----
  2 | FOO
  4 | BAR
(2 rows)

=> UPDATE t SET s = repeat('X', 1/(id-4));
ERROR:  division by zero

=> SELECT * FROM t;
 id |  s  
----+-----
  2 | FOO
  4 | BAR
(2 rows)

=> COMMIT;

It is easy to guess that in this mode psql actually puts an implicit savepoint before each command, and in case of failure initiates a rollback to it. 
This mode is not used by default, since setting safepoints (even without rolling back to them) incurs significant overhead costs.

------------
- snapshot -
------------

Physically, data pages can contain multiple versions of the same row. 
Moreover, each transaction should see only one (or none) version of each row so that together they make up an ACID-consistent picture of the data 
at a certain point in time.
Isolation in PostgreSQL is based on snapshots: each transaction operates with its own snapshot, which "contains" the data that was committed before 
the snapshot was taken, and does not "contain" the data that has not yet been captured. We have already seen that the isolation is stricter than 
required by the standard, but not devoid of anomalies.
At the Read Committed isolation level, a snapshot is taken at the start of each statement in a transaction. 
This snapshot is active while the statement is being executed. In the figure, the moment of snapshot creation 
(which, as we remember, is determined by the transaction number) is shown in blue.
https://mega.nz/file/Uk9RVQRI#M264RUhBfARxNiMx_BS44WjBrhaqK-D6FQvcMJh5Gj0

At the Repeatable Read and Serializable levels, the snapshot is taken once at the beginning of the first statement in the transaction. 
This snapshot remains active until the very end of the transaction.
https://mega.nz/file/dpkh1K6a#h7Az74L9hPuAOI9GwwXQsP6pqmmG8cMADVkaTPcjg4E

---------------------------------------
- Row Versions Visibility in Snapshot -
---------------------------------------

--------------------
- Visibility rules -
--------------------

Of course, the snapshot is not a physical copy of all the required row versions. 
In fact, the snapshot is specified by several numbers, and the visibility of the row versions in the snapshot is determined by the rules.
Whether or not this version of the row is visible in the snapshot depends on two fields of its header - xmin and xmax - that is, 
on the numbers of the transactions that created and deleted them. Such intervals do not overlap, therefore, 
one line is represented in any picture by at most one version of it.
The exact rules for visibility are quite complex and account for many different situations and edge cases.

You can easily verify this by looking at src/backend/utils/time/tqual.c (in version 12, the check moved to src/backend/access/heap/heapam_visibility.c).
To simplify, we can say that the row version is visible when the changes made by the xmin transaction are visible in the snapshot, 
and the changes made by the xmax transaction are not visible (in other words, it is already visible that the row version has appeared, 
but it is not yet visible that it was deleted).

In turn, changes to the transaction are visible in the snapshot if either it is the same transaction that created the snapshot (it sees its own changes), 
or the transaction was committed before the snapshot was taken.
You can represent transactions graphically in the form of segments (from the moment of beginning to the moment of committing):
https://mega.nz/file/051xiCBL#XhCEkaaKQh2aOnFW8UZqrzFGzErzgsZJ01VsG7tzwD4

Here:

 * changes to transaction 2 will be visible because it completed before the snapshot was taken,
 * changes to transaction 1 will not be visible because it was active at the time the snapshot was taken,
 * changes to transaction 3 will not be visible, because it started later than the snapshot was taken (it does not matter if it ended or not).

Unfortunately, the moment of committing transactions is unknown to the system. Only the moment of its beginning is known 
(it is determined by the transaction number and is marked in the figures above with a dashed line), but the fact of completion is not recorded anywhere.
All we can do is find out the current status of transactions when the snapshot is taken. 
This information is stored in the server's shared memory in the ProcArray structure, which contains a list of all active sessions and their transactions.
And after the fact, we will no longer be able to understand whether any transaction was active at the time of the snapshot creation or not. 
Therefore, a list of all currently active transactions must be stored in a snapshot.
It follows from the above that in PostgreSQL you cannot create a snapshot showing consistent data as of an arbitrary time ago, 
even ifall the row versions required for this exist in the table pages. 
One often hears the question why there are no flashback (or temporal; in Oracle, this is called flashback query)
queries in PostgreSQL - this is one of the reasons.

It's funny that initially there was such functionality, but later it was removed from the DBMS. You can read about this in the article by Joseph Hellerstein .

So, a snapshot of data is determined by several parameters:
the moment of the snapshot creation, namely, the number of the next transaction that does not yet exist in the system ( snapshot.xmax );
the list of active transactions at the moment of snapshot creation ( snapshot.xip ).
For convenience and optimization, the number of the earliest active transaction ( snapshot.xmin ) is also saved separately . 
This value has an important meaning, which we will discuss below.
Also, a few more parameters are saved in the snapshot, but they are not important for us.
https://mega.nz/file/Zlk1XQrJ#IBJwaDfOGLrz9QXHIn45oUqJPuBh1NHlW_JcT8zWv6g

-----------
- Example -
-----------

To see how visibility is determined by a snapshot, let's reproduce the three-transaction situation discussed above. The table will have three lines, and:

the first one was added by a transaction that started before the snapshot was taken but ended later,
the second is added by a transaction that started and ended before the snapshot was taken,
the third was added after the snapshot was taken.

=> TRUNCATE TABLE accounts;

First transaction (not completed yet):

=> BEGIN;
=> INSERT INTO accounts VALUES (1, '1001', 'alice', 1000.00);
=> SELECT txid_current();
=> SELECT txid_current();
 txid_current 
--------------
         3695
(1 row)

Second transaction (completed before snapshot was taken):

|  => BEGIN;
|  => INSERT INTO accounts VALUES (2, '2001', 'bob', 100.00);
|  => SELECT txid_current();
|   txid_current 
|  --------------
|           3696
|  (1 row)
|  => COMMIT;

Create a snapshot in a transaction in another session.

||    => BEGIN ISOLATION LEVEL REPEATABLE READ;
||    => SELECT xmin, xmax, * FROM accounts;
||     xmin | xmax | id | number | client | amount 
||    ------+------+----+--------+--------+--------
||     3696 |    0 |  2 | 2001   | bob    | 100.00
||    (1 row)

We complete the first transaction after the snapshot is created:

=> COMMIT;

And the third transaction (appeared after the snapshot was taken):

|  => BEGIN;
|  => INSERT INTO accounts VALUES (3, '2002', 'bob', 900.00);
|  => SELECT txid_current();
|   txid_current 
|  --------------
|           3697
|  (1 row)
|  => COMMIT;

Obviously, one line is still visible in our snapshot:

||    => SELECT xmin, xmax, * FROM accounts;
||     xmin | xmax | id | number | client | amount 
||    ------+------+----+--------+--------+--------
||     3696 |    0 |  2 | 2001   | bob    | 100.00
||    (1 row)

The question is how PostgreSQL understands this.

Everything is determined by the snapshot. Let's look at it:

||    => SELECT txid_current_snapshot();
||     txid_current_snapshot 
||    -----------------------
||     3695:3697:3695
||    (1 row)

Here snapshot.xmin, snapshot.xmax and snapshot.xip are listed separated by colons (in this case, one number, but generally a list).

According to the rules formulated above, 
the snapshot should show the changes made by transactions with numbers snapshot.xmin <= xid <snapshot.xmax, 
except for those included in the snapshot.xip list. Let's look at all the rows of the table (in a new snapshot):

=> SELECT xmin, xmax, * FROM accounts ORDER BY id;
 xmin | xmax | id | number | client | amount  
------+------+----+--------+--------+---------
 3695 |    0 |  1 | 1001   | alice  | 1000.00
 3696 |    0 |  2 | 2001   | bob    |  100.00
 3697 |    0 |  3 | 2002   | bob    |  900.00
(3 rows)

 * The first line is not visible - it was created by a transaction that is in the active list (xip).
 * The second line is visible - it was created by a transaction that falls within the snapshot range.
 * The third row is not visible - it was created by a transaction that is not in the snapshot range.

||    => COMMIT;

---------------
- Own changes -
---------------

The case of determining the visibility of the transaction's own changes somewhat complicates the picture. 
You may want to see only a subset of these changes here. For example, 
a cursor that is open at a certain point in any isolation level should not see changes made after that point.
To do this, there is a special field in the header of the row version (which is displayed in the cmin and cmax pseudo-columns) 
that shows the ordinal number of the operation within the transaction. Cmin represents the number to insert, cmax represents the number to delete, 
but to save space in the header of the line, this is actually one field, not two different ones. 
It is believed that inserts and deletions of the same row in the same transaction are rare.
If this does happen, then a special "combo" -number is inserted into the same field, about which the service process remembers the real cmin and cmax. 
But this is already quite exotic.

A simple example. Let's start a transaction and add a row to the table:

=> BEGIN;
=> SELECT txid_current();
 txid_current 
--------------
         3698
(1 row)
INSERT INTO accounts(id, number, client, amount) VALUES (4, 3001, 'charlie', 100.00);

Let's display the contents of the table along with the cmin field (but only for rows added by our transaction - for others it doesn't make sense):

=> SELECT xmin, CASE WHEN xmin = 3698 THEN cmin END cmin, * FROM accounts;
 xmin | cmin | id | number | client  | amount  
------+------+----+--------+---------+---------
 3695 |      |  1 | 1001   | alice   | 1000.00
 3696 |      |  2 | 2001   | bob     |  100.00
 3697 |      |  3 | 2002   | bob     |  900.00
 3698 |    0 |  4 | 3001   | charlie |  100.00
(4 rows)

Now let's open a cursor for a query that returns the number of rows in a table.

=> DECLARE c CURSOR FOR SELECT count(*) FROM accounts;

And after that add another line:

=> INSERT INTO accounts(id, number, client, amount) VALUES (5, 3002, 'charlie', 200.00);

The request will return 4 - the row added after opening the cursor will not be included in the data snapshot:

=> FETCH c;
 count 
-------
     4
(1 row)

Why? Because the snapshot only takes into account row versions with cmin <1.

=> SELECT xmin, CASE WHEN xmin = 3698 THEN cmin END cmin, * FROM accounts;
 xmin | cmin | id | number | client  | amount  
------+------+----+--------+---------+---------
 3695 |      |  1 | 1001   | alice   | 1000.00
 3696 |      |  2 | 2001   | bob     |  100.00
 3697 |      |  3 | 2002   | bob     |  900.00
 3698 |    0 |  4 | 3001   | charlie |  100.00
 3698 |    1 |  5 | 3002   | charlie |  200.00
(5 rows)
=> ROLLBACK;

Event horizon

The number of the earliest active transaction (snapshot.xmin) has an important meaning - it defines the "event horizon" of the transaction. 
Namely, beyond its horizon, a transaction always sees only the latest row versions.
Indeed, an irrelevant version is required to be seen only if the current one was created by an uncompleted transaction, and therefore is not visible yet. 
But beyond the horizon, all transactions are guaranteed to be completed.
https://mega.nz/file/Zt9DFCZZ#AqMY2xUjc16gX_sWlQFxCE2UtwhMfYr76rF39Yd1XLA

The "event horizon" of a transaction can be seen in the system catalog:

=> BEGIN;
=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();
 backend_xmin 
--------------
         3699
(1 row)

You can also define an "event horizon" at the database level. To do this, take all active snapshots and find the oldest xmin among them. 
It will determine the horizon beyond which irrelevant versions of rows in this database will never be seen by any transaction. 
Such line versions can be cleaned up - which is why the concept of the horizon is so important from a practical point of view.
If any transaction will hold a snapshot for a long time, it will also hold the event horizon of the database. 
Moreover, an unfinished transaction will hold the horizon by its very existence, even if it does not hold a snapshot.
This means that irrelevant versions of rows in this database cannot be cleared. In this case, 
a "long-running" transaction may not overlap with other transactions in any way - this is absolutely not important, the database horizon is the same for all.
Now, if a segment does not represent a transaction, and pictures (from snapshot.xmin to snapshot.xmax), then the situation can be represented as follows:
https://mega.nz/file/oh0XzYbY#7w_DoiS0vRxN-WbGgGFydjA5DsGDpQnfA03l7DT3NFk

In the figure, the lowest picture refers to the incomplete transaction and the remaining images can not be snapshot.xmin more than her number.
In our example, a transaction with the Read Committed isolation level was started. Even though it doesn't have any active snapshot of data, 
it continues to hold the horizon:

|  => BEGIN;
|  => UPDATE accounts SET amount = amount + 1.00;
|  => COMMIT;
=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();
 backend_xmin 
--------------
         3699
(1 row)

And only after the transaction is completed, the horizon moves forward, allowing you to clear outdated row versions:

=> COMMIT;
=> SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();
 backend_xmin 
--------------
         3700
(1 row)

If the situation described does create problems and there is no way to avoid it at the application level, then, starting with version 9.6, 
two parameters are available:
old_snapshot_threshold defines the maximum snapshot lifetime. After this time, the server gets the right to delete outdated row versions, 
and if they are needed by a "long-running" transaction, it will receive a snapshot too old error.
idle_in_transaction_session_timeout defines the maximum idle transaction lifetime. After this time, the transaction is aborted.

------------------------
- Exporting a snapshot -
------------------------

There are situations where multiple concurrent transactions must be guaranteed to see the same picture of the data. An example is the pg_dump utility, 
which can work in parallel: all worker processes must see the database in the same state for the backup to be consistent.
Of course, you cannot rely on data patterns to match simply because transactions are running "simultaneously." 
There is a mechanism for exporting and importing a snapshot for this.
The pg_export_snapshot function returns the snapshot identifier that can be transferred (by means external to the DBMS) to another transaction.

=> BEGIN ISOLATION LEVEL REPEATABLE READ;
=> SELECT count(*) FROM accounts; -- любой запрос
 count 
-------
     3
(1 row)
=> SELECT pg_export_snapshot();
 pg_export_snapshot  
---------------------
 00000004-00000E7B-1
(1 row)

Another transaction can import a snapshot using the SET TRANSACTION SNAPSHOT command before executing the first query in it. 
The isolation level must also be set to Repeatable Read or Serializable, because at the Read Committed level, statements will use their own snapshots.

|  => DELETE FROM accounts;
|  => BEGIN ISOLATION LEVEL REPEATABLE READ;
|  => SET TRANSACTION SNAPSHOT '00000004-00000E7B-1';

Now the second transaction will work with a snapshot of the first and, accordingly, will see three rows (and not zero):

|  => SELECT count(*) FROM accounts;
|   count 
|  -------
|       3
|  (1 row)

The exported snapshot has the same lifetime as the exporting transaction.

|    => COMMIT;
=> COMMIT;

-------------------------------------
- In-page cleanup on normal updates -
-------------------------------------

When a page is accessed, whether it is refreshed or read, a quick in-page flush can occur if PostgreSQL realizes that the page is running out of space. 
This happens in two cases.

An UPDATE previously performed on this page did not find enough space to accommodate the new version of the row on the same page. 
This situation is remembered in the page header and the next time the page is cleared.
The page is more full than fillfactor. In this case, cleaning occurs immediately, without postponing the next time.
Fillfactor is a storage parameter that can be defined for a table (and for an index). 
PostgreSQL inserts a new row (INSERT) into a page only if that page is less than fillfactor percent full. 
The rest of the space is reserved for new versions of rows that result from updates (UPDATE). 
The default value for tables is 100, which means no space is reserved (and the value for indexes is 90).
In-page cleanup removes row versions that are not visible in any snapshot (located beyond the "event horizon" of the database, we talked about this last time), 
but works strictly within one tabular page. Pointers to cleaned-up row versions are not freed because they can be referenced from the indexes, 
and the index is another page. In-page flushing never goes beyond a single table page, but it is very fast.
For the same reasons, the free space map is not updated; it also saves the freed space for updates rather than inserts. 
The visibility map is not updated either.
The fact that a page can be cleared while reading means that a read request (SELECT) can cause the pages to change. 
This is another such case, in addition to the previously discussed delayed hint bit change.
Let's see how it works with an example. Let's create a table and indexes on both columns.

=> CREATE TABLE hot(id integer, s char(2000)) WITH (fillfactor = 75);
=> CREATE INDEX hot_id ON hot(id);
=> CREATE INDEX hot_s ON hot(s);

If only Latin letters are stored in column s, then each line version will take 2004 bytes plus 24 bytes of the header. 
Set the fillfactor storage parameter to 75% - there will be enough space for three lines.

For convenience, let's recreate the already familiar function, adding two fields to the output:

=> CREATE FUNCTION heap_page(relname text, pageno integer)
RETURNS TABLE(ctid tid, state text, xmin text, xmax text, hhu text, hot text, t_ctid tid)
AS $$
SELECT (pageno,lp)::text::tid AS ctid,
       CASE lp_flags
         WHEN 0 THEN 'unused'
         WHEN 1 THEN 'normal'
         WHEN 2 THEN 'redirect to '||lp_off
         WHEN 3 THEN 'dead'
       END AS state,
       t_xmin || CASE
         WHEN (t_infomask & 256) > 0 THEN ' (c)'
         WHEN (t_infomask & 512) > 0 THEN ' (a)'
         ELSE ''
       END AS xmin,
       t_xmax || CASE
         WHEN (t_infomask & 1024) > 0 THEN ' (c)'
         WHEN (t_infomask & 2048) > 0 THEN ' (a)'
         ELSE ''
       END AS xmax,
       CASE WHEN (t_infomask2 & 16384) > 0 THEN 't' END AS hhu,
       CASE WHEN (t_infomask2 & 32768) > 0 THEN 't' END AS hot,
       t_ctid
FROM heap_page_items(get_raw_page(relname,pageno))
ORDER BY lp;
$$ LANGUAGE SQL;

And let's also create a function to look inside the index page:

=> CREATE FUNCTION index_page(relname text, pageno integer)
RETURNS TABLE(itemoffset smallint, ctid tid)
AS $$
SELECT itemoffset,
       ctid
FROM bt_page_items(relname,pageno);
$$ LANGUAGE SQL;

Let's see how in-page cleanup works. To do this, insert one line and change it several times:

=> INSERT INTO hot VALUES (1, 'A');
=> UPDATE hot SET s = 'B';
=> UPDATE hot SET s = 'C';
=> UPDATE hot SET s = 'D';

The page currently has four line versions:

=> SELECT * FROM heap_page('hot',0);
 ctid  | state  |   xmin   |   xmax   | hhu | hot | t_ctid 
-------+--------+----------+----------+-----+-----+--------
 (0,1) | normal | 3979 (c) | 3980 (c) |     |     | (0,2)
 (0,2) | normal | 3980 (c) | 3981 (c) |     |     | (0,3)
 (0,3) | normal | 3981 (c) | 3982     |     |     | (0,4)
 (0,4) | normal | 3982     | 0 (a)    |     |     | (0,4)
(4 rows)

As expected, we have just exceeded the fillfactor threshold. 
This is indicated by the difference between the pagesize and upper values: it exceeds the threshold of 75% of the page size, which is 6144 bytes.

=> SELECT lower, upper, pagesize FROM page_header(get_raw_page('hot',0));
 lower | upper | pagesize 
-------+-------+----------
    40 |    64 |     8192
(1 row)

So, the next time the page is accessed, an in-page cleanup should occur. Let's check it out.

=> UPDATE hot SET s = 'E';
=> SELECT * FROM heap_page('hot',0);
 ctid  | state  |   xmin   | xmax  | hhu | hot | t_ctid 
-------+--------+----------+-------+-----+-----+--------
 (0,1) | dead   |          |       |     |     | 
 (0,2) | dead   |          |       |     |     | 
 (0,3) | dead   |          |       |     |     | 
 (0,4) | normal | 3982 (c) | 3983  |     |     | (0,5)
 (0,5) | normal | 3983     | 0 (a) |     |     | (0,5)
(5 rows)

All out-of-date versions of lines (0,1), (0,2) and (0,3) are cleared; after that, a new version of the line (0.5) is added to the vacant space.
The line versions remaining after clearing are physically shifted towards higher page addresses so that all free space is represented by one contiguous fragment.
The values of the pointers are changed accordingly. Thanks to this, there are no problems with fragmentation of free space on the page.
Pointers to deleted row versions cannot be freed because they are referenced from the index page. 
Let's look at the first page of the hot_s index (because the zero is occupied by meta information):

=> SELECT * FROM index_page('hot_s',1);
 itemoffset | ctid  
------------+-------
          1 | (0,1)
          2 | (0,2)
          3 | (0,3)
          4 | (0,4)
          5 | (0,5)
(5 rows)

We will see the same picture in another index:

=> SELECT * FROM index_page('hot_id',1);
 itemoffset | ctid  
------------+-------
          1 | (0,5)
          2 | (0,4)
          3 | (0,3)
          4 | (0,2)
          5 | (0,1)
(5 rows)

You can note that pointers to table rows go backwards here, but this does not matter, since all versions of rows contain the same id = 1. 
But in the previous index the pointers are ordered by the values of s, and this essential.
On index access, PostgreSQL can get (0,1), (0,2) or (0,3) as the row version identifier. Then he will try to get the corresponding row from the table page, 
but due to the dead status of the pointer, he will find that such a version no longer exists and will ignore it. 
(In fact, the first time it detects that a table row version is missing, 
PostgreSQL will also change the status of the pointer in the index page to avoid re-accessing the table page.)

---------------
- HOT updates -
---------------

Why is it bad to keep references to all versions of a row in the index?
First, whenever a row changes, all the indexes created for the table have to be updated: once a new version has appeared, you must have references to it. 
And this must be done in any case, even if the fields that are not included in the index change. Obviously, this is not very effective.
Secondly, the indexes accumulate links to the historical versions of the row, 
which then have to be cleared along with the versions themselves (we will look at how this is done a little later).
Moreover, there is a peculiarity of the B-tree implementation in PostgreSQL. If there is not enough space on the index page to insert a new row, 
the page is split into two and all data is redistributed between them. This is called splitting the page. However, 
when rows are deleted, the two index pages are no longer "glued" into one. Because of this, 
the size of the index may not decrease even if a significant portion of the data is deleted.
Naturally, the more indexes are created on a table, the more difficulties you have to face.
However, if the value of a column that is not included in any index changes, 
then there is no point in creating an additional record in the B-tree containing the same key value. 
This is how an optimization called a HOT update - Heap-Only Tuple Update works.
With this update, there is only one record in the index page that refers to the very first version of the row in the table page. 
And already inside this table page, a chain of versions is organized:
lines that are changed and enter the chain are marked with the Heap Hot Updated bit;
rows that are not referenced from the index are marked with the Heap Only Tuple bit (that is, “table version of the row only”);
the usual linking of row versions through the ctid field is maintained.
If, during an index scan, PostgreSQL hits a table page and finds a version marked as Heap Hot Updated, 
it understands that it does not need to stop and goes further along the entire chain of updates. Of course, 
all row versions obtained this way are checked for visibility before being returned to the client.

To see how the HOT update works, let's drop one index and clear the table.

=> DROP INDEX hot_s;
=> TRUNCATE TABLE hot;

Let's repeat the insertion and update of the row.

=> INSERT INTO hot VALUES (1, 'A');
=> UPDATE hot SET s = 'B';

This is what we see in the tabular page:

=> SELECT * FROM heap_page('hot',0);
 ctid  | state  |   xmin   | xmax  | hhu | hot | t_ctid 
-------+--------+----------+-------+-----+-----+--------
 (0,1) | normal | 3986 (c) | 3987  | t   |     | (0,2)
 (0,2) | normal | 3987     | 0 (a) |     | t   | (0,2)
(2 rows)

The page contains a chain of changes:

the Heap Hot Updated flag indicates that you need to follow the ctid chain,
the Heap Only Tuple flag indicates that there are no index references to this version of the row.

With further changes, the chain will grow (within the page):

=> UPDATE hot SET s = 'C';
=> UPDATE hot SET s = 'D';
=> SELECT * FROM heap_page('hot',0);
 ctid  | state  |   xmin   |   xmax   | hhu | hot | t_ctid 
-------+--------+----------+----------+-----+-----+--------
 (0,1) | normal | 3986 (c) | 3987 (c) | t   |     | (0,2)
 (0,2) | normal | 3987 (c) | 3988 (c) | t   | t   | (0,3)
 (0,3) | normal | 3988 (c) | 3989     | t   | t   | (0,4)
 (0,4) | normal | 3989     | 0 (a)    |     | t   | (0,4)
(4 rows)

In this case, the index contains only one reference to the "head" of the chain:

=> SELECT * FROM index_page('hot_id',1);
 itemoffset | ctid  
------------+-------
          1 | (0,1)
(1 row)

We emphasize that HOT updates work if the fields being updated are not included in any index. 
Otherwise, in some index there would be a link directly to the new version of the row, which contradicts the idea of this optimization.
Optimization works only within one page, so additional traversal of the chain does not require access to other pages and does not degrade performance.
Fundamentally, in-page flushing works only within one table page and does not flush index pages.

----------------------------------
- In-page cleanup on HOT updates -
----------------------------------

A particular but important case of in-page scrubbing is the scrubbing of HOT updates.

As last time, we have already exceeded the fillfactor threshold, so the next update should result in an in-page flush. But this time, the page contains a chain of updates. The "head" of this HOT chain must always remain in its place, since the index refers to it, and the rest of the pointers can be freed: it is known that there are no external references to them.

In order not to touch the "head", double addressing is used: the pointer to which the index refers - in this case (0,1) - receives the "redirect" status, redirecting to the correct version of the string.

=> UPDATE hot SET s = 'E';
=> SELECT * FROM heap_page('hot',0);
 ctid  |     state     |   xmin   | xmax  | hhu | hot | t_ctid 
-------+---------------+----------+-------+-----+-----+--------
 (0,1) | redirect to 4 |          |       |     |     | 
 (0,2) | normal        | 3990     | 0 (a) |     | t   | (0,2)
 (0,3) | unused        |          |       |     |     | 
 (0,4) | normal        | 3989 (c) | 3990  | t   | t   | (0,2)
(4 rows)

Note that:

versions (0.1), (0.2) and (0.3) have been cleared,
The "head" pointer (0,1) remained, but received the redirect status,
the new version of the string is written in place (0,2), 
since this version was guaranteed to be unreferenced from the indices and the pointer was freed (unused).

Let's update a few more times:

=> UPDATE hot SET s = 'F';
=> UPDATE hot SET s = 'G';
=> SELECT * FROM heap_page('hot',0);
 ctid  |     state     |   xmin   |   xmax   | hhu | hot | t_ctid 
-------+---------------+----------+----------+-----+-----+--------
 (0,1) | redirect to 4 |          |          |     |     | 
 (0,2) | normal        | 3990 (c) | 3991 (c) | t   | t   | (0,3)
 (0,3) | normal        | 3991 (c) | 3992     | t   | t   | (0,5)
 (0,4) | normal        | 3989 (c) | 3990 (c) | t   | t   | (0,2)
 (0,5) | normal        | 3992     | 0 (a)    |     | t   | (0,5)
(5 rows)

The next update triggers in-page cleanup again:

=> UPDATE hot SET s = 'H';
=> SELECT * FROM heap_page('hot',0);
 ctid  |     state     |   xmin   | xmax  | hhu | hot | t_ctid 
-------+---------------+----------+-------+-----+-----+--------
 (0,1) | redirect to 5 |          |       |     |     | 
 (0,2) | normal        | 3993     | 0 (a) |     | t   | (0,2)
 (0,3) | unused        |          |       |     |     | 
 (0,4) | unused        |          |       |     |     | 
 (0,5) | normal        | 3992 (c) | 3993  | t   | t   | (0,2)
(5 rows)

Again, some of the versions are cleared, and the pointer to the "head" is shifted accordingly.

Takeaway: If there are frequent updates to non-index columns, 
it might make sense to decrease the fillfactor parameter to reserve some space on the page for updates. 
Of course, it should be borne in mind that the lower the fillfactor, the more unallocated space remains on the page and, accordingly, 
the physical size of the table increases.

--------------------------
- Breaking the HOT chain -
--------------------------

If there is not enough free space on the page to accommodate the new version of the row, the chain will break. 
The version of the row hosted on another page will have to be linked separately from the index.

To get this situation, let's start a parallel transaction and build a snapshot of the data in it.

|  => BEGIN ISOLATION LEVEL REPEATABLE READ;
|  => SELECT count(*) FROM hot;
|   count 
|  -------
|       1
|  (1 row)

The snapshot will prevent you from clearing the row versions on the page. Now we update in the first session:

=> UPDATE hot SET s = 'I';
=> UPDATE hot SET s = 'J';
=> UPDATE hot SET s = 'K';
=> SELECT * FROM heap_page('hot',0);
 ctid  |     state     |   xmin   |   xmax   | hhu | hot | t_ctid 
-------+---------------+----------+----------+-----+-----+--------
 (0,1) | redirect to 2 |          |          |     |     | 
 (0,2) | normal        | 3993 (c) | 3994 (c) | t   | t   | (0,3)
 (0,3) | normal        | 3994 (c) | 3995 (c) | t   | t   | (0,4)
 (0,4) | normal        | 3995 (c) | 3996     | t   | t   | (0,5)
 (0,5) | normal        | 3996     | 0 (a)    |     | t   | (0,5)
(5 rows)

The next time you refresh the page, there won't be enough space, but in-page cleanup won't be able to free up anything:

=> UPDATE hot SET s = 'L';

|  => COMMIT; -- снимок больше не нужен

=> SELECT * FROM heap_page('hot',0);
 ctid  |     state     |   xmin   |   xmax   | hhu | hot | t_ctid 
-------+---------------+----------+----------+-----+-----+--------
 (0,1) | redirect to 2 |          |          |     |     | 
 (0,2) | normal        | 3993 (c) | 3994 (c) | t   | t   | (0,3)
 (0,3) | normal        | 3994 (c) | 3995 (c) | t   | t   | (0,4)
 (0,4) | normal        | 3995 (c) | 3996 (c) | t   | t   | (0,5)
 (0,5) | normal        | 3996 (c) | 3997     |     | t   | (1,1)
(5 rows)

In version (0.5), we see a link to (1,1) leading to page 1.

=> SELECT * FROM heap_page('hot',1);
 ctid  | state  | xmin | xmax  | hhu | hot | t_ctid 
-------+--------+------+-------+-----+-----+--------
 (1,1) | normal | 3997 | 0 (a) |     |     | (1,1)
(1 row)

Now the index contains two lines, each of which points to the beginning of its HOT chain:

=> SELECT * FROM index_page('hot_id',1);
 itemoffset | ctid  
------------+-------
          1 | (1,1)
          2 | (0,1)
(2 rows)

Unfortunately, information about in-page cleanups and HOT updates is practically absent in the documentation, 
and the truth must be looked for in the source code. I recommend starting with README.HOT .

------------------
- Routine vacuum -
------------------

----------------------
- What cleaning does -
----------------------

In-page cleanup is quick, but only frees up some space. It works within a single table page and does not affect indexes.
The main, "normal" cleaning is performed by the VACUUM command and we will call it simply cleaning (and we will talk about autovacuum separately).
So, cleanup processes the table completely. It cleans out not only unnecessary row versions, but also references to them from all indexes.
Processing occurs in parallel with other activity in the system. In this case, the table and indexes can be used in the usual way for 
both reading and modification (however, the simultaneous execution of commands such as CREATE INDEX, ALTER TABLE and some others will not be possible).
In the table, only those pages are viewed in which some activity took place. For this, a visibility map is used (let me remind you 
that it marks pages that contain only fairly old versions of strings that are guaranteed to be visible in all data snapshots). 
Only pages not marked in the map are processed, and the map itself is updated.
In the process, the free space map is also updated to reflect the appeared free space in the pages.
As usual, let's create a table:

=> CREATE TABLE vac(
  id serial,
  s char(100)
) WITH (autovacuum_enabled = off);
=> CREATE INDEX vac_s ON vac(s);
=> INSERT INTO vac(s) VALUES ('A');
=> UPDATE vac SET s = 'B';
=> UPDATE vac SET s = 'C';

With the autovacuum_enabled parameter , we disable automatic cleaning. 
We will talk about it next time, but for now - for experiments - it is important for us to manage the cleaning manually.

Now the table has three versions of the row, and each is referenced from the index:

=> SELECT * FROM heap_page('vac',0);
 ctid  | state  |   xmin   |   xmax   | hhu | hot | t_ctid 
-------+--------+----------+----------+-----+-----+--------
 (0,1) | normal | 4000 (c) | 4001 (c) |     |     | (0,2)
 (0,2) | normal | 4001 (c) | 4002     |     |     | (0,3)
 (0,3) | normal | 4002     | 0 (a)    |     |     | (0,3)
(3 rows)

=> SELECT * FROM index_page('vac_s',1);
 itemoffset | ctid  
------------+-------
          1 | (0,1)
          2 | (0,2)
          3 | (0,3)
(3 rows)

After cleaning, the “dead” versions disappear and only one, actual, remains. And one link remains in the index too:

=> VACUUM vac;
=> SELECT * FROM heap_page('vac',0);
 ctid  | state  |   xmin   | xmax  | hhu | hot | t_ctid 
-------+--------+----------+-------+-----+-----+--------
 (0,1) | unused |          |       |     |     | 
 (0,2) | unused |          |       |     |     | 
 (0,3) | normal | 4002 (c) | 0 (a) |     |     | (0,3)
(3 rows)
=> SELECT * FROM index_page('vac_s',1);
 itemoffset | ctid  
------------+-------
          1 | (0,3)
(1 row)

Note that the first two pointers are unused, not dead, as would be the case with in-page clearing.

------------------------------------------------
- And once again about the transaction horizon -
------------------------------------------------

How does PostgreSQL determine which row versions are dead? We already covered the concept of a transaction horizon when we talked about snapshots, 
but this is such an important topic that it is not a sin to repeat it.

Let's start the previous experiment again.

=> TRUNCATE vac;
=> INSERT INTO vac(s) VALUES ('A');
=> UPDATE vac SET s = 'B';

But before updating the row again, let's start (but not end) another transaction. In our example, it will work at the Read Committed level, 
but should receive a real (not virtual) transaction number. For example, it can change or even just lock some rows in any table, not necessarily in vac:

|  => BEGIN;
|  => SELECT s FROM t FOR UPDATE;
|    s  
|  -----
|   FOO
|   BAR
|  (2 rows)

=> UPDATE vac SET s = 'C';

The table now has three rows and the index has three links. What happens after cleaning?

=> VACUUM vac;
=> SELECT * FROM heap_page('vac',0);
 ctid  | state  |   xmin   |   xmax   | hhu | hot | t_ctid 
-------+--------+----------+----------+-----+-----+--------
 (0,1) | unused |          |          |     |     | 
 (0,2) | normal | 4005 (c) | 4007 (c) |     |     | (0,3)
 (0,3) | normal | 4007 (c) | 0 (a)    |     |     | (0,3)
(3 rows)
=> SELECT * FROM index_page('vac_s',1);
 itemoffset | ctid  
------------+-------
          1 | (0,2)
          2 | (0,3)
(2 rows)

There are two versions of the row left in the table: cleanup decided that version (0,2) could not be deleted yet. The reason is, of course, 
in the database transaction horizon, which in our example is defined by an uncommitted transaction:

|  => SELECT backend_xmin FROM pg_stat_activity WHERE pid = pg_backend_pid();
|   backend_xmin 
|  --------------
|           4006
|  (1 row)

You can ask the cleanup to tell you what is happening:

=> VACUUM VERBOSE vac;
INFO:  vacuuming "public.vac"
INFO:  index "vac_s" now contains 2 row versions in 2 pages
DETAIL:  0 index row versions were removed.
0 index pages have been deleted, 0 are currently reusable.
CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s.
INFO:  "vac": found 0 removable, 2 nonremovable row versions in 1 out of 1 pages
DETAIL:  1 dead row versions cannot be removed yet, oldest xmin: 4006
There were 1 unused item pointers.
Skipped 0 pages due to buffer pins, 0 frozen pages.
0 pages are entirely empty.
CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s.
VACUUM

Note:

2 nonremovable row versions - 2 versions found in the table that cannot be deleted,
1 dead row versions cannot be removed yet - of which 1 is "dead",
oldest xmin shows the current horizon.

To reiterate the conclusion: the presence of long-lived transactions in the database (incomplete or really long running) can lead to growth (bloat) tables, 
no matter how often the cleanup is performed. Therefore, PostgreSQL does not mix well OLTP and OLAP loads in one database: 
reports that run for hours will prevent frequently updated tables from being cleaned up in time. 
A possible solution would be to create a separate "reporting" replica.

After the completion of an open transaction, the horizon shifts and the situation is corrected:

|  => COMMIT;

=> VACUUM VERBOSE vac;
INFO:  vacuuming "public.vac"
INFO:  scanned index "vac_s" to remove 1 row versions
DETAIL:  CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s
INFO:  "vac": removed 1 row versions in 1 pages
DETAIL:  CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s
INFO:  index "vac_s" now contains 1 row versions in 2 pages
DETAIL:  1 index row versions were removed.
0 index pages have been deleted, 0 are currently reusable.
CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s.
INFO:  "vac": found 1 removable, 1 nonremovable row versions in 1 out of 1 pages
DETAIL:  0 dead row versions cannot be removed yet, oldest xmin: 4008
There were 1 unused item pointers.
Skipped 0 pages due to buffer pins, 0 frozen pages.
0 pages are entirely empty.
CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s.
VACUUM

Now only the last actual version of the line remains in the page:

=> SELECT * FROM heap_page('vac',0);
 ctid  | state  |   xmin   | xmax  | hhu | hot | t_ctid 
-------+--------+----------+-------+-----+-----+--------
 (0,1) | unused |          |       |     |     | 
 (0,2) | unused |          |       |     |     | 
 (0,3) | normal | 4007 (c) | 0 (a) |     |     | (0,3)
(3 rows)

There is also only one entry in the index:

=> SELECT * FROM index_page('vac_s',1);
 itemoffset | ctid  
------------+-------
          1 | (0,3)
(1 row)

--------------------------
- What's going on inside -
--------------------------

The cleanup must process both the table and the indexes at the same time, and do so in a way that does not block other processes. How does she do it?
It all starts with scanning the table (taking into account the visibility map, as already noted). In the read pages, unnecessary versions of strings are determined and their identifiers (tid) are written to a special array. The array resides in the local memory of the cleanup process; a fragment with the size of maintenance_work_mem is allocated for it . The default value for this parameter is 64 MB. Note that this memory is allocated immediately in full, and not as needed. True, if the table is small, then the fragment is allocated less.
Then one of two things: either we reach the end of the table, or the memory allocated for the array will end. In either case, the index cleanup phase begins . To do this, each of the indexes created on the table is completely scanned for records that reference the stored versions of the rows. Found records are cleared from the index pages.
At this point, we get the following picture: in the indexes there are no longer references to unnecessary row versions, but in the table they still exist. This does not contradict anything: when executing the query, we either will not end up on dead row versions (with index access), or we will mark them when checking the visibility (when scanning the table).
After that, the phase of cleaning the table begins.... The table is scanned again to read the desired pages, flush the stored version of the rows from them, and free the pointers. We can do this because the links from the indexes are gone.
If on the first pass the table was not read completely, then the array is cleared and everything is repeated from the point where we left off.

In this way:

 * the table is always scanned twice;
 * if the cleanup removes so many row versions that they all do not fit in maintenance_work_mem memory, 
 then all indexes will be fully scanned as many times as necessary.

On large tables, this can take a significant amount of time and create a significant load on the system. Of course, requests will not block, 
but "extra" I / O is also unpleasant.
To speed up the process, it makes sense to either call the flush more often (so that not very many row versions are flushed each time), 
or allocate more memory.
Note in parentheses that starting with version 11, PostgreSQL can skip index scans unless absolutely necessary. 
This should make life easier for the owners of large tables to which rows are only added (but not changed).

--------------
- Monitoring -
--------------

How do you know if cleaning isn't doing the job in one pass?
We have already seen the first method: you can call the VACUUM command with the VERBOSE indication. 
Then information about the phases of work execution will be displayed on the console.
Second, since version 9.6 there is a pg_stat_progress_vacuum view, which also contains all the necessary information.
(There is also a third way - to output information to the message log, but this only works for autovacuum, which will be discussed next time.)

Let's insert more rows into the table so that the cleanup takes a noticeable amount of time, 
and we update all of them so that cleanup has something to do ...

=> TRUNCATE vac;
=> INSERT INTO vac(s) SELECT 'A' FROM generate_series(1,500000);
=> UPDATE vac SET s  = 'B';

Let's reduce the size of the memory allocated for the array of identifiers:

=> ALTER SYSTEM SET maintenance_work_mem = '1MB';
=> SELECT pg_reload_conf();

We start the cleanup and, while it is running, we turn to the pg_stat_progress_vacuum view several times:

=> VACUUM VERBOSE vac;

|  => SELECT * FROM pg_stat_progress_vacuum \gx
|  -[ RECORD 1 ]------+------------------
|  pid                | 6715
|  datid              | 41493
|  datname            | test
|  relid              | 57383
|  phase              | vacuuming indexes
|  heap_blks_total    | 16667
|  heap_blks_scanned  | 2908
|  heap_blks_vacuumed | 0
|  index_vacuum_count | 0
|  max_dead_tuples    | 174762
|  num_dead_tuples    | 174480

|  => SELECT * FROM pg_stat_progress_vacuum \gx
|  -[ RECORD 1 ]------+------------------
|  pid                | 6715
|  datid              | 41493
|  datname            | test
|  relid              | 57383
|  phase              | vacuuming indexes
|  heap_blks_total    | 16667
|  heap_blks_scanned  | 5816
|  heap_blks_vacuumed | 2907
|  index_vacuum_count | 1
|  max_dead_tuples    | 174762
|  num_dead_tuples    | 174480

Here, in particular, we see:

the name of the current phase (phase) - we talked about three main phases, but in general there are more of them ;
total number of table pages (heap_blks_total);
number of scanned pages (heap_blks_scanned);
number of already flushed pages (heap_blks_vacuumed);
number of index passes (index_vacuum_count).

The overall progress is determined by the ratio of heap_blks_vacuumed to heap_blks_total, 
but it should be borne in mind that this value does not change smoothly, but in “jerks” due to index scans. However, 
the main attention should be paid to the number of cleaning cycles - a value greater than 1 means that the allocated memory was not enough to complete 
the cleaning in one pass.

The output of the VACUUM VERBOSE command that has completed by this time will show the general picture:

INFO:  vacuuming "public.vac"
INFO:  scanned index "vac_s" to remove 174480 row versions
DETAIL:  CPU: user: 0.50 s, system: 0.07 s, elapsed: 1.36 s
INFO:  "vac": removed 174480 row versions in 2908 pages
DETAIL:  CPU: user: 0.02 s, system: 0.02 s, elapsed: 0.13 s
INFO:  scanned index "vac_s" to remove 174480 row versions
DETAIL:  CPU: user: 0.26 s, system: 0.07 s, elapsed: 0.81 s
INFO:  "vac": removed 174480 row versions in 2908 pages
DETAIL:  CPU: user: 0.01 s, system: 0.02 s, elapsed: 0.10 s
INFO:  scanned index "vac_s" to remove 151040 row versions
DETAIL:  CPU: user: 0.13 s, system: 0.04 s, elapsed: 0.47 s
INFO:  "vac": removed 151040 row versions in 2518 pages
DETAIL:  CPU: user: 0.01 s, system: 0.02 s, elapsed: 0.08 s
INFO:  index "vac_s" now contains 500000 row versions in 17821 pages
DETAIL:  500000 index row versions were removed.
8778 index pages have been deleted, 0 are currently reusable.
CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s.
INFO:  "vac": found 500000 removable, 500000 nonremovable row versions in 16667 out of 16667 pages
DETAIL:  0 dead row versions cannot be removed yet, oldest xmin: 4011
There were 0 unused item pointers.
0 pages are entirely empty.
CPU: user: 1.10 s, system: 0.37 s, elapsed: 3.71 s.
VACUUM

Here you can see that a total of three index passes were performed, each of which cleared 174480 pointers to dead string versions. 
Where does this number come from? One link (tid) takes 6 bytes, and 1024 * 1024/6 = 174762 is the number that we see in pg_stat_progress_vacuum.max_dead_tuples. 
In reality, it can be used a little less: this is how it is guaranteed that when reading the next page, all pointers to the "dead" versions will fit exactly into memory.

------------
- Analysis -
------------

Analysis, or, in other words, the collection of statistical information for the query planner, formally has nothing to do with cleaning. 
Nevertheless, we can perform analysis not only with the ANALYZE command, 
but also combine cleaning with analysis: VACUUM ANALYZE. This involves cleaning first and then analyzing - no savings.
But, as we will see later, automatic cleanup and automatic analysis are performed by the same process and are managed in a similar way.

---------------
- Vacuum full -
---------------

As we have seen, a simple cleanup frees up more space than an in-page cleanup, but it doesn't always solve the problem completely.
If the table or index for some reason has grown a lot in size, 
then the usual cleanup will free up space inside the existing pages: "holes" will appear in them, which will then be used to insert new versions of rows. 
But the number of pages will not change, and therefore, from the point of view of the operating system, 
the files will take up exactly the same amount of space as they did before cleaning. And this is bad because:
full table (or index) scan slows down;
a larger buffer cache may be required (after all, pages are stored, and the density of useful information decreases);
an “extra” level may appear in the index tree, which will slow down index access;
files take up extra disk space and in backups.

(The only exception is completely cleaned pages at the end of the file - such pages are “bite off” from the file and returned to the operating system.)
If the share of useful information in the files falls below a reasonable limit, the administrator can perform a complete cleanup of the table. 
In this case, the table and all its indexes are rebuilt completely from scratch, and the data is packed as compactly as possible 
(of course, taking into account the fillfactor parameter). During a rebuild, PostgreSQL rebuilds sequentially, first the table and then each of its indexes. 
New files are created for each object, and at the end of the rebuild, the old files are deleted. 
It should be borne in mind that additional space is required on the disk during operation.

To illustrate, let's again insert a number of rows into the table:

=> TRUNCATE vac;
=> INSERT INTO vac(s) SELECT 'A' FROM generate_series(1,500000);

How to evaluate the density of information? To do this, it is convenient to use a special extension:

=> CREATE EXTENSION pgstattuple;
=> SELECT * FROM pgstattuple('vac') \gx
-[ RECORD 1 ]------+---------
table_len          | 68272128
tuple_count        | 500000
tuple_len          | 64500000
tuple_percent      | 94.47
dead_tuple_count   | 0
dead_tuple_len     | 0
dead_tuple_percent | 0
free_space         | 38776
free_percent       | 0.06

The function reads the entire table and shows statistics on how much space is occupied by what data in the files. 
The main information that we are interested in now is the tuple_percent field: the percentage occupied by the payload. 
It is less than 100 because of the inevitable overhead for internal information within the page, but still quite high.

Different information is displayed for the index, but the avg_leaf_density field has the same meaning: the percentage of useful information (in leaf pages).

=> SELECT * FROM pgstatindex('vac_s') \gx
-[ RECORD 1 ]------+---------
version            | 3
tree_level         | 3
index_size         | 72802304
root_block_no      | 2722
internal_pages     | 241
leaf_pages         | 8645
empty_pages        | 0
deleted_pages      | 0
avg_leaf_density   | 83.77
leaf_fragmentation | 64.25

And here is the size of the table and index:

=> SELECT pg_size_pretty(pg_table_size('vac')) table_size,
  pg_size_pretty(pg_indexes_size('vac')) index_size;
 table_size | index_size 
------------+------------
 65 MB      | 69 MB
(1 row)

Now let's delete 90% of all lines. We select the lines for deletion at random so that in each page with a high probability at least one line remains:

=> DELETE FROM vac WHERE random() < 0.9;
DELETE 450189

What size will the objects be after normal cleaning?

=> VACUUM vac;
=> SELECT pg_size_pretty(pg_table_size('vac')) table_size,
  pg_size_pretty(pg_indexes_size('vac')) index_size;
 table_size | index_size 
------------+------------
 65 MB      | 69 MB
(1 row)

We can see that the size has not changed: regular cleaning cannot reduce the size of the files in any way. 
Although the density of information has obviously decreased by about 10 times:

=> SELECT vac.tuple_percent, vac_s.avg_leaf_density
FROM pgstattuple('vac') vac, pgstatindex('vac_s') vac_s;
 tuple_percent | avg_leaf_density 
---------------+------------------
          9.41 |             9.73
(1 row)

Now let's check what happens after a complete cleaning. The files are currently used by the table and indexes:

=> SELECT pg_relation_filepath('vac'), pg_relation_filepath('vac_s');
 pg_relation_filepath | pg_relation_filepath 
----------------------+----------------------
 base/41493/57392     | base/41493/57393
(1 row)

=> VACUUM FULL vac;
=> SELECT pg_relation_filepath('vac'), pg_relation_filepath('vac_s');
 pg_relation_filepath | pg_relation_filepath 
----------------------+----------------------
 base/41493/57404     | base/41493/57407
(1 row)

The files are now replaced with new ones. The size of the table and index has decreased significantly, and the information density has increased accordingly:

=> SELECT pg_size_pretty(pg_table_size('vac')) table_size,
  pg_size_pretty(pg_indexes_size('vac')) index_size;
 table_size | index_size 
------------+------------
 6648 kB    | 6480 kB
(1 row)
=> SELECT vac.tuple_percent, vac_s.avg_leaf_density
FROM pgstattuple('vac') vac, pgstatindex('vac_s') vac_s;
 tuple_percent | avg_leaf_density 
---------------+------------------
         94.39 |            91.08
(1 row)

Note that the information density in the index has even increased from the original one. 
Rebuilding the index (B-tree) on the existing data is more advantageous than inserting the data into the existing index line by line.
The pgstattuple extension functions that we have used read the entire table. If the table is large, then this is inconvenient, 
and therefore there is also a pgstattuple_approx function there, which skips the pages marked in the visibility map and shows approximate numbers.
An even faster, but even less accurate way is to estimate the ratio of data volume to file size in the system directory. 
You can find options for such queries on the wiki .
Full cleaning does not imply regular use, since it completely blocks all work with the table (including the execution of queries to it) 
for the entire duration of its work. It is clear that on a heavily used system this may not be acceptable. Locks will be discussed separately, 
but for now we will limit ourselves to mentioning the pg_repack extension , which locks the table only for a short time at the end of the work.

--------------------
- Similar commands -
--------------------

There are several commands that also rebuild tables and indexes completely, and are similar to a complete cleanup. 
All of them completely block work with the table, they all delete old data files and create new ones.
CLUSTER is similar in every way to VACUUM FULL, but additionally physically orders the row versions according to one of the available indices. 
This gives the planner the ability to use index access more efficiently in some cases. However, 
you must understand that clustering is not supported: with subsequent changes to the table, the physical order of the row versions will be violated.
The REINDEX command rebuilds a single index on a table. In fact, VACUUM FULL and CLUSTER use this command to rebuild indexes.
The TRUNCATE command works logically in the same way as DELETE - it deletes all table rows. But DELETE, as already discussed, 
only marks the row versions as deleted, which requires further cleanup. TRUNCATE just creates a new, clean file. As a rule, this works faster, 
but keep in mind that TRUNCATE will completely block work with the table for the entire time until the end of the transaction.

--------------
- Autovacuum -
--------------

We have already said that a normal cleanup under normal conditions (when no one holds the transaction horizon for a long time) should do its job. 
The question is how often to call it.
If you flush the changing table too rarely, it will grow larger than you would like. In addition, 
the next cleanup may require several index passes if there are too many changes.
If you flush the table too often, then instead of doing useful work, the server will constantly be doing maintenance - also bad.
Note that running a regular cleanup on a schedule does not solve the problem in any way, because the load can change over time. 
If the table is being updated more actively, then it should be cleared more often.
Automatic cleanup is the very mechanism that allows you to trigger cleanup depending on the activity of changes in the tables.
When autovacuum is enabled (configuration parameter autovacuum ), there is always an autovacuum launcher process in the system, which schedules work, 
and the autovacuum worker worker processes are engaged in real cleaning, several instances of which can work in parallel.
The autovacuum launcher process compiles a list of databases that have any activity. Activity is determined by statistics, 
and in order for it to be collected, the track_counts parameter must be set . Never turn off autovacuum and track_counts, 
otherwise autovacuum won't work.

Once at autovacuum_naptimethe autovacuum launcher process launches (via the postmaster process) a worker process for each database in the list. 
In other words, if there is some activity in the database, then worker processes will come to it with the autovacuum_naptime interval. 
For this, if there are several active databases (N pieces), then worker processes are launched N times more often than autovacuum_naptime. 
However, the total number of simultaneously working worker processes is limited by the autovacuum_max_workers parameter .

Once launched, the workflow connects to the database specified to it and begins by building a list:
all tables, materialized views and toast tables that need to be cleaned up,
all tables and materialized views that require analysis (toast tables are not parsed, because they are always accessed by index).
Further, the workflow cleans and / or analyzes the selected objects in turn and ends when cleaning is complete.

If the process does not have time to complete all the intended work in autovacuum_naptime, 
the autovacuum launcher process will send another worker process to the same database, and they will work together. 
Together simply means that the second process will build its list of tables and walk through it. 
Thus, different tables will be processed in parallel, 
but there is no parallelism at the level of one table - if one of the worker processes is already working on the table, 
the other will skip it and move on.

The discussion about the need for parallel processing has been going on for a long time, but the patch has not yet been adopted.
Now let's take a closer look at what “requires cleaning” and “requires analysis” mean.

------------------------------
- Which tables need cleaning -
------------------------------

It is considered that flushing is necessary if the number of “dead”, that is, irrelevant, row versions exceeds a specified threshold. 
The number of dead versions is continuously collected by the statistics collector and stored in the pg_stat_all_tables table. 
And the threshold is set by two parameters:

autovacuum_vacuum_threshold defines the absolute value (in pieces),
autovacuum_vacuum_scale_factor defines the proportion of rows in the table.

The final formula is: cleaning is required if pg_stat_all_tables.n_dead_tup> = autovacuum_vacuum_threshold + autovacuum_vacuum_scale_factor * pg_class.reltupes.

The default settings are autovacuum_vacuum_threshold = 50 and
autovacuum_vacuum_scale_factor = 0.2. The main parameter here, of course, 
is autovacuum_vacuum_scale_factor - it is it that is important for large tables (and it is with them that possible problems are associated). 
The value of 20% seems to be highly overestimated, most likely it will need to be significantly reduced.

The optimal parameter values may differ for different tables, depending on their size and the nature of the changes. 
It makes sense to set generally adequate values, 
and - if necessary - customize parameters at the level of some tables in a special way using storage parameters:

autovacuum_vacuum_threshold and toast.autovacuum_vacuum_threshold ,
autovacuum_vacuum_scale_factor and toast.autovacuum_vacuum_scale_factor .

In order not to get confused, this should be done only for a small number of tables, which stand out among others by the volume or intensity of changes, 
and only when the globally set values are not suitable.

In addition, autovacuum can be disabled at the table level (although it's hard to think of a reason why this would be necessary):

autovacuum_enabled and toast.autovacuum_enabled .

For example, last time we created the vac table with autovacuum disabled so that - for demonstration purposes - we could manually control the vacancy. 
The storage parameter can be changed as follows:

=> ALTER TABLE vac SET (autovacuum_enabled = off);

To formalize all of the above, let's create a view showing which tables are currently in need of cleaning. 
It will use a function that returns the current value of the parameter, considering that it can be overridden at the table level:

=> CREATE FUNCTION get_value(param text, reloptions text[], relkind "char")
RETURNS float
AS $$
  SELECT coalesce(
    -- if the storage parameter is given, then we take it
    (SELECT option_value
     FROM   pg_options_to_table(reloptions)
     WHERE  option_name = CASE
              - for toast tables the parameter name is different
              WHEN relkind = 't' THEN 'toast.' ELSE ''
            END || param
    ),
    -- otherwise we take the value of the configuration parameter
    current_setting(param)
  )::float;
$$ LANGUAGE sql;

And here is the view:

=> CREATE VIEW need_vacuum AS
  SELECT st.schemaname || '.' || st.relname tablename,
         st.n_dead_tup dead_tup,
         get_value('autovacuum_vacuum_threshold', c.reloptions, c.relkind) +
         get_value('autovacuum_vacuum_scale_factor', c.reloptions, c.relkind) * c.reltuples
         max_dead_tup,
         st.last_autovacuum
  FROM   pg_stat_all_tables st,
         pg_class c
  WHERE  c.oid = st.relid
  AND    c.relkind IN ('r','m','t');

---------------------------------
- Which tables require analysis -
---------------------------------

With autoanalysis, the situation is about the same. 
It is considered that the analysis is required for those tables for which the number of modified (since the last analysis) 
row versions exceeds the threshold value set by two similar parameters: 
pg_stat_all_tables.n_mod_since_analyze> = autovacuum_analyze_threshold + autovacuum_analyze_scale_factor * pg_class.reltupes.

The default autoanalyze settings are slightly different: autovacuum_analyze_threshold = 50 and autovacuum_analyze_scale_factor = 0.1. 
They can also be defined at the level of storage parameters for individual tables:

autovacuum_analyze_threshold ,
autovacuum_analyze_scale_factor

Since toast tables are not analyzed, there are no corresponding parameters for them.

Let's create a view for analysis:

=> CREATE VIEW need_analyze AS
  SELECT st.schemaname || '.' || st.relname tablename,
         st.n_mod_since_analyze mod_tup,
         get_value('autovacuum_analyze_threshold', c.reloptions, c.relkind) +
         get_value('autovacuum_analyze_scale_factor', c.reloptions, c.relkind) * c.reltuples
         max_mod_tup,
         st.last_autoanalyze
  FROM   pg_stat_all_tables st,
         pg_class c
  WHERE  c.oid = st.relid
  AND    c.relkind IN ('r','m');
  
  -----------
  - Example -
  -----------
  
  For experiments, set the following parameter values:

=> ALTER SYSTEM SET autovacuum_naptime = ‘1s’; -- чтобы долго не ждать
=> ALTER SYSTEM SET autovacuum_vacuum_scale_factor = 0.03;  -- 3%
=> ALTER SYSTEM SET autovacuum_vacuum_threshold = 0;
=> ALTER SYSTEM SET autovacuum_analyze_scale_factor = 0.02; -- 2%
=> ALTER SYSTEM SET autovacuum_analyze_threshold = 0;
=> SELECT pg_reload_conf();
 pg_reload_conf
----------------
 t
(1 row)

Now let's create a table similar to the one we used last time and insert a thousand rows into it. 
Autovacuum is disabled at the table level, and we'll enable it ourselves. If this is not done, then the examples will not be reproducible, 
since the autovacuum may be triggered at the wrong time.

=> CREATE TABLE autovac(
  id serial,
  s char(100)
) WITH (autovacuum_enabled = off);
=> INSERT INTO autovac SELECT g.id,'A' FROM generate_series(1,1000) g(id);

Here's what our cleanup view will show:

=> SELECT * FROM need_vacuum WHERE tablename = 'public.autovac';
   tablename    | dead_tup | max_dead_tup | last_autovacuum 
----------------+----------+--------------+-----------------
 public.autovac |        0 |            0 | 
(1 row)

There are two points to note here. First, max_dead_tup = 0, even though 3% of 1000 rows is 30 rows. 
The point is that we do not yet have statistics on the table, since INSERT does not update it by itself. 
Until our table is analyzed, the zeros will remain, since pg_class.reltuples = 0. However, let's take a look at the second view for analysis:

=> SELECT * FROM need_analyze WHERE tablename = 'public.autovac';
   tablename    | mod_tup | max_mod_tup | last_autoanalyze 
----------------+---------+-------------+------------------
 public.autovac |    1000 |           0 | 
(1 row)

Since 1000 rows have changed (added) in the table, and this is greater than zero, autoanalysis should work. Let's check this:

=> ALTER TABLE autovac SET (autovacuum_enabled = on);

After a short pause, we see that the table has been analyzed and instead of zeros in max_mod_tup we see correct 20 lines:

=> SELECT * FROM need_analyze WHERE tablename = 'public.autovac';
   tablename    | mod_tup | max_mod_tup |       last_autoanalyze        
----------------+---------+-------------+-------------------------------
 public.autovac |       0 |          20 | 2019-05-21 11:59:48.465987+03
(1 row)

=> SELECT reltuples, relpages FROM pg_class WHERE relname = 'autovac';
 reltuples | relpages 
-----------+----------
      1000 |       17
(1 row)

Let's go back to autovacuum:

=> SELECT * FROM need_vacuum WHERE tablename = 'public.autovac';
   tablename    | dead_tup | max_dead_tup | last_autovacuum
----------------+----------+--------------+-----------------
 public.autovac |        0 |           30 |
(1 row)

Max_dead_tup, as we can see, has already been fixed. The second point to pay attention to is dead_tup = 0. 
Statistics show that there are no dead row versions in the table ... and this is true. There is nothing to clear in our table yet. 
So any table used only in the append-only mode will not be cleared and, therefore, the visibility map will not be updated for it. 
This makes it impossible to use only index-only scan.

(Next time we will see that the cleanup will come to the append-only table sooner or later, but this will happen very rarely.) The

practical takeaway: if it is important to use only index scans, you may need to invoke cleanup manually.

Now, turn off autovacuum again and update 31 rows - one more than the threshold.

=> ALTER TABLE autovac SET (autovacuum_enabled = off);
=> UPDATE autovac SET s = 'B' WHERE id <= 31;
=> SELECT * FROM need_vacuum WHERE tablename = 'public.autovac';
   tablename    | dead_tup | max_dead_tup | last_autovacuum 
----------------+----------+--------------+-----------------
 public.autovac |       31 |           30 | 
(1 row)

Now the autovacuum trigger condition is met. Turn on autovacuum and after a short pause we will see that the table has been processed:

=> ALTER TABLE autovac SET (autovacuum_enabled = on);
=> SELECT * FROM need_vacuum WHERE tablename = 'public.autovac';
   tablename    | dead_tup | max_dead_tup |        last_autovacuum        
----------------+----------+--------------+-------------------------------
 public.autovac |        0 |           30 | 2019-05-21 11:59:52.554571+03
(1 row)

----------------
- Load control -
----------------

Cleaning does not block other processes because it works page by page, 
but it nevertheless creates a load on the system and can have a noticeable effect on performance.

----------------------------------
- Regulation for normal cleaning -
----------------------------------

In order to be able to control the intensity of cleaning and therefore its effect on the system, the process alternates between working and waiting. 
The cleanup performs approximately vacuum_cost_limit conditional units of work and then sleeps for vacuum_cost_delay  ms.

The default settings are vacuum_cost_limit = 200, vacuum_cost_delay = 0. The last zero effectively means that (normal) vacuuming does not sleep, 
so the specific vacuum_cost_limit value does not matter. This is done with the understanding that if the administrator had to start VACUUM manually, 
then he probably wants to clean up as quickly as possible.

Nevertheless, if you still set the sleep time, 
then the specified invacuum_cost_limit The amount of work will be the sum of the costs of working with pages in the buffer cache. 
Each page hit is rated as follows:

 * if the page was found in the buffer cache, then vacuum_cost_page_hit = 1;
 * if not found, then vacuum_cost_page_miss = 10;
 * if not found, and even had to evict a dirty page from the buffer, then vacuum_cost_page_dirty = 20.

That is, with the default vacuum_cost_limit settings, 200 pages from the cache, or 20 pages from disk, or 10 pages with preemption can be processed in one go. 
It is clear that these are rather arbitrary numbers, but there is no point in choosing them more precisely.

--------------------------------
- Regulation for auto-cleaning -
--------------------------------

Load control for automatic cleaning works in the same way as for normal cleaning. 
But so that a manual cleanup and autovacuum can work with different intensities, 
we have made our own parameters for autovacuum : autovacuum_vacuum_cost_limit and autovacuum_vacuum_cost_delay. 
If these parameters are set to -1, then the value from vacuum_cost_limit and / or vacuum_cost_delay is used .

By default autovacuum_vacuum_cost_limit = -1 (i.e. vacuum_cost_limit = 200 is used ) and autovacuum_vacuum_cost_delay = 20ms. 
On modern equipment with these numbers, auto-cleaning will work very, very slowly.

In version 12, the autovacuum_vacuum_cost_delay value will be reduced to 2ms, which can be considered a more suitable first approximation.
In addition, it should be borne in mind that the limit set by these parameters is common to all work processes. 
In other words, as the number of simultaneous worker processes changes, the total load will remain constant. Therefore, if the task is to increase the performance of autovacuum, then when adding worker processes it is worth increasing autovacuum_vacuum_cost_limit .

-------------------------------
- Memory usage and monitoring -
-------------------------------

The last time we looked, like cleaning uses memory size maintenance_work_mem storage identifiers row versions to be cleaned.
Autocleaning does exactly the same. But there can be many concurrent processes if you set autovacuum_max_workers to a high value. 
In addition, all memory is allocated immediately and completely, and not out of necessity. 
Therefore, you can set your own limit for the autovacuum workflow using the autovacuum_work_mem parameter. 
By default, this parameter is -1, which means it is not used.

As mentioned, cleanup can work with minimal memory. But if indexes are created on the table, 
then a small valuemaintenance_work_mem can lead to repeated index scans. The same is true for autovacuum. Ideally, 
you should choose such a minimum autovacuum_work_mem value , at which repeated scans do not occur.

We have seen that the VERBOSE parameter can be used to monitor purge (but cannot be specified for autovacuum) or the pg_stat_progress_vacuum view 
(but it only shows current information). Therefore, the main method for monitoring autovacuum is the log_autovacuum_min_duration parameter, 
which outputs information to the server message log. It is off by default (set to -1). 
There is a reason to enable this parameter (with a value of 0, information about all autovacuum starts will be displayed) and watch the numbers.

This is what the output looks like:

=> ALTER SYSTEM SET log_autovacuum_min_duration = 0;
=> SELECT pg_reload_conf();
 pg_reload_conf 
----------------
 t
(1 row)

=> UPDATE autovac SET s = 'C' WHERE id <= 31;

student$ tail -n 7 /var/log/postgresql/postgresql-11-main.log
2019-05-21 11:59:55.675 MSK [9737] LOG:  automatic vacuum of table "test.public.autovac": index scans: 0
	pages: 0 removed, 18 remain, 0 skipped due to pins, 0 skipped frozen
	tuples: 31 removed, 1000 remain, 0 are dead but not yet removable, oldest xmin: 4040
	buffer usage: 78 hits, 0 misses, 0 dirtied
	avg read rate: 0.000 MB/s, avg write rate: 0.000 MB/s
	system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s
2019-05-21 11:59:55.676 MSK [9737] LOG:  automatic analyze of table "test.public.autovac" system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s

All the necessary information is here.

Recall that often you should not increase the memory size, but rather reduce the flushing threshold so that less data is processed at a time.

It might also make sense to monitor the length of the list of tables to clean using the views above. 
An increase in the length of the list will indicate that autovacuum does not have time to do its job and a change in settings is required.


https://habr.com/ru/company/postgrespro/blog/452320/
https://habr.com/ru/company/postgrespro/blog/452762/
https://habr.com/ru/company/postgrespro/blog/455590/





