#############################
# POSTGRES MVCC INSULATION  #
#############################

* Isolation as understood by the standard and PostgreSQL (this article);
* Layers, files, pages - what's going on at the physical level;
* Row Versions, Virtual and Nested Transactions;
* Snapshots and row version visibility, event horizon;
* In-page cleaning and HOT updates;
* Routine vacuum;
* Automatic cleaning (autovacuum);
* Transaction counter overflow and freeze;

What Is Insulation And Why It Matters

Probably, everyone at least knows about the existence of transactions, met the abbreviation ACID and heard about isolation levels. 
But we still have to meet the opinion that this is de theory, which is not necessary in practice. So I'll spend some time trying to explain why this really matters.
You are unlikely to be happy if the application receives incorrect data from the database, or if the application writes incorrect data to the database.
But what is “correct” data? It is known that at the database level can create integrity constraints (integrity constraints, such as NOT NULL or UNIQUE). 
If the data always satisfies the integrity constraints (and this is so, because the DBMS guarantees it), then it is complete.

Correct and holistic- same? Not really. Not all constraints can be formulated at the database level. Some of the restrictions are too complex, for example, 
they cover several tables at once. And even if the constraint could in principle be defined in the database, but for some reason they did not do it, 
this does not mean that it can be violated.

So, correctness is stricter than integrity , but what it is, we do not know exactly. It remains to admit that the standard of correctness is an application that, 
as we would like to believe, is written correctly and never makes mistakes. In any case, if the application does not violate the integrity, but violates the correctness, 
the DBMS will not know about it and will not catch its hand.

In what follows, we will call correctness by the termcoherence (consistency).
Let's assume, however, that the application only executes the correct sequences of statements. What then is the role of the DBMS if the application is already correct?
First, it turns out that the correct sequence of statements can temporarily break the data consistency, 
and this - oddly enough - is normal. A hackneyed but understandable example is the transfer of funds from one account to another. 
The consistency rule might sound like this: the transfer never changes the total amount of money in the accounts
(Such a rule is rather difficult to write in SQL as an integrity constraint, so it exists at the application level and is invisible to the DBMS). 
The transfer consists of two operations: the first decreases funds on one account, the second increases on another. The first operation breaks the data consistency, 
the second restores.

A good exercise is to implement the above rule at the integrity constraint level. Are you weak?
What if the first operation succeeds and the second fails? After all, it's easy: during the second operation, electricity may disappear, 
the server may fall, division by zero may occur - but you never know. It is clear that consistency will be violated, and this cannot be allowed. 
In principle, it is possible to resolve such situations at the application level at the cost of incredible efforts, but, fortunately, it is not necessary: 
the DBMS takes care of it. But for this she must know that the two operations make up an indivisible whole. That is, a transaction .
It turns out to be interesting: knowing that operations constitute a transaction, the DBMS helps maintain consistency by guaranteeing the atomicity of transactions, 
while not knowing anything about specific consistency rules.
But there is also a second, more subtle point. As soon as several simultaneously working transactions appear in the system, which are absolutely correct one at a time, 
together they may work incorrectly. This is due to the fact that the order of operations is mixed: 
it cannot be assumed that all the operations of one transaction are performed first, and only then all the operations of another.
A note about simultaneity. Indeed, transactions can work simultaneously on a system with a multi-core processor, with a disk array, etc. 
But all the same reasoning is also true for a server that executes commands sequentially, in a time-sharing mode: one transaction is executed so many cycles, 
another one ... The term concurrent execution is sometimes used to summarize .
Situations where valid transactions do not work together correctly are called concurrency anomalies .
A simple example: if an application wants to get the correct data from the database, then at least it should not see changes in other uncommitted transactions. 
Otherwise, you can not only get inconsistent data, but also see something that has never been in the database (if the transaction is canceled). 
This anomaly is called dirty reading .

If there are other, more complex anomalies, which we will deal with a little later.
Of course, you cannot refuse concurrent execution: otherwise, what kind of performance can we talk about? But you can’t work with incorrect data either.
And again the DBMS comes to the rescue. You can make transactions run as if sequentially, as if one after the other. In other words, it is isolated from each other. 
In reality, the DBMS can perform intermixing operations, 
but at the same time guarantee that the result of the simultaneous execution will match the result of any of the possible sequential executions. 
This removes any possible anomalies.

So, we come to the definition:

A transaction is a set of operations performed by an application that moves a database from one correct state to another correct state (consistency), 
provided that the transaction is complete (atomicity) and without interference from other transactions (isolation).

This definition combines the first three letters of ACID. They are so closely related to each other that there is simply no point in considering one without the other. 
In fact, it is difficult to tear off the letter D (durability). After all, when a system crashes, changes in uncommitted transactions remain in it, 
with which you have to do something to restore data consistency.

Everything would be fine, but the implementation of complete isolation is a technically difficult task, associated with a decrease in system throughput. 
Therefore, in practice, very often (not always, but almost always) weakened isolation is used, which prevents some, but not all, anomalies. 
This means that part of the work to ensure the correctness of the data falls on the application. 
That is why it is very important to understand what isolation level is used in the system, what guarantees it gives and what it does not, 
and how to write correct code under such conditions.

------------------------------------------------------
- Isolation levels and anomalies in the SQL standard -
------------------------------------------------------

The SQL standard has long described four isolation levels. 
These levels are determined by enumerating anomalies that are allowed or not allowed when transactions are being executed at this level at the same time. 
Therefore, to talk about these levels, you need to get acquainted with anomalies.

Let me emphasize that in this part we are talking about a standard, that is, about a certain theory on which practice relies heavily, 
but from which at the same time it is quite at odds with. Therefore, all examples here are speculative. 
They will use the same operations on customer accounts: this is quite obvious, although, admittedly, 
it has nothing to do with how banking operations are arranged in reality

---------------
- Lost update -
---------------

Let's start with the lost update . This anomaly occurs when two transactions read the same row in a table, then one transaction updates that row, 
and then the second transaction also updates the same row, ignoring the changes made by the first transaction.

For example, two transactions are going to increase the amount on the same account by 100 ₽. The first transaction reads the current value (1000 ₽), 
then the second transaction reads the same value. The first transaction increases the amount (it turns out 1100 ₽) and writes this value. 
The second transaction does the same - it receives the same 1100 ₽ and writes them down. As a result, the client lost 100 ₽.

A lost update is not allowed by the standard at any isolation level.

-----------------------------------
- Dirty Read and Read Uncommitted -
-----------------------------------

We already got acquainted with dirty reading above. This anomaly occurs when a transaction reads uncommitted changes made by another transaction.
For example, the first transaction transfers all the money from the customer's account to another account, but does not commit the change. 
Another transaction reads the account balance, receives $ 0, 
and refuses to withdraw cash from the client - even though the first transaction is aborted and reverses its changes, so the value 0 never existed in the database.
Dirty reads are allowed by the standard at the Read Uncommitted level.

------------------------------------------
- Non-repetitive Read and Read Committed -
------------------------------------------

A nonrepeatable read anomaly occurs when a transaction reads the same row twice, and between reads, a second transaction modifies (or deletes) that row and commits the changes. Then the first transaction will get different results.
For example, suppose a consistency rule prohibits negative amounts in customer accounts. 
The first transaction is going to reduce the amount on the account by 100 ₽. She checks the current value, gets 1000 ₽ and decides that a decrease is possible. 
At this time, the second transaction reduces the amount on the account to zero and commits the changes. If now the first transaction re-checked the amount, 
it would receive 0 ₽ (but it has already decided to decrease the value, and the account “goes into negative territory”).
Non-repetitive reads are allowed by the standard at the Read Uncommitted and Read Committed levels. But Read Committed does not allow dirty reading.

------------------------------------
- Phantom Read and Repeatable Read -
------------------------------------

A phantom read occurs when a transaction reads a set of rows twice for the same condition, and between reads, 
the second transaction adds rows that satisfy this condition (and commits the changes). Then the first transaction will receive different sets of rows.
For example, suppose a consistency rule prohibits a customer from having more than 3 accounts . The first transaction is about to open a new account, 
checks their current number (say 2) and decides that opening is possible. At this time, the second transaction also opens a new account for the client and fixes the changes. 
If now the first transaction rechecked the quantity, it would receive 3 (but it is already opening another account and the client has 4).
Phantom reading is allowed by the standard at the Read Uncommitted, Read Committed and Repeatable Read levels. But at the Repeatable Read level, 
non-repeated reads are not allowed.

--------------------------------------
- Lack of anomalies and Serializable -
--------------------------------------

The standard also defines another level - Serializable - at which no anomalies are allowed. This is not at all the same as prohibiting lost updates and dirty, 
non-repetitive and phantom reading.
The fact is that there are significantly more known anomalies than are listed in the standard, and an unknown number are still unknown.
The Serializable layer should prevent all anomalies in general . This means that at this level, the application developer does not need to think about concurrency. 
If the transactions execute the correct sequence of statements, working alone, the data will be consistent even when these transactions are running simultaneously.

-----------------
- Summary plate -
-----------------

Now you can give a well-known table. But here, for the sake of clarity, the last column is added to it, which is not in the standard.

------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+
	                |   lost changes  |   dirty reading   |   non-repetitive reading    | 	phantom reading	  |   other anomalies |
------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+                  
Read Uncommitted	|     -           |     yes           |     yes                     |     yes             |     yes           |
Read Committed    |     -           |       -           |     yes                     |     yes             |     yes           |
Repeatable Read   |     -           |       -           |       -                     |     yes             |     yes           |
Serializable	    |     -           |       -           |       -                     |     -               |     yes           |
------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+

---------------------------------
- Why exactly these anomalies?  -
---------------------------------

Why, of the many possible anomalies in the standard, only a few are listed and why exactly such?
Probably, nobody knows for certain. But practice here definitely outstripped theory, 
so it is possible that other anomalies were not thought of at that time (talking about the SQL: 92 standard).
In addition, it was assumed that the isolation should be built on locks. 
The idea behind a widely used two-phase blocking protocol(2PL) is that during execution the transaction locks the rows it is working with, 
and upon completion it releases the locks. To simplify a lot, the more locks a transaction acquires, the better it is isolated from other transactions. 
But the performance of the system suffers the more, because instead of working together, transactions begin to line up in a queue for the same lines.
It seems to me that the difference between the isolation levels of the standard is due precisely to the number of required locks.
If the transaction locks the modified rows from being modified, but not from being read, we get the Read Uncommitted level: no lost changes are allowed, 
but uncommitted data can be read.
If a transaction locks mutable rows from both read and change, we get the Read Committed level: uncommitted data cannot be read, but when the row is re-accessed, 
a different value can be obtained (non-repeated read).
If a transaction locks both readable and modified rows from both reading and modification, we get the Repeatable Read level: rereading the row will return the same value.
But there is a problem with Serializable: it is impossible to lock a row that does not exist yet. Because of this, 
the possibility of a phantom reading remains: another transaction can add (but not delete) a row that matches the conditions of a previously executed query, 
and this row will be resampled.

Therefore, to implement the Serializable level, ordinary locks are not enough - you need to lock not rows, but conditions (predicates). 
These locks were called predicate locks . They were proposed back in 1976, 
but their practical applicability is limited to rather simple conditions for which it is clear how to combine two different predicates. As far as I know, 
it did not come to the implementation of such locks in any system.

-----------------------------------
- Isolation levels in PostgreSQL  -
-----------------------------------

Over time, replace the locking transaction management protocols came isolation protocol based on images (Snapshot Isolation). 
Its idea is that each transaction operates with a consistent snapshot of data at a specific point in time, 
which includes only those changes that were committed before the moment the snapshot was taken.

This isolation automatically prevents dirty reads. Formally, in PostgreSQL you can specify the Read Uncommitted level, 
but it will work exactly the same as Read Committed. Therefore, we will not talk about the Read Uncommitted level further.

PostgreSQL implements multiversiona variant of such a protocol. The idea of multiversion is that several versions of the same row can coexist in a DBMS. 
This allows you to build a snapshot of the data using the existing versions and get by with a minimum of blocking. In fact, 
only repeated changes to the same row are blocked. All other operations are performed concurrently: writers never block read transactions, and readers never block anyone.

Due to the use of snapshots of data, isolation in PostgreSQL is stricter than required by the standard: the Repeatable Read level does not allow not only non-repeatable, 
but also phantom reads (although it does not provide complete isolation). And this is achieved without loss of efficiency.

------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+
	                |   lost changes  |   dirty reading   |   non-repetitive reading    | 	phantom reading	  |   other anomalies |
------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+
Read Uncommitted	|     -           |       -           |     yes                     |     yes             |     yes           |
Read Committed    |     -           |       -           |     yes                     |     yes             |     yes           |
Repeatable Read   |     -           |       -           |       -                     |     -               |     yes           |
Serializable	    |     -           |       -           |       -                     |     -               |     -             |
------------------+-----------------+-------------------+-----------------------------+---------------------+-------------------+

We will talk about how multiversion is implemented “under the hood” in the following articles, 
and now we will take a closer look at each of the three levels through the eyes of the user (as you understand, the most interesting is hidden behind “other anomalies”). 
To do this, we will create a table of accounts. Alice and Bob have 1,000 rubles each, but Bob has two accounts:

=> CREATE TABLE accounts(
  id integer PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
  number text UNIQUE,
  client text,
  amount numeric
);
=> INSERT INTO accounts VALUES
  (1, '1001', 'alice', 1000.00),
  (2, '2001', 'bob', 100.00),
  (3, '2002', 'bob', 900.00);

-------------------
- Read Committed  -
-------------------

No dirty reading

It is easy to verify that dirty data cannot be read. Let's start a transaction. By default, it will use the Read Committed isolation level:

=> BEGIN;
=> SHOW transaction_isolation;
 transaction_isolation 
-----------------------
 read committed
(1 row)

More precisely, the default level is set by a parameter, it can be changed if necessary:

=> SHOW default_transaction_isolation;
 default_transaction_isolation 
-------------------------------
 read committed
(1 row)

So, in an open transaction, we withdraw funds from the account, but we do not commit the changes. The transaction sees its own changes:

=> UPDATE accounts SET amount = amount - 200 WHERE id = 1;
=> SELECT * FROM accounts WHERE client = 'alice';
 id | number | client | amount 
----+--------+--------+--------
  1 | 1001   | alice  | 800.00
(1 row)

In the second session, let's start another transaction with the same Read Committed level. To distinguish between different transactions, 
the commands for the second transaction will be shown indented and strikethrough.

In order to repeat the above commands (which is useful), you need to open two terminals and run psql in each. In the first one, 
you can enter commands from one transaction, and in the second, commands from another.

|  => BEGIN;
|  => SELECT * FROM accounts WHERE client = 'alice';
|   id | number | client | amount  
|  ----+--------+--------+---------
|    1 | 1001   | alice  | 1000.00
|  (1 row)

As expected, the other transaction does not see uncommitted changes — dirty reads are not allowed.

---------------------------
- Non-repetitive reading  -
---------------------------

Now let the first transaction commit the changes, and the second will re-execute the same query.

=> COMMIT;

|  => SELECT * FROM accounts WHERE client = 'alice';
|   id | number | client | amount 
|  ----+--------+--------+--------
|    1 | 1001   | alice  | 800.00
|  (1 row)
|  => COMMIT;

The request already receives new data - this is the non-repeatable read anomaly , which is allowed at the Read Committed level.
Practical conclusion : in a transaction, 
you cannot make decisions based on the data read by the previous operator - because everything can change during the time between the execution of the operators. 
Here's an example, the variations of which are so common in application code that it is a classic anti-pattern:

IF (SELECT amount FROM accounts WHERE id = 1) >= 1000 THEN
        UPDATE accounts SET amount = amount - 1000 WHERE id = 1;
      END IF;
      
During the time that elapses between the check and the update, other transactions can change the state of the account in any way, 
so that such a "check" does not save you from anything. 
It is convenient to imagine that between operators of one transaction any other operators of other transactions can "wedge in", for example, like this:      

IF (SELECT amount FROM accounts WHERE id = 1) >= 1000 THEN
       -----
      |   UPDATE accounts SET amount = amount - 200 WHERE id = 1;
      |   COMMIT;
       -----
        UPDATE accounts SET amount = amount - 1000 WHERE id = 1;
      END IF;

If, by rearranging the operators, you can spoil everything, then the code is written incorrectly. 
And do not deceive yourself that such a combination of circumstances will not happen - it will definitely happen.

How to write the code correctly? The possibilities usually boil down to the following:

  * Don't write code.
    It is not joke. For example, in this case, the check easily turns into an integrity constraint:
    ALTER TABLE accounts ADD CHECK amount >= 0;
    Now no checks are needed: you just need to perform an action and, if necessary, handle the exception that will be thrown in the event of an attempt to violate integrity.

  * Use one SQL statement.
    Consistency issues arise because in between statements another transaction can complete and the visible data changes. 
    And if there is only one operator, then there are no gaps.
    PostgreSQL has enough tools to solve complex problems with a single SQL statement. Note the general table expressions (CTE), in which, 
    among other things, you can use the INSERT / UPDATE / DELETE statements, as well as the INSERT ON CONFLICT statement, which implements the "insert, 
    and if the row already exists, update" logic in one statement.

  * Custom locks.
    The last resort is to manually set an exclusive lock either on all required rows (SELECT FOR UPDATE), or on the entire table (LOCK TABLE). 
    This always works, but negates the advantages of multi-versioning: instead of being executed simultaneously, some of the operations will be performed sequentially.

---------------------
- Inconsistent read -
---------------------

Before moving on to the next level of isolation, you have to admit that it is not so simple. PostgreSQL's implementation is such that it allows for other, 
lesser known anomalies that are not specified in the standard.

Let's say the first transaction starts transferring funds from one of Bob's accounts to another:

=> BEGIN;
=> UPDATE accounts SET amount = amount - 100 WHERE id = 2;

At this time, another transaction calculates Bob's balance, and the calculation is performed in a loop through all of Bob's accounts. 
In fact, the transaction starts from the first account (and obviously sees the previous state):

|  => BEGIN;
|  => SELECT amount FROM accounts WHERE id = 2;
|   amount 
|  --------
|   100.00
|  (1 row)

At this point, the first transaction completes successfully:

=> UPDATE accounts SET amount = amount + 100 WHERE id = 3;
=> COMMIT;

And the other reads the state of the second account (and already sees the new value):

|  => SELECT amount FROM accounts WHERE id = 3;
|   amount  
|  ---------
|   1000.00
|  (1 row)
|  => COMMIT;

Thus, the second transaction received a total of 1100 ₽, that is, incorrect data. This is the inconsistent read anomaly .

How to avoid such an anomaly while staying at the Read Committed level? Of course, use one operator. For example like this:

      SELECT sum(amount) FROM accounts WHERE client = 'bob';

So far I have argued that data visibility can only change between operators, but is that obvious? And if the request takes a long time, 
can it see some of the data in one state, and some in another?

Let's check. A convenient way to do this is to insert an artificial delay into the statement by calling the pg_sleep function. 
Its parameter sets the delay time in seconds.

=> SELECT amount, pg_sleep(2) FROM accounts WHERE client = 'bob';

While this construction is being executed, in another transaction we transfer funds back:

|  => BEGIN;
|  => UPDATE accounts SET amount = amount + 100 WHERE id = 2;
|  => UPDATE accounts SET amount = amount - 100 WHERE id = 3;
|  => COMMIT;

The result shows that the operator sees the data in the same state in which it was at the time of the start of its execution. This is definitely correct.

 amount  | pg_sleep 
---------+----------
    0.00 | 
 1000.00 | 
(2 rows)

But even here it is not so simple. PostgreSQL allows you to define functions, and functions have the concept of a mutability category. 
If a volatile function is called in a request (with the VOLATILE category), and another request is executed in this function, 
then this request inside the function will see data that is not consistent with the data of the main request.

=> CREATE FUNCTION get_amount(id integer) RETURNS numeric AS $$
  SELECT amount FROM accounts a WHERE a.id = get_amount.id;
$$ VOLATILE LANGUAGE sql;

=> SELECT get_amount(id), pg_sleep(2)
FROM accounts WHERE client = 'bob';

|  => BEGIN;
|  => UPDATE accounts SET amount = amount + 100 WHERE id = 2;
|  => UPDATE accounts SET amount = amount - 100 WHERE id = 3;
|  => COMMIT;

In this case, we will receive incorrect data - 100 ₽ lost:

 get_amount | pg_sleep 
------------+----------
     100.00 | 
     800.00 | 
(2 rows)

Let me emphasize that this effect is possible only at the Read Committed isolation level, and only with the VOLATILE volatility category. 
The trouble is that this is the isolation level and this particular category of variability is used by default, so I must admit that the rake is very good. Don't step on!

---------------------------------------------------
- Inconsistent read in exchange for lost changes  -
---------------------------------------------------

Inconsistent reads within a single statement can — in a somewhat unexpected way — also occur during an update.
Let's see what happens when two transactions try to update the same row. Now Bob has 1,000 rubles on two accounts:

=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount 
----+--------+--------+--------
  2 | 2001   | bob    | 200.00
  3 | 2002   | bob    | 800.00
(2 rows)

Beginning a transaction that reduces Bob's balance:

=> BEGIN;
=> UPDATE accounts SET amount = amount - 100 WHERE id = 3;

At the same time, another transaction charges interest on all customer accounts with a total balance equal to or greater than RUB 1,000:

|  => UPDATE accounts SET amount = amount * 1.01
|  WHERE client IN (
|    SELECT client
|    FROM accounts
|    GROUP BY client
|    HAVING sum(amount) >= 1000
|  );

There are two parts to executing an UPDATE statement. First, a SELECT is actually executed, which selects the rows that match the condition for updating. 
Since the change in the first transaction is not committed, 
the second transaction cannot see it and it does not in any way affect the selection of rows for interest calculation. 
So, Bob's accounts fall under the condition and after the update is completed, his balance should increase by 10 ₽.

The second stage of execution - the selected rows are updated one after another. 
Here the second transaction is forced to "hang" because the id = 3 row is already locked by the first transaction.

Meanwhile, the first transaction commits the changes:

=> COMMIT;

What will be the result?

=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client |  amount  
----+--------+--------+----------
  2 | 2001   | bob    | 202.0000
  3 | 2002   | bob    | 707.0000
(2 rows)

Yes, on the one hand, the UPDATE command should not see the changes in the second transaction. 
But on the other hand, it should not lose the changes committed in the second transaction.

After the lock is released, UPDATE rereads the row it is trying to update (but only one!). As a result, it turns out that Bob was credited with 9 ₽, 
based on the amount of 900 ₽. But if Bob had 900 rubles, his accounts should not have been included in the sample at all.

So, the transaction receives incorrect data: some of the rows are visible at one point in time, some - at another. In return for the lost update, 
we again get an inconsistent read anomaly .

Astute readers will note that with some help from the application at the Read Committed level, it is possible to get a lost update. For example like this:

      x := (SELECT amount FROM accounts WHERE id = 1);
      UPDATE accounts SET amount = x + 100 WHERE id = 1;

The database is not at fault: it receives two SQL statements and does not know anything about the fact that the value x + 100 has something to do with accounts.amount. 
Don't write your code this way.

-------------------
- Repeatable Read -
-------------------

No non-repetitive and phantom readings

The very name of the isolation level indicates that the read is repeatable. 
Let's check this, and at the same time make sure that there are no phantom readings. 
To do this, in the first transaction, return Bob's accounts to their previous state and create a new account for Charlie:

=> BEGIN;
=> UPDATE accounts SET amount = 200.00 WHERE id = 2;
=> UPDATE accounts SET amount = 800.00 WHERE id = 3;
=> INSERT INTO accounts VALUES
  (4, '3001', 'charlie', 100.00);
=> SELECT * FROM accounts ORDER BY id;
 id | number | client  | amount 
----+--------+---------+--------
  1 | 1001   | alice   | 800.00
  2 | 2001   | bob     | 200.00
  3 | 2002   | bob     | 800.00
  4 | 3001   | charlie | 100.00
(4 rows)

In the second session, start a transaction with the Repeatable Read level, specifying it in the BEGIN command (the level of the first transaction is not important).

|  => BEGIN ISOLATION LEVEL REPEATABLE READ;
|  => SELECT * FROM accounts ORDER BY id;
|   id | number | client |  amount  
|  ----+--------+--------+----------
|    1 | 1001   | alice  |   800.00
|    2 | 2001   | bob    | 202.0000
|    3 | 2002   | bob    | 707.0000
|  (3 rows)

Now the first transaction commits the changes, and the second reruns the same request.

=> COMMIT;

|  => SELECT * FROM accounts ORDER BY id;
|   id | number | client |  amount  
|  ----+--------+--------+----------
|    1 | 1001   | alice  |   800.00
|    2 | 2001   | bob    | 202.0000
|    3 | 2002   | bob    | 707.0000
|  (3 rows)
|  => COMMIT;

The second transaction continues to see exactly the same data as at the beginning: neither changes in existing rows nor new rows are visible.
At this level, you don't have to worry about changing something between the two operators.

----------------------------------------------------
- Serialization error in exchange for lost changes -
----------------------------------------------------

Above, we talked about the fact that when two transactions update the same row at the Read Committed level, an inconsistent read anomaly may occur. 
This is because a pending transaction reads a locked row and thus sees it at a different point in time than the rest of the rows.

At the Repeatable Read level, such an anomaly is not allowed, but if it does occur, nothing can be done - so the transaction is aborted with a serialization error. 
Let's check by repeating the same scenario with percentages:

=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount 
----+--------+--------+--------
  2 | 2001   | bob    | 200.00
  3 | 2002   | bob    | 800.00
(2 rows)
=> BEGIN;
=> UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;

|  => BEGIN ISOLATION LEVEL REPEATABLE READ;
|  => UPDATE accounts SET amount = amount * 1.01
|  WHERE client IN (
|    SELECT client
|    FROM accounts
|    GROUP BY client
|    HAVING sum(amount) >= 1000
|  );

=> COMMIT;

|  ERROR:  could not serialize access due to concurrent update
|  => ROLLBACK;

The data remained consistent:

=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount 
----+--------+--------+--------
  2 | 2001   | bob    | 200.00
  3 | 2002   | bob    | 700.00
(2 rows)

The same error will occur in the case of any other concurrent row change, even if the columns of interest to us have not actually changed.

The practical takeaway is that if an application uses the Repeatable Read isolation level for writing transactions, 
it must be prepared to repeat transactions that failed serialization. This is not possible for read-only transactions.

-----------------------
- Inconsistent record -
-----------------------

So, PostgreSQL prevents all anomalies described in the standard at the Repeatable Read isolation level. 
But not all at all. It turns out there are exactly two anomalies that remain possible.
(This is true not only of PostgreSQL, but also of other snapshot-based isolation implementations.)

The first of these anomalies is an inconsistent write .

Let the following rule of consistency apply: negative amounts on the accounts of a client are allowed if the total amount on all accounts of this client remains non-negative .

The first transaction receives the amount on Bob's accounts: 900 ₽.

=> BEGIN ISOLATION LEVEL REPEATABLE READ;
=> SELECT sum(amount) FROM accounts WHERE client = 'bob';
  sum   
--------
 900.00
(1 row)

The second transaction receives the same amount.

|  => BEGIN ISOLATION LEVEL REPEATABLE READ;
|  => SELECT sum(amount) FROM accounts WHERE client = 'bob';
|    sum   
|  --------
|   900.00
|  (1 row)

The first transaction rightly believes that the amount of one of the accounts can be reduced by 600 ₽.

=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 2;

And the second transaction comes to the same conclusion. But reduces another score:

|  => UPDATE accounts SET amount = amount - 600.00 WHERE id = 3;
|  => COMMIT;

=> COMMIT;
=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount  
----+--------+--------+---------
  2 | 2001   | bob    | -400.00
  3 | 2002   | bob    |  100.00
(2 rows)

We managed to take Bob's balance into the negative, although each of the transactions works correctly one by one.

---------------------------------
- Read-only transaction anomaly -
---------------------------------

This is the second and final anomaly possible at the Repeatable Read level. To demonstrate it, three transactions are required, two of which will modify data and the third will only read.

But first, let's restore Bob's account state:

=> UPDATE accounts SET amount = 900.00 WHERE id = 2;
=> SELECT * FROM accounts WHERE client = 'bob';
 id | number | client | amount
----+--------+--------+--------
  3 | 2002   | bob    | 100.00
  2 | 2001   | bob    | 900.00
(2 rows)

The first transaction charges Bob interest on the amount of funds in all accounts. Interest is credited to one of his accounts:

=> BEGIN ISOLATION LEVEL REPEATABLE READ; -- 1
=> UPDATE accounts SET amount = amount + (
  SELECT sum(amount) FROM accounts WHERE client = 'bob'
) * 0.01
WHERE id = 2;

Then another transaction withdraws money from Bob's other account and commits its changes:

|  => BEGIN ISOLATION LEVEL REPEATABLE READ; -- 2
|  => UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;
|  => COMMIT;

If at this moment the first transaction is committed, no anomaly will arise: we could assume that the first transaction was performed first, 
and then the second (but not vice versa, because the first transaction saw the state of the account id = 3 before this account was changed by the second transaction).

But let's imagine that at this moment the third (read-only) transaction begins, which reads the state of some account not affected by the first two transactions:

|  => BEGIN ISOLATION LEVEL REPEATABLE READ; -- 3
|  => SELECT * FROM accounts WHERE client = 'alice';
|   id | number | client | amount 
|  ----+--------+--------+--------
|    1 | 1001   | alice  | 800.00
|  (1 row)

And only after that, the first transaction is completed:

=> COMMIT;

What state should the third transaction see now?

|    SELECT * FROM accounts WHERE client = ‘bob’;

Once started, the third transaction could see changes to the second transaction (which had already been committed), 
but not the first (which had not yet been committed). On the other hand, 
we have already established above that the second transaction should be considered to have started after the first. 
Whatever state the third transaction sees will be inconsistent - this is a read-only transaction anomaly. But at the Repeatable Read level, it is allowed:

|    id | number | client | amount
|   ----+--------+--------+--------
|     2 | 2001   | bob    | 900.00
|     3 | 2002   | bob    |   0.00
|   (2 rows)
|   => COMMIT;

-----------------
- Serializable  -
-----------------

All possible anomalies are prevented at the Serializable level. In fact, Serializable is implemented as an add-on over snapshot-based isolation. 
Anomalies that do not occur with Repeatable Read (such as dirty, non-repeatable, phantom reads) do not occur at the Serializable level either. 
And those anomalies that arise (inconsistent write and anomaly of only a read transaction) 
are detected and the transaction is aborted - the already familiar could not serialize access serialization error occurs.

Inconsistent record

To illustrate, let's repeat the scenario with the inconsistent write anomaly:

=> BEGIN ISOLATION LEVEL SERIALIZABLE;
=> SELECT sum(amount) FROM accounts WHERE client = 'bob';
   sum    
----------
 910.0000
(1 row)

|   => BEGIN ISOLATION LEVEL SERIALIZABLE;
|   => SELECT sum(amount) FROM accounts WHERE client = 'bob';
|      sum    
|   ----------
|    910.0000
|   (1 row)

=> UPDATE accounts SET amount = amount - 600.00 WHERE id = 2;

|   => UPDATE accounts SET amount = amount - 600.00 WHERE id = 3;
|   => COMMIT;

=> COMMIT;
ERROR:  could not serialize access due to read/write dependencies among transactions
DETAIL:  Reason code: Canceled on identification as a pivot, during commit attempt.
HINT:  The transaction might succeed if retried.

As with the Repeatable Read level, an application using the Serializable isolation level must repeat transactions that failed serialization, 
which is also indicated by the hint in the error message.

We gain programming simplicity, but the price for it is the forced termination of a certain proportion of transactions and the need to repeat them. 
The whole question, of course, is how large this share is. If only those transactions were terminated, which really incompatible data intersect with other transactions, 
everything would be fine. But such an implementation would inevitably turn out to be resource-intensive and inefficient, 
since it would have to keep track of operations with each row.

In reality, PostgreSQL's implementation is such that it allows false negatives: some perfectly normal transactions that are simply "out of luck" will also be aborted. 
As we will see later, this depends on many reasons, such as the availability of suitable indexes or the amount of RAM available. In addition, 
there are some other (rather serious) implementation restrictions, for example, queries at the Serializable level will not work on replicas, 
and parallel execution plans will not be used for them. And while work continues to improve the implementation, 
the existing limitations make this isolation level less attractive.
Parallel plans will appear in PostgreSQL 12 ( patch ). And queries on replicas can work in PostgreSQL 13 ( another patch ).

---------------------------------
- Read-only transaction anomaly -
---------------------------------

To prevent a read-only transaction from causing an anomaly and not being harmed by it, 
PostgreSQL offers an interesting mechanism: such a transaction can be blocked until it is safe to execute. 
This is the only case where a SELECT statement might be blocked by row updates. This is how it looks:

=> UPDATE accounts SET amount = 900.00 WHERE id = 2;
=> UPDATE accounts SET amount = 100.00 WHERE id = 3;
=> SELECT * FROM accounts WHERE client = 'bob' ORDER BY id;
 id | number | client | amount 
----+--------+--------+--------
  2 | 2001   | bob    | 900.00
  3 | 2002   | bob    | 100.00
(2 rows)

=> BEGIN ISOLATION LEVEL SERIALIZABLE; -- 1
=> UPDATE accounts SET amount = amount + (
  SELECT sum(amount) FROM accounts WHERE client = 'bob'
) * 0.01
WHERE id = 2;

|  => BEGIN ISOLATION LEVEL SERIALIZABLE; -- 2
|  => UPDATE accounts SET amount = amount - 100.00 WHERE id = 3;
|  => COMMIT;

We explicitly declare the third transaction to be read only (READ ONLY) and deferred (DEFERRABLE):

|   => BEGIN ISOLATION LEVEL SERIALIZABLE READ ONLY DEFERRABLE; -- 3
|   => SELECT * FROM accounts WHERE client = 'alice';

When you try to execute the request, the transaction is blocked, because otherwise its execution will lead to an anomaly.

=> COMMIT;

And only after the first transaction is committed, the third continues execution:

|    id | number | client | amount
|   ----+--------+--------+--------
|     1 | 1001   | alice  | 800.00
|   (1 row)
|   => SELECT * FROM accounts WHERE client = 'bob';
|    id | number | client |  amount  
|   ----+--------+--------+----------
|     2 | 2001   | bob    | 910.0000
|     3 | 2002   | bob    |     0.00
|   (2 rows)
|   => COMMIT;

Another important note: if Serializable isolation is used, then all transactions in the application must use this layer. 
You cannot mix Read Committed (or Repeatable Read) transactions with Serializable. That is, 
you can mix something, but then Serializable will behave like a Repeatable Read without any warnings. 
Why this happens will be discussed later when we talk about implementation.

So if you decide to use Serializble, it is best to globally set the default level (although this, of course, 
does not prevent you from specifying the wrong level explicitly):

      ALTER SYSTEM SET default_transaction_isolation = 'serializable';

You will find a more rigorous presentation of issues related to transactions, 
consistency and anomalies in the book and lecture course by Boris Asenovich Novikov "Fundamentals of Database Technologies".

---------------------------------------
- What isolation level should I use?  -
---------------------------------------

The Read Committed isolation level is used by default in PostgreSQL and, apparently, this level is used in the vast majority of applications. 
It is convenient in that a transaction can be broken on it only in case of a failure, but not to prevent inconsistency. In other words, a serialization error cannot occur.

The flip side of the coin is a large number of possible anomalies, which were discussed in detail above. 
The developer has to constantly keep them in mind and write code so as not to allow them to appear. 
If you cannot formulate the necessary actions in a single SQL statement, you have to resort to explicitly setting locks. 
The most annoying thing is that the code is difficult to test for errors associated with receiving inconsistent data, 
and the errors themselves can occur in an unpredictable and non-reproducible way and therefore are difficult to fix.

The Repeatable Read isolation level removes some inconsistency problems, but, alas, not all. Therefore, you have to not only remember about the remaining anomalies,
but also modify the application so that it correctly handles serialization errors. This is, of course, inconvenient. But for read-only transactions, 
this level perfectly complements Read Committed and is very convenient, for example, for building reports that use several SQL queries.

Finally, the Serializable layer allows you not to worry about inconsistencies at all, which greatly simplifies writing code. 
The only thing that is required of the application is to be able to repeat any transaction when it receives a serialization error. 
But the proportion of interrupted transactions, additional overhead, and the inability to parallelize queries can significantly reduce system throughput. 
It should also be borne in mind that the Serializable level is not applicable on replicas, and that it cannot be mixed with other isolation levels.

-------------
- Relations -
-------------

If you look inside tables and indexes, you will find that they are arranged in a similar way. Both are base objects that contain some data consisting of strings.

There is no doubt that the table consists of rows; this is less obvious for the index. However, 
imagine a B-tree: it consists of nodes that contain indexed values and references to other nodes or table rows. These nodes can be considered index lines - in fact, they are.

In fact, there are still a number of objects that are arranged in a similar way: sequences (in fact, single-row tables), 
materialized views (in fact, tables that remember a query). And then there are ordinary views, which by themselves do not store data, 
but in all other respects are similar to tables.

All these objects in PostgreSQL are called the general word relation (in English relation ). The word is extremely unfortunate, because it is a term from relational theory. 
You can draw a parallel between a relation and a table (view), but certainly not between a relation and an index. 
But it just so happened: PostgreSQL's academic roots are making themselves felt. I think that at first it was tables and views that were called that, 
and the rest has grown over time.

Further, for simplicity, we will only talk about tables and indexes, but the rest of the relationships are arranged in the same way.

Layers (forks) and files
Usually several layers (forks) correspond to each relation. 
There are several types of layers, and each of them contains a certain kind of data.

If there is a layer, then it is initially represented by a single file . The file name consists of a numeric identifier, 
to which an ending can be appended to match the layer name.

The file gradually grows and when its size reaches 1 GB, the next file of the same layer is created (such files are sometimes called segments ). 
The segment sequence number is appended to the end of the file name.

The 1 GB file size limitation has arisen historically to support various file systems, some of which cannot handle large files. 
The limitation can be changed when building PostgreSQL ( ./configure --with-segsize).

Thus, several files can correspond to one relation on the disk. For example, for a small table there will be 3 of them.
All object files belonging to one table space and one database will be placed in one directory. This must be considered because filesystems usually 
do not work well with a large number of files in a directory.
Let's notice right away that the files, in turn, are divided into pages (or blocks ), usually 8 KB each. Let's talk about the internal structure of pages below.
https://mega.nz/file/xwcQzTpZ#oTJ8XGpycrLbX8OL8WfczAjgapfb9LXmhFqa4TNOQyA

Now let's look at the types of layers.
The main layer is the data itself: those same table or index rows. The base layer exists for any relationship (except for views that do not contain data).
The file names of the main layer consist only of a numeric identifier. For example, here is the path to the file of the table that we created last time:

=> SELECT pg_relation_filepath('accounts');
 pg_relation_filepath 
----------------------
 base/41493/41496
(1 row)

Where do these identifiers come from? The base directory corresponds to the pg_default tablespace, the next subdirectory corresponds to the database, 
and the file we are interested in is already in it:

=> SELECT oid FROM pg_database WHERE datname = 'test';
  oid  
-------
 41493
(1 row)

=> SELECT relfilenode FROM pg_class WHERE relname = 'accounts';
 relfilenode 
-------------
       41496
(1 row)

The path is relative, it is counted from the data directory (PGDATA). Moreover, almost all paths in PostgreSQL are PGDATA-based. 
Thanks to this, you can painlessly transfer PGDATA to another place - nothing holds it (unless you may need to configure the path to the libraries in LD_LIBRARY_PATH).

Then we look in the file system:

postgres$ ls -l --time-style=+ /var/lib/postgresql/11/main/base/41493/41496
-rw------- 1 postgres postgres 8192  /var/lib/postgresql/11/main/base/41493/41496

An initialization layer exists only for unlogged tables (created with UNLOGGED) and their indexes. Such objects are no different from ordinary objects, 
except that actions with them are not recorded in the write-ahead log. Due to this, work with them is faster, but in the event of a failure, 
it is impossible to restore the data to a consistent state. Therefore, when restoring, 
PostgreSQL simply deletes all layers of such objects and writes the initialization layer in place of the main layer. The result is a "dummy". 
We will talk about logging in detail, but in a different cycle.

The accounts table is journaled, so there is no initialization layer for it. But for the experiment, you can turn off logging:

=> ALTER TABLE accounts SET UNLOGGED;
=> SELECT pg_relation_filepath('accounts');
 pg_relation_filepath 
----------------------
 base/41493/41507
(1 row)

The ability to turn on and off logging on the fly, as you can see from the example, is associated with overwriting data into files with different names.

The initialization layer has the same name as the main layer, but with the "_init" suffix:

postgres$ ls -l --time-style=+ /var/lib/postgresql/11/main/base/41493/41507_init
-rw------- 1 postgres postgres 0  /var/lib/postgresql/11/main/base/41493/41507_init

Map free space (free space map) - layer in which noted the presence of empty space within the pages. 
This place is constantly changing: when new versions of lines are added, it decreases, when cleaned, it increases. 
The free space map is used when inserting new versions of rows to quickly find a suitable page to accommodate the added data.

Free space map is suffixed with "_fsm". But the file does not appear immediately, but only when necessary. 
The easiest way to achieve this is to clean up the table (we'll talk about why in due time):

=> VACUUM accounts;

postgres$ ls -l --time-style=+ /var/lib/postgresql/11/main/base/41493/41507_fsm
-rw------- 1 postgres postgres 24576  /var/lib/postgresql/11/main/base/41493/41507_fsm

A visibility map is a layer in which pages are marked with one bit, which contain only the current version of the lines. 
Roughly speaking, this means that when a transaction tries to read a row from such a page, the row can be displayed without checking its visibility. 
We will look in detail at how this happens in future articles.

postgres$ ls -l --time-style=+ /var/lib/postgresql/11/main/base/41493/41507_vm
-rw------- 1 postgres postgres 8192  /var/lib/postgresql/11/main/base/41493/41507_vm

---------
- Pages -
---------

As we said, the files are logically divided into pages.

Typically the page is 8KB in size. The size can be changed within some limits (16 KB or 32 KB), but only during assembly ( ./configure --with-blocksize). 
The assembled and launched instance can only work with pages of one size.

Regardless of which layer the files belong to, they are used by the server in about the same way. Pages are first read into the buffer cache, 
where they can be read and modified by processes; then the pages are preempted back to disk as needed.

Each page has internal markup and generally contains the following sections:

       0 + ----------------------------------- +
          | heading |
      24 + ----------------------------------- +
          | array of pointers to string versions |
   lower + ----------------------------------- +
          | free space |
   upper + ----------------------------------- +
          | row versions |
 special + ----------------------------------- +
          | special area |
pagesize + ----------------------------------- +

The size of these sections is easy to find out with the pageinspect "exploratory" extension:

=> CREATE EXTENSION pageinspect;
=> SELECT lower, upper, special, pagesize FROM page_header(get_raw_page('accounts',0));
 lower | upper | special | pagesize 
-------+-------+---------+----------
    40 |  8016 |    8192 |     8192
(1 row)

Here we are looking at the title of the very first (zero) page of the table. In addition to the size of the other areas, 
the header contains other information about the page, but we are not interested in it yet.

There is a special area at the bottom of the page , in our case it is empty. It is used only for indexes, and even then not for all. 
"Below" here corresponds to the picture; perhaps it would be more correct to say "in senior addresses".

Following the special area are the row versions - the very data that we store in the table, plus some service information.
At the top of the page, right after the heading, is the table of contents: an array of pointers to the line versions available on the page.
Free space can remain between line versions and pointers (which is marked in the free space map). Note that there is no fragmentation inside the page, 
all free space is always represented by one fragment.

------------
- Pointers -
------------

Why are pointers to row versions needed? The point is that index rows must somehow refer to the row versions in the table. 
It is clear that the link must contain the file number, the page number in the file, and some indication of the line version. 
An offset relative to the beginning of the page could be used as such an indication, but this is inconvenient. 
We would not be able to move the version of the line within the page because that would break existing links. 
This would lead to fragmentation of space within pages and other unpleasant consequences. 
Therefore, the index refers to the pointer number, and the pointer refers to the current position of the row version in the page. Indirect addressing turns out.

Each pointer takes exactly 4 bytes and contains:

link to the line version;
the length of this version of the string;
several bits defining the status of the row version.

---------------
- Data format -
---------------

The format of the data on the disk completely coincides with the representation of data in the RAM. 
The page is read into the buffer cache "as is", without any conversion. Therefore, data files from one platform are not compatible with other platforms.
For example, the x86 architecture uses little-endian byte ordering, z / Architecture uses big-endian ordering, and ARM uses switchable ordering.
Many architectures provide for data alignment on word boundaries. For example, on a 32-bit x86 system, integers (integer, occupies 4 bytes) 
will be aligned on 4-byte word boundaries, just like double precision floating point numbers (double precision, 8 bytes). On a 64-bit system, 
double values will be aligned on the boundary of 8-byte words. This is another reason for incompatibility.
Due to alignment, the size of the table row depends on the order of the fields. Usually this effect is not very noticeable, 
but in some cases it can lead to a significant increase in size. For example, if you mix char (1) and integer fields, there will usually be 3 bytes wasted between them. 
More details can be found in the presentation of Nikolai Shaplov " What's inside him".

--------------------------
- Row Versions and TOAST -
--------------------------

We will talk in detail about how the string versions work from the inside next time. For now, 
the only important thing for us is that each version should fit entirely on one page: PostgreSQL does not provide a way to "continue" a line on the next page. 
Instead, a technology called TOAST (The Oversized Attributes Storage Technique) is used. The name itself suggests that the string can be sliced into toast.
Seriously speaking, TOAST involves several strategies. "Long" attribute values can be sent to a separate service table, having previously cut them into small toast fragments. 
Another option is to shrink the value so that the row version still fits on a regular table page. And you can do both: first squeeze, and only then cut and send.
For each main table, if necessary, a separate, but one for all attributes, TOAST table (and a special index to it) is created. 
The need is determined by the presence of potentially long attributes in the table. For example, if the table has a column of type numeric or text, 
the TOAST table will be created immediately, even if long values are not used.

Since a TOAST table is essentially a regular table, it still has the same set of layers. And this doubles the number of files that "serve" the table.
Strategies are initially determined by the data types of the columns. You can view them with a command \d+in psql, but since it displays 
a lot of other information at the same time, we will use a query to the system catalog:

=> SELECT attname, atttypid::regtype, CASE attstorage
  WHEN 'p' THEN 'plain'
  WHEN 'e' THEN 'external'
  WHEN 'm' THEN 'main'
  WHEN 'x' THEN 'extended'
END AS storage
FROM pg_attribute
WHERE attrelid = 'accounts'::regclass AND attnum > 0;
 attname | atttypid | storage  
---------+----------+----------
 id      | integer  | plain
 number  | text     | extended
 client  | text     | extended
 amount  | numeric  | main
(4 rows)

Strategy names have the following meanings:

plain - TOAST is not used (it is used for obviously “short” data types, like integer);
extended - both compression and storage in a separate TOAST table are allowed;
external - long values are stored uncompressed in the TOAST table;
main - long values are first compressed, and only get into the TOAST table if compression did not help.

In general terms, the algorithm looks like this. PostgreSQL strives to have at least 4 lines per page. Therefore, if the line size exceeds a fourth of the page, 
taking into account the header (for a normal 8K page, this is 2040 bytes), you must apply TOAST to some of the values. 
We proceed in the order described below and stop as soon as the line stops exceeding the threshold:

First, we iterate over the attributes with the external and extended strategies, moving from the longest to the shortest. 
Extended attributes are compressed (if it has an effect) and if the value itself exceeds a quarter of a page, it is immediately sent to the TOAST table. 
External attributes are handled the same way, but not compressed.
If after the first pass the row version still does not fit, we send the remaining attributes with the external and extended strategies to the TOAST table.
If this does not help, we try to compress the attributes with the main strategy, while leaving them in the table page.
And only if after that the string is still not short enough, the main attributes are sent to the TOAST table.

Sometimes it can be helpful to change the strategy for some columns. For example, if you know in advance that the data in a column is not compressed, 
you can set the external strategy for it - this will save you on unnecessary compression attempts. This is done as follows:

=> ALTER TABLE accounts ALTER COLUMN number SET STORAGE external;

Repeating the request, we get:

 attname | atttypid | storage  
---------+----------+----------
 id      | integer  | plain
 number  | text     | external
 client  | text     | extended
 amount  | numeric  | main

TOAST tables and indexes are located in a separate pg_toast schema and are therefore usually not visible. For temporary tables used pg_toast_temp_ 
circuit N is similar to a conventional pg_temp_ N .
Of course, if you wish, no one bothers to spy on the internal mechanics of the process. Let's say there are three potentially long attributes in the accounts table, 
so there must be a TOAST table. Here she is:

=> SELECT relnamespace::regnamespace, relname
FROM pg_class WHERE oid = (
  SELECT reltoastrelid FROM pg_class WHERE relname = 'accounts'
);
 relnamespace |    relname     
--------------+----------------
 pg_toast     | pg_toast_33953
(1 row)

=> \d+ pg_toast.pg_toast_33953
TOAST table "pg_toast.pg_toast_33953"
   Column   |  Type   | Storage 
------------+---------+---------
 chunk_id   | oid     | plain
 chunk_seq  | integer | plain
 chunk_data | bytea   | plain

It is logical that for "toasts", into which a string is cut, the plain strategy is used: there is no second level TOAST.
PostgreSQL hides the index more carefully, but it is not difficult to find it either:

=> SELECT indexrelid::regclass FROM pg_index
WHERE indrelid = (
  SELECT oid FROM pg_class WHERE relname = 'pg_toast_33953'
);
          indexrelid           
-------------------------------
 pg_toast.pg_toast_33953_index
(1 row)

=> \d pg_toast.pg_toast_33953_index
Unlogged index "pg_toast.pg_toast_33953_index"
  Column   |  Type   | Key? | Definition 
-----------+---------+------+------------
 chunk_id  | oid     | yes  | chunk_id
 chunk_seq | integer | yes  | chunk_seq
primary key, btree, for table "pg_toast.pg_toast_33953"

The client column uses the extended strategy: the values ​​in it will be compressed. Let's check:

=> UPDATE accounts SET client = repeat('A',3000) WHERE id = 1;
=> SELECT * FROM pg_toast.pg_toast_33953;
 chunk_id | chunk_seq | chunk_data 
----------+-----------+------------
(0 rows)

There is nothing in the TOAST table: duplicate characters are perfectly compressed and after that the value fits into a regular table page.
Now let's say the customer's name consists of random characters:

=> UPDATE accounts SET client = (
  SELECT string_agg( chr(trunc(65+random()*26)::integer), '') FROM generate_series(1,3000)
)
WHERE id = 1
RETURNING left(client,10) || '...' || right(client,10);
        ?column?         
-------------------------
 TCKGKZZSLI...RHQIOLWRRX
(1 row)

This sequence cannot be compressed, and it ends up in the TOAST table:

=> SELECT chunk_id,
  chunk_seq,
  length(chunk_data),
  left(encode(chunk_data,'escape')::text, 10) ||
  '...' ||
  right(encode(chunk_data,'escape')::text, 10) 
FROM pg_toast.pg_toast_33953;
 chunk_id | chunk_seq | length |        ?column?         
----------+-----------+--------+-------------------------
    34000 |         0 |   2000 | TCKGKZZSLI...ZIPFLOXDIW
    34000 |         1 |   1000 | DDXNNBQQYH...RHQIOLWRRX
(2 rows)

As you can see, the data is cut into fragments of 2000 bytes.
When accessing a "long" value, PostgreSQL automatically, transparently to the application, restores the original value and returns it to the client.
Of course, both sliced compression and subsequent recovery are a lot of resources. Therefore, storing bulk data in PostgreSQL is not a good idea, 
especially if it is actively used and does not require transactional logic (for example: scanned originals of accounting documents). 
A more profitable alternative may be storing such data on the file system, and in the DBMS - the names of the corresponding files.
The TOAST table is used only when accessing a "long" value. 
In addition, the toast table maintains its own versioning: if the data update does not affect the "long" value, 
the new version of the row will refer to the same value in the TOAST table - this saves space.
Note that TOAST only works for tables, not indexes. This imposes a limit on the size of the indexed keys.

-----------
- Heading -
-----------

As we said, each row can be present in the database in several versions at the same time. One version from another must be somehow distinguished. 
For this purpose, each version has two marks that determine the "time" of the given version (xmin and xmax). 
In quotes - because not time is used as such, but a special incremental counter. And this counter is the transaction number.

(As usual, in fact, everything is more complicated: the number of transactions cannot increase all the time due to the limited capacity of the counter. 
But we will consider these details in detail when we get to freezing.)

When the row is created, xmin is set to the transaction number that issued the INSERT command, and xmax is not populated.

When a row is deleted, the xmax value of the current version is marked with the transaction number that performed the DELETE.

When a row is modified with an UPDATE command, two operations are actually performed: DELETE and INSERT. 
The current version of the line sets xmax to the number of the transaction that performed the UPDATE. Then a new version of the same string is created; 
its xmin value is the same as the xmax value of the previous version.

The xmin and xmax fields are included in the row version header. In addition to these fields, the header contains others, for example:

infomask is a set of bits that define the properties of this version. There are quite a few of them; we will gradually consider the main ones.
ctid is a link to the next, newer, version of the same line. For the most recent, current version of the line, the ctid refers to that version itself. 
The number has the form (x, y), where x is the page number, y is the ordinal number of the pointer in the array.
Nulls bitmap - marks those columns of this version that contain NULL. NULL is not one of the usual values of data types, so the attribute has to be stored separately.

As a result, the header is quite large - at least 23 bytes per row version, and usually more due to the NULL bitmap. 
If the table is "narrow" (that is, contains few columns), the overhead can take up more than useful information.

----------
- Insert -
----------

Let's take a closer look at how low-level row operations are performed and start with insertion.
To experiment, let's create a new table with two columns and an index on one of them:

=> CREATE TABLE t(
  id serial,
  s text
);
=> CREATE INDEX ON t(s);

Let's insert one row after starting the transaction.

=> BEGIN;
=> INSERT INTO t(s) VALUES ('FOO');

Here is our current transaction number:

=> SELECT txid_current();
 txid_current 
--------------
         3664
(1 row)

Let's take a look at the page content. The heap_page_items function of the pageinspect extension allows you to get information about pointers and row versions:

=> SELECT * FROM heap_page_items(get_raw_page('t',0)) \gx
-[ RECORD 1 ]-------------------
lp          | 1
lp_off      | 8160
lp_flags    | 1
lp_len      | 32
t_xmin      | 3664
t_xmax      | 0
t_field3    | 0
t_ctid      | (0,1)
t_infomask2 | 2
t_infomask  | 2050
t_hoff      | 24
t_bits      | 
t_oid       | 
t_data      | \x0100000009464f4f

Note that heap in PostgreSQL refers to tables. This is another strange use of the term - the heap is a well - known data structure that has nothing to do with a table. 
Here the word is used in the sense of "heaped everything", in contrast to ordered indices.
The function shows the data “as is”, in a format that is difficult to understand. To understand, we will leave only part of the information and decipher it:

=> SELECT '(0,'||lp||')' AS ctid,
       CASE lp_flags
         WHEN 0 THEN 'unused'
         WHEN 1 THEN 'normal'
         WHEN 2 THEN 'redirect to '||lp_off
         WHEN 3 THEN 'dead'
       END AS state,
       t_xmin as xmin,
       t_xmax as xmax,
       (t_infomask & 256) > 0  AS xmin_commited,
       (t_infomask & 512) > 0  AS xmin_aborted,
       (t_infomask & 1024) > 0 AS xmax_commited,
       (t_infomask & 2048) > 0 AS xmax_aborted,
       t_ctid
FROM heap_page_items(get_raw_page('t',0)) \gx
-[ RECORD 1 ]-+-------
ctid          | (0,1)
state         | normal
xmin          | 3664
xmax          | 0
xmin_commited | f
xmin_aborted  | f
xmax_commited | f
xmax_aborted  | t
t_ctid        | (0,1)

Here's what we did:

 * Added a zero to the pointer number to make it look the same as t_ctid: (page number, pointer number).
 * Decrypted the state of the lp_flags pointer. Here it is “normal”, which means that the pointer is actually referring to the line version. 
   We will consider other values later.
 * Of all the information bits, only two pairs have been allocated so far. 
   The xmin_committed and xmin_aborted bits indicate whether transaction number xmin is committed (canceled). 
   Two similar bits refer to transaction numbered xmax.

What do we see? When you insert a row, an index number 1 appears in the table page, referencing the first and only version of the row.
In the row version, the xmin field is filled with the current transaction number. The transaction is still active, so both xmin_committed and xmin_aborted are not set.
The row version ctid field refers to the same row. This means that there is no newer version.
The xmax field is filled with a dummy number 0, since this version of the line has not been deleted and is current. 
Transactions will ignore this number because the xmax_aborted bit is set.
Let's take another step towards improving readability by adding information bits to the transaction numbers. And let's create a function, 
since we will need the request more than once:

=> CREATE FUNCTION heap_page(relname text, pageno integer)
RETURNS TABLE(ctid tid, state text, xmin text, xmax text, t_ctid tid)
AS $$
SELECT (pageno,lp)::text::tid AS ctid,
       CASE lp_flags
         WHEN 0 THEN 'unused'
         WHEN 1 THEN 'normal'
         WHEN 2 THEN 'redirect to '||lp_off
         WHEN 3 THEN 'dead'
       END AS state,
       t_xmin || CASE
         WHEN (t_infomask & 256) > 0 THEN ' (c)'
         WHEN (t_infomask & 512) > 0 THEN ' (a)'
         ELSE ''
       END AS xmin,
       t_xmax || CASE
         WHEN (t_infomask & 1024) > 0 THEN ' (c)'
         WHEN (t_infomask & 2048) > 0 THEN ' (a)'
         ELSE ''
       END AS xmax,
       t_ctid
FROM heap_page_items(get_raw_page(relname,pageno))
ORDER BY lp;
$$ LANGUAGE SQL;

In this form, it is much clearer what is going on in the header of the line version:

=> SELECT * FROM heap_page('t',0);
 ctid  | state  | xmin | xmax  | t_ctid 
-------+--------+------+-------+--------
 (0,1) | normal | 3664 | 0 (a) | (0,1)
(1 row)

Similar, but much less detailed, information can be obtained from the table itself using the pseudo-columns xmin and xmax:

=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3664 |    0 |  1 | FOO
(1 row)

------------
- Fixation -
------------

Upon successful completion of the transaction, you need to remember its status - note that it is committed. 
For this, a structure called XACT is used (and before version 10 it was called CLOG (commit log) and this name can still be found in different places).

XACT is not a system catalog table; these are the files in the PGDATA / pg_xact directory. 
They have two bits allocated for each transaction: committed and aborted - just like in the row version header. 
This information is split into several files solely for convenience, we will come back to this issue when we consider freezing. 
And work with these files is carried out page by page, as with all others.

So, when a transaction is committed, the committed bit is set in XACT for this transaction. 
And that's all that happens on commit (although we're not talking about the write-ahead log yet).

When any other transaction accesses the tabular page we just looked at, it will have to answer a few questions.

Has transaction xmin been completed? If not, then the generated row version should not be visible.
This check is done by looking at another structure that resides in the shared memory of the instance and is called ProcArray. 
It contains a list of all active processes, and for each the number of its current (active) transaction is indicated.
If it ended, how - by committing or canceling? If canceled, then the line version should not be visible either.
That's what XACT is for. But, although the last XACT pages are stored in buffers in RAM, it is still expensive to check XACT every time. 
Therefore, once the status of a transaction is clarified, it is written into the xmin_committed and xmin_aborted bits of the row version. 
If one of these bits is set, then the state of the transaction xmin is considered known and the next transaction will no longer need to contact XACT.

Why aren't these bits set by the insert transaction itself? When an insert occurs, the transaction does not yet know if it will succeed. 
And at the moment of committing, it is no longer clear which lines in which pages were changed. There may be many such pages, 
and it is not profitable to memorize them. In addition, some pages can be flushed out of the buffer cache to disk; 
reading them again to change the bits would slow down the commit significantly.

The downside to saving is that after changes, any transaction (even performing a simple read - SELECT) can start changing the data pages in the buffer cache.

So let's commit the change.

=> COMMIT;

Nothing has changed on the page (but we know that the status of the transaction has already been recorded in XACT):

=> SELECT * FROM heap_page('t',0);
 ctid  | state  | xmin | xmax  | t_ctid 
-------+--------+------+-------+--------
 (0,1) | normal | 3664 | 0 (a) | (0,1)
(1 row)

Now the transaction that first accesses the page will have to determine the status of the transaction xmin and write it to the information bits:

=> SELECT * FROM t;
 id |  s  
----+-----
  1 | FOO
(1 row)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3664 (c) | 0 (a) | (0,1)
(1 row)

------------
- Deleting -
------------

When a row is deleted, the number of the current deleting transaction is written in the xmax field of the current version, and the xmax_aborted bit is cleared.

Note that the set xmax value corresponding to an active transaction acts as a row lock. If another transaction is about to update or delete this row, 
it will have to wait for the xmax transaction to complete. We will talk more about blocking later. For now, let's just note that the number of row locks is unlimited. 
They do not take up space in RAM and system performance does not suffer from their number. True, “long” transactions have other disadvantages, but more on that later.

Let's delete the line.

=> BEGIN;
=> DELETE FROM t;
=> SELECT txid_current();
 txid_current 
--------------
         3665
(1 row)

We see that the transaction number was written in the xmax field, but the information bits are not set:

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax | t_ctid 
-------+--------+----------+------+--------
 (0,1) | normal | 3664 (c) | 3665 | (0,1)
(1 row)

----------------
- Cancellation -
----------------

Undoing works in the same way as committing, except that the aborted bit is set for the transaction in XACT. The undo is as fast as the commit. 
Although the command is called ROLLBACK, changes are not rolled back: everything that the transaction has changed in the data pages remains unchanged.

=> ROLLBACK;
=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax | t_ctid 
-------+--------+----------+------+--------
 (0,1) | normal | 3664 (c) | 3665 | (0,1)
(1 row)

When the page is accessed, the status will be checked and the hint bit xmax_aborted will be set in the string version. 
The number xmax itself remains in the page, but no one will look at it anymore.

=> SELECT * FROM t;
 id |  s  
----+-----
  1 | FOO
(1 row)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   |   xmax   | t_ctid 
-------+--------+----------+----------+--------
 (0,1) | normal | 3664 (c) | 3665 (a) | (0,1)
(1 row)

----------
- Update -
----------

Updating works as if first deleting the current version of the row and then inserting a new one.

=> BEGIN;
=> UPDATE t SET s = 'BAR';
=> SELECT txid_current();
 txid_current 
--------------
         3666
(1 row)

The query throws out one line (new version):

=> SELECT * FROM t;
 id |  s  
----+-----
  1 | BAR
(1 row)

But in the page we see both versions:

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3664 (c) | 3666  | (0,2)
 (0,2) | normal | 3666     | 0 (a) | (0,2)
(2 rows)

The deleted version is marked with the current transaction number in the xmax field. Moreover, this value is written over the old one, 
since the previous transaction was canceled. And the xmax_aborted bit is cleared because the status of the current transaction is still unknown.

The first version of the line now refers to the second (t_ctid field) as newer.
A second index appears in the index page, and a second row references the second version in the table page.
As with the deletion, the xmax value in the first version of the row indicates that the row is locked.
Well, let's complete the transaction.

=> COMMIT;

-----------
- Indexes -
-----------

So far, we've only talked about table pages. What happens inside the indexes?

The information in index pages is highly dependent on the specific type of index. And even one type of index has different types of pages. 
For example, a B-tree has a metadata page and "regular" pages.

However, usually a page has an array of pointers to strings and the strings themselves (just like a table page). In addition, 
there is space at the end of the page for special data.

Rows in indexes can also have very different structures depending on the type of index. For example, for a B-tree, 
the leaf page rows contain the index key value and a reference (ctid) to the corresponding table row. In general, the index can be arranged in a completely different way.
The most important point is that there are no row versions in any type of index. Or, 
you can assume that each line is represented by exactly one version. In other words, there are no xmin and xmax fields in the index line header. 
You can assume that links from the index lead to all table row versions - so you can figure out which version a transaction will see only by looking at the table. 
(As usual, this is not the whole truth. In some cases, the visibility map allows you to optimize the process, but we will consider this in more detail later.)
At the same time, in the index page, we find pointers to both versions, both the current and the old:

=> SELECT itemoffset, ctid FROM bt_page_items('t_s_idx',1);
 itemoffset | ctid  
------------+-------
          1 | (0,2)
          2 | (0,1)
(2 rows)

------------------------
- Virtual transactions -
------------------------

In practice, PostgreSQL uses optimizations to "save" transaction numbers.

If a transaction only reads data, then it has no effect on the visibility of row versions. 
Therefore, at the beginning, the service process issues a virtual xid to the transaction. The number consists of a process ID and a sequential number.

The issue of this number does not require synchronization between all processes and is therefore very fast. 
We'll get to know another reason for using virtual numbers when we talk about freezing.

Virtual numbers are not counted in any way in data snapshots.

At different points in time, the system may well contain virtual transactions with numbers that have already been used, and this is normal. 
But such a number cannot be written into data pages, because the next time the page is accessed, it can lose all meaning.

=> BEGIN;
=> SELECT txid_current_if_assigned();
 txid_current_if_assigned 
--------------------------
                         
(1 row)

If a transaction starts changing data, it is given a real, unique transaction number.

=> UPDATE accounts SET amount = amount - 1.00;
=> SELECT txid_current_if_assigned();
 txid_current_if_assigned 
--------------------------
                     3667
(1 row)

=> COMMIT;

-----------------------
- Nested transactions -
-----------------------

---------------
- Save points -
---------------

In SQL are defined in terms of conservation (savepoint), that allow you to cancel the operation of the transaction, without interrupting it completely. 
But this does not fit into the above scheme, since the status of a transaction is one for all its changes, and physically no data is rolled back.

To implement such functionality, a transaction with the point of saving is divided into several separate nested transactions (subtransaction), 
the status of which can be controlled separately.

Nested transactions have their own number (greater than the number of the main transaction). The status of nested transactions is recorded in the usual way in XACT, 
but the final status depends on the status of the main transaction: if it is canceled, then all nested transactions are canceled as well.

Transaction nesting information is stored in files in the PGDATA / pg_subtrans directory. The files are accessed through buffers in the instance's shared memory, 
organized in the same way as XACT buffers.

Don't confuse nested transactions and autonomous transactions. Autonomous transactions are independent of each other, while nested transactions are. 
There are no autonomous transactions in regular PostgreSQL, and perhaps for the best: they are very, 
very rarely needed for business, and their presence in other DBMSs provokes abuse, from which everyone then suffers.

Let's clear the table, start a transaction and insert a row:

=> TRUNCATE TABLE t;
=> BEGIN;
=> INSERT INTO t(s) VALUES ('FOO');
=> SELECT txid_current();
 txid_current 
--------------
         3669
(1 row)

=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3669 |    0 |  2 | FOO
(1 row)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  | xmin | xmax  | t_ctid 
-------+--------+------+-------+--------
 (0,1) | normal | 3669 | 0 (a) | (0,1)
(1 row)

Now let's put a savepoint and insert another line.

=> SAVEPOINT sp;
=> INSERT INTO t(s) VALUES ('XYZ');
=> SELECT txid_current();
 txid_current 
--------------
         3669
(1 row)

Note that the txid_current () function returns the number of the main, not the nested, transaction.

=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3669 |    0 |  2 | FOO
 3670 |    0 |  3 | XYZ
(2 rows)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  | xmin | xmax  | t_ctid 
-------+--------+------+-------+--------
 (0,1) | normal | 3669 | 0 (a) | (0,1)
 (0,2) | normal | 3670 | 0 (a) | (0,2)
(2 rows)

Let's roll back to the savepoint and insert the third line.

=> ROLLBACK TO sp;
=> INSERT INTO t(s) VALUES ('BAR');
=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3669 |    0 |  2 | FOO
 3671 |    0 |  4 | BAR
(2 rows)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3669     | 0 (a) | (0,1)
 (0,2) | normal | 3670 (a) | 0 (a) | (0,2)
 (0,3) | normal | 3671     | 0 (a) | (0,3)
(3 rows)

In the page, we continue to see the row added by the canceled nested transaction.

We commit the changes.

=> COMMIT;
=> SELECT xmin, xmax, * FROM t;
 xmin | xmax | id |  s  
------+------+----+-----
 3669 |    0 |  2 | FOO
 3671 |    0 |  4 | BAR
(2 rows)

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3669 (c) | 0 (a) | (0,1)
 (0,2) | normal | 3670 (a) | 0 (a) | (0,2)
 (0,3) | normal | 3671 (c) | 0 (a) | (0,3)
(3 rows)

Now you can clearly see that each nested transaction has its own status.

Note that nested transactions cannot be used explicitly in SQL, that is, you cannot start a new transaction without completing the current one. 
This mechanism is used implicitly when using savepoints, as well as when handling PL / pgSQL exceptions and in a number of other, more exotic cases.

=> BEGIN;
BEGIN
=> BEGIN;
WARNING:  there is already a transaction in progress
BEGIN
=> COMMIT;
COMMIT
=> COMMIT;
WARNING:  there is no transaction in progress
COMMIT

--------------------------------------
- Errors and atomicity of operations -
--------------------------------------

What happens if an error occurs during the operation? For example, like this:

=> BEGIN;
=> SELECT * FROM t;
 id |  s  
----+-----
  2 | FOO
  4 | BAR
(2 rows)

=> UPDATE t SET s = repeat('X', 1/(id-4));
ERROR:  division by zero

An error has occurred. Now the transaction is considered interrupted and no operation is allowed in it:

=> SELECT * FROM t;
ERROR:  current transaction is aborted, commands ignored until end of transaction block

And even if you try to commit the changes, PostgreSQL will report the cancellation:

=> COMMIT;
ROLLBACK

Why can't the transaction continue after a crash? 
The fact is that an error could arise in such a way that we would get access to some of the changes - the atomicity of not even a transaction, 
but an operator would be violated. As in our example, where the operator managed to update one line before the error:

=> SELECT * FROM heap_page('t',0);
 ctid  | state  |   xmin   | xmax  | t_ctid 
-------+--------+----------+-------+--------
 (0,1) | normal | 3669 (c) | 3672  | (0,4)
 (0,2) | normal | 3670 (a) | 0 (a) | (0,2)
 (0,3) | normal | 3671 (c) | 0 (a) | (0,3)
 (0,4) | normal | 3672     | 0 (a) | (0,4)
(4 rows)

I must say that psql has a mode that nevertheless allows the transaction to continue working after a failure, 
as if the actions of the erroneous statement are being rolled back.

=> \set ON_ERROR_ROLLBACK on
=> BEGIN;
=> SELECT * FROM t;
 id |  s  
----+-----
  2 | FOO
  4 | BAR
(2 rows)

=> UPDATE t SET s = repeat('X', 1/(id-4));
ERROR:  division by zero

=> SELECT * FROM t;
 id |  s  
----+-----
  2 | FOO
  4 | BAR
(2 rows)

=> COMMIT;

It is easy to guess that in this mode psql actually puts an implicit savepoint before each command, and in case of failure initiates a rollback to it. 
This mode is not used by default, since setting safepoints (even without rolling back to them) incurs significant overhead costs.





