+++++++++++++++++++++++++++++++++++++++++
+       POSTGRES GENERATE REPORT        +
+++++++++++++++++++++++++++++++++++++++++

#WAS USED POSTGRES DATABASE VERSION 9.6
#OR CODE CAN BE COMPILED TO PROTECT DATA USED IN THE FILE
#BEFORE TO START RUN SCRIPT, CREATE THE EXTENSION pg_stat_statements
AND ENABLE THESES PARAMETERS ON POSTGRES.CONF

CREATE EXTENSION pg_stat_statements;

vi postgresql.conf
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.max = 10000
pg_stat_statements.track = all

#SAMPLE OUTPUT
SELECT pg_stat_statements_reset();

#SCRIPT REPORT POSTGRES
vi generate_report.sh
#!/bin/bash

# Has been tested on CentOS 6.x
# author: digoal
# 2015-10
# Permission requirements, OS: root PG: Superuser
# To run ./generate_report.sh >/tmp/report.log 2>&1
# Generate reports directory grep -E "^ ----- >>> | ^ \ |" /tmp/report.log | sed 's / ^ ----- >>> ---- >>> / / '| sed' 1 i \ \ directory \ n \ n '| sed' $ a \ \ n \ n \ \ body \ n \ n '

# Add the following variables modified to be consistent with the current environment, and make sure to connect any database using this configuration does not require a password
export PGHOST=centos7db1
export PGPORT=5432
export PGDATABASE=postgres
export PGUSER=postgres
export PGPASSWORD=<password>
export PGDATA=/postgres/data
export PGHOME=/usr/local/pgsql-9.6.14

export PATH=$PGHOME/bin:$PATH:.
export DATE=`date +"%Y%m%d%H%M"`
export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib:$LD_LIBRARY_PATH


# Remember the current directory
PWD=`pwd`

# Get postgresql log directory
pg_log_dir=`grep '^\ *[a-z]' $PGDATA/postgresql.conf|awk -F "#" '{print $1}'|grep log_directory|awk -F "=" '{print $2}'`

# Check whether standby
is_standby=`psql --pset=pager=off -q -A -t -c 'select pg_is_in_recovery()'`


echo  "     ----- PostgreSQL inspection report -----   "
echo "    ===== $DATE        =====  "


if [ $is_standby == 't' ]; then
echo  "     ===== This is the standby node =====   "
else
echo  "     ===== This is the primary node =====   "
fi
echo ""


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Operating System Information | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Hostname: "
hostname -s
echo ""
echo  " ----- >>> ---- >>> Ethernet link information: "
ip link show
echo ""
echo  " ----- >>> ---- >>> IP address information: "
ip addr show
echo ""
echo  " ----- >>> ---- >>> routing information: "
ip route show
echo ""
echo  " ----- >>> ---- >>> Operating System Kernel: "
uname -a
echo ""
echo "----->>>---->>> Memory(MB): "
free -m
echo ""
echo "----->>>---->>> CPU: "
lscpu
echo ""
echo  " ----- >>> ---- >>> block device: "
lsblk
echo ""
echo "----->>>---->>> Topology: "
lstopo-no-graphics
echo ""
echo  " ----- >>> ---- >>> Process tree: "
pstree -a -A -c -l -n -p -u -U -Z
echo ""
echo  " ----- >>> ---- >>> Static configuration information of the operating system configuration file: "
echo "----->>>---->>>  /etc/sysctl.conf "
grep "^[a-z]" /etc/sysctl.conf
echo ""
echo "----->>>---->>>  /etc/security/limits.conf "
grep -v "^#" /etc/security/limits.conf|grep -v "^$"
echo ""
echo "----->>>---->>>  /etc/security/limits.d/*.conf "
for dir in `ls /etc/security/limits.d`; do echo "/etc/security/limits.d/$dir : "; grep -v "^#" /etc/security/limits.d/$dir|grep -v "^$"; done 
echo ""
echo "----->>>---->>>  /etc/sysconfig/iptables "
cat /etc/sysconfig/iptables
echo ""
echo "----->>>---->>>  /etc/fstab "
cat /etc/fstab
echo ""
echo "----->>>---->>>  /etc/rc.local "
cat /etc/rc.local
echo ""
echo "----->>>---->>>  /etc/selinux/config "
cat /etc/selinux/config
echo ""
echo "----->>>---->>>  /boot/grub/grub.conf "
cat /boot/grub/grub.conf
echo ""
echo  " ----- >>> ---- >>> / var / spool / cron user cron configuration "
for dir in `ls /var/spool/cron`; do echo "/var/spool/cron/$dir : "; cat /var/spool/cron/$dir; done 
echo ""
echo "----->>>---->>>  chkconfig --list "
chkconfig --list
echo ""
echo  " ----- >>> ---- >>> iptables -L -v -n -t filter dynamic configuration information: "
iptables -L -v -n -t filter
echo ""
echo  " ----- >>> ---- >>> iptables -L -v -n -t nat dynamic configuration information: "
iptables -L -v -n -t nat
echo ""
echo  " ----- >>> ---- >>> iptables -L -v -n -t mangle Dynamic configuration information: "
iptables -L -v -n -t mangle
echo ""
echo  " ----- >>> ---- >>> iptables -L -v -n -t raw dynamic configuration information: "
iptables -L -v -n -t raw
echo ""
echo  " ----- >>> ---- >>> sysctl -a dynamic configuration information: "
sysctl -a
echo ""
echo  " ----- >>> ---- >>> mount dynamic configuration information: "
mount -l
echo ""
echo  " ----- >>> ---- >>> selinux dynamic configuration information: "
getsebool
sestatus
echo ""
echo  " ----- >>> ---- >>> Disable Transparent Huge Pages (THP): "
cat /sys/kernel/mm/transparent_hugepage/enabled
cat /sys/kernel/mm/transparent_hugepage/defrag
cat /sys/kernel/mm/redhat_transparent_hugepage/enabled
cat /sys/kernel/mm/redhat_transparent_hugepage/defrag
echo ""
echo  " ----- >>> ---- >>> Hard disk SMART information (requires root): "
smartctl --scan|awk -F "#" '{print $1}' | while read i; do echo -e "\n\nDEVICE $i"; smartctl -a $i; done
echo ""
echo "----->>>---->>>  /var/log/boot.log "
cat /var/log/boot.log
echo ""
echo "----->>>---->>>  /var/log/cron(need root) "
cat / var / log / cron
echo ""
echo "----->>>---->>>  /var/log/dmesg "
cat / var / log / dmesg
echo ""
echo "----->>>---->>>  /var/log/messages(need root) "
tail -n 500 /var/log/messages
echo ""
echo "----->>>---->>>  /var/log/secure(need root) "
cat / var / log / secure
echo ""
echo "----->>>---->>>  /var/log/wtmp "
who -a /var/log/wtmp
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database Information | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Database Version: "
psql --pset=pager=off -q -c 'select version()'

echo  " ----- >>> ---- >>> User installed plugin version: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),* from pg_extension'
done

echo  " ----- >>> ---- >>> How many data types does the user use: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),b.typname,count(*) from pg_attribute a,pg_type b where a.atttypid=b.oid and a.attrelid in (select oid from pg_class where relnamespace not in (select oid from pg_namespace where nspname ~ $$^pg_$$ or nspname=$$information_schema$$)) group by 1,2 order by 3 desc'
done

echo  " ----- >>> ---- >>> How many objects the user created: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),rolname,nspname,relkind,count(*) from pg_class a,pg_authid b,pg_namespace c where a.relnamespace=c.oid and a.relowner=b.oid and nspname !~ $$^pg_$$ and nspname<>$$information_schema$$ group by 1,2,3,4 order by 5 desc'
done

echo  " ----- >>> ---- >>> Histogram of space occupied by user objects: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),buk this_buk_no,cnt rels_in_this_buk,pg_size_pretty(min) buk_min,pg_size_pretty(max) buk_max from( select row_number() over (partition by buk order by tsize),tsize,buk,min(tsize) over (partition by buk),max(tsize) over (partition by buk),count(*) over (partition by buk) cnt from ( select pg_relation_size(a.oid) tsize, width_bucket(pg_relation_size(a.oid),tmin-1,tmax+1,10) buk from (select min(pg_relation_size(a.oid)) tmin,max(pg_relation_size(a.oid)) tmax from pg_class a,pg_namespace c where a.relnamespace=c.oid and nspname !~ $$^pg_$$ and nspname<>$$information_schema$$) t, pg_class a,pg_namespace c where a.relnamespace=c.oid and nspname !~ $$^pg_$$ and nspname<>$$information_schema$$ ) t)t where row_number=1;'
done

echo  " ----- >>> ---- >>> Operating system scheduled tasks for the current user: "
echo "I am `whoami`"
crontab -l
echo  " Suggest: "
echo  "     Success careful examination of the need for regular tasks, and the timing of the task or not criteria, as well as control measures. "
echo  "     Please execute this script as the OS user who started the database. "
echo -e "\n"


common() {
# Into the working directory pg_log
cd  $ PGDATA
eval  cd  $ pg_log_dir

echo  " ----- >>> ---- >>> Get md_hba.conf md5 value: "
md5sum $PGDATA/pg_hba.conf
echo  " Suggest: "
echo  "     The active and standby md5 values are the same (a means to determine whether the contents of the active and standby configuration files are consistent, or use diff). "
echo -e "\n"

echo  " ----- >>> ---- >>> Get pg_hba.conf configuration: "
grep '^\ *[a-z]' $PGDATA/pg_hba.conf
echo  " Suggest: "
echo  "The     main and backup configuration should be as consistent as possible. Pay attention to the harm of the trust and password authentication methods (the clear text of the network transmission password during the password method authentication is recommended to be changed to md5). It is recommended to use md5 or LDAP authentication methods except for unix sockets. . "
echo  "It is     recommended to set up a white list (source IPs allowed by super users, accessible databases), then set a black list (don't allow super users to log in, reject), and then set a white list (general applications), see pg_hba.conf Description. "
echo -e "\n"

echo  " ----- >>> ---- >>> Get postgresql.conf md5 value: "
md5sum $PGDATA/postgresql.conf
echo  " Suggest: "
echo  "     The active and standby md5 values are the same (a means to determine whether the contents of the active and standby configuration files are consistent, or use diff). "
echo -e "\n"

echo  " ----- >>> ---- >>> Get postgresql.conf configuration: "
grep '^\ *[a-z]' $PGDATA/postgresql.conf|awk -F "#" '{print $1}'
echo  " Suggest: "
echo  "The master and backup configuration should be as consistent as possible, and reasonable parameter values should be configured. "
echo -e "is recommended to modify the parameter list as follows (assuming the operating system memory is 128GB, the database exclusive operating system, the database version 9.4.x, other versions may be slightly different, and update it in the future): "
"echo ""
listen_addresses = '0.0.0.0' # listen on all IPV4 addresses
port = 1921 # listen on non-default port
max_connections = 4000 # maximum allowed connections
superuser_reserved_connections = 20 # Connections reserved for superusers
unix_socket_directories = '.' # unix socket file directory is best placed in $ PGDATA to ensure security
unix_socket_permissions = 0700 # Ensure permission security
tcp_keepalives_idle = 30 # Send TCP heartbeat packets intermittently to prevent the connection from being interrupted by the network device.
tcp_keepalives_interval = 10
tcp_keepalives_count = 10
shared_buffers = 16GB # The size of the shared memory managed by the database itself. If large pages are used, the recommended setting is: memory-100 * work_mem-autovacuum_max_workers * (autovacuum_work_mem or autovacuum_work_mem)-max_connections * 1MB
huge_pages = try # Use large pages as much as possible, requires operating system support, configure vm.nr_hugepages * 2MB larger than shared_buffers.
maintenance_work_mem = 512MB # can speed up index creation and garbage collection (assuming autovacuum_work_mem is not set)
autovacuum_work_mem = 512MB # can speed up garbage collection
shared_preload_libraries = 'auth_delay, passwordcheck, pg_stat_statements, auto_explain' # It is recommended to prevent brute force cracking, password complexity detection, enable pg_stat_statements, enable auto_explain, refer to http://blog.163.com/digoal@126/blog/static/16387704020149852941586  
bgwriter_delay = 10ms # How often does the bgwriter process call the write interface (note that it is not fsync) to write the dirty page in the shared buffer to the file system.
bgwriter_lru_maxpages = 1000 # How many dirty pages are written at most in one cycle
max_worker_processes = 20 # If you want to use a worker process, you can fork up to how many worker processes.
wal_level = logical # If you plan to use logical replication in the future, configure it first without changing the machine without downtime.
synchronous_commit = off # If the disk's IOPS capability is normal, it is recommended to use asynchronous commit to improve performance, but when the database crashes or the operating system crash, the transaction log generated in the 2 * wal_writer_delay time period (in the wal buffer) may be lost at most. 
wal_sync_method = open_datasync # Use pg_test_fsync to test the fsync interface of the disk where wal is located. Use good performance.
wal_buffers = 16MB
wal_writer_delay = 10ms
checkpoint_segments = 1024 # is equal to shared_buffers divided by the size of a single wal segment.
checkpoint_timeout = 50min
checkpoint_completion_target = 0.8
archive_mode = on # It is best to open it first, otherwise you need to restart the database to modify
archive_command = '/ bin / date' # It is best to open it first, otherwise you need to restart the database to modify it. In the future, modify it to the correct command. For example, test! -f / home / postgres / archivedir / pg_root /% f && cp% p / home / postgres / archivedir / pg_root /% f
max_wal_senders = 32 # Maximum number of wal sender processes allowed.
wal_keep_segments = 2048 # The number of WAL files retained in the pg_xlog directory is estimated based on the delay of the streaming replication service and the size of the pg_xlog directory.
max_replication_slots = 32 # maximum number of replication slots allowed
hot_standby = on
max_standby_archive_delay = 300s # If the standby database is to be used for read-only, in the case of large queries, if conflicts are encountered, you can consider adjusting this value to avoid conflict queries.
max_standby_streaming_delay = 300s # If the standby database is to be used for read-only, in the case of large queries, if conflicts are encountered, you can consider adjusting this value to avoid conflict queries caused by conflicts.
wal_receiver_status_interval = 1s
hot_standby_feedback = off # It is recommended to turn off, if there is a long query in the standby database, it may cause frequent autovacuum of the main database (such as when the garbage that cannot be collected is needed)
vacuum_defer_cleanup_age = 0 # It is recommended to set it to 0 to avoid frequent autovacuum useless work in the main library, maybe the new version will improve.
random_page_cost = 1.3 # Adjust according to IO capabilities (Enterprise SSD as an example 1.3 is an experience value)
effective_cache_size = 100GB # Adjusted to be as large as memory, or slightly smaller (minus shared_buffer). It is used to evaluate the amount of memory that OS PAGE CACHE can use.
log_destination = 'csvlog'
logging_collector = on
log_truncate_on_rotation = on
log_rotation_size = 10MB
log_min_duration_statement = 1s
log_checkpoints = on
log_connections = on
log_disconnections = on
log_error_verbosity = verbose # print code location in log
log_lock_waits = on
log_statement = 'ddl'
autovacuum = on
log_autovacuum_min_duration = 0
autovacuum_max_workers = 10 # Based on the number of objects that frequently change or delete records
autovacuum_naptime = 30s # fast wake-up, prevent inflation
autovacuum_vacuum_scale_factor = 0.1 # When the garbage exceeds the scale, start the garbage collection worker process
autovacuum_analyze_scale_factor = 0.2  
autovacuum_freeze_max_age = 1600000000
autovacuum_multixact_freeze_max_age = 1600000000
vacuum_freeze_table_age = 1500000000
vacuum_multixact_freeze_table_age = 1500000000
auth_delay.milliseconds = 5000 # authentication failed, how many milliseconds to delay feedback
auto_explain.log_min_duration = 5000 # log how many milliseconds the SQL execution plan was at the time
auto_explain.log_analyze = true
auto_explain.log_verbose = true
auto_explain.log_buffers = true
auto_explain.log_nested_statements = true
pg_stat_statements.track_utility=off
    Recommended operating system configuration (modified according to the actual situation): 
vi /etc/sysctl.conf
# add by digoal.zhou
fs.aio-max-nr = 1048576
fs.file-max = 76724600
kernel.core_pattern= /data01/corefiles/core_%e_%u_%t_%s.%p         
# / data01 / corefiles built in advance, permissions 777
kernel.sem = 4096 2147483647 2147483646 512000    
# Semaphore, ipcs -l or -u view, one group for every 16 processes, each group needs 17 semaphores.
kernel.shmall = 107374182      
# All shared memory segments add size limit (80% of recommended memory)
kernel.shmmax = 274877906944   
# Maximum single shared memory segment size (suggested to be half of the memory),> 9.2 version has greatly reduced the use of shared memory
kernel.shmmni = 819200         
# How many shared memory segments can be generated in total, at least 2 shared memory segments per PG database cluster
net.core.netdev_max_backlog = 10000
net.core.rmem_default = 262144       
# The default setting of the socket receive buffer in bytes.
net.core.rmem_max = 4194304          
# The maximum receive socket buffer size in bytes
net.core.wmem_default = 262144       
# The default setting (in bytes) of the socket send buffer.
net.core.wmem_max = 4194304          
# The maximum send socket buffer size in bytes.
net.core.somaxconn = 4096
net.ipv4.tcp_max_syn_backlog = 4096
net.ipv4.tcp_keepalive_intvl = 20
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_time = 60
net.ipv4.tcp_mem = 8388608 12582912 16777216
net.ipv4.tcp_fin_timeout = 5
net.ipv4.tcp_synack_retries = 2
net.ipv4.tcp_syncookies = 1    
# Enable SYN Cookies. When the SYN waiting queue overflows, enable cookies to handle it to prevent a small number of SYN attacks
net.ipv4.tcp_timestamps = 1    
# Reduce time_wait
net.ipv4.tcp_tw_recycle = 0    
# If = 1, enable fast recovery of TIME-WAIT socket in TCP connection, but the NAT environment may cause the connection to fail, it is recommended that the server close it
net.ipv4.tcp_tw_reuse = 1      
# Turn on reuse. Allow reuse of TIME-WAIT sockets for new TCP connections
net.ipv4.tcp_max_tw_buckets = 262144
net.ipv4.tcp_rmem = 8192 87380 16777216
net.ipv4.tcp_wmem = 8192 65536 16777216
net.nf_conntrack_max = 1200000
net.netfilter.nf_conntrack_max = 1200000
vm.dirty_background_bytes = 409600000       
# The system dirty page reaches this value, the system background brush dirty page scheduling process pdflush (or other) automatically flush the dirty page (dirty_expire_centisecs / 100) seconds to disk
vm.dirty_expire_centisecs = 3000             
# Dirty pages older than this value will be flushed to disk. 3000 means 30 seconds.
vm.dirty_ratio = 95                          
# If the system process is too slow to flush dirty pages, causing the system dirty pages to exceed 95% of memory, if the user process has a write operation to the disk (such as fsync, fdatasync, etc.), you need to actively flush the system dirty pages.
# Effectively prevent user processes from brushing dirty pages, which is very effective in a single machine with multiple instances and using CGROUP to limit single instance IOPS.  
vm.dirty_writeback_centisecs = 100            
# pdflush (or other) wakeup interval for background dirty page process, 100 means 1 second.
vm.extra_free_kbytes = 4096000
vm.min_free_kbytes = 2097152
vm.mmap_min_addr = 65536
vm.overcommit_memory = 0     
# When allocating memory, allow a small amount of over malloc. If it is set to 1, it is considered that there is always enough memory, and a test environment with less memory can use 1.  
vm.overcommit_ratio = 90     
# When overcommit_memory = 2 is used to participate in the calculation of the allowed memory size.
vm.swappiness = 0            
# Shut down the swap partition
vm.zone_reclaim_mode = 0     
# Disable numa, or disable it in vmlinux. 
net.ipv4.ip_local_port_range = 40000 65535    
# Locally assigned TCP and UDP port number range
# vm.nr_hugepages = 102352    
# It is recommended to use large pages when the shared buffer setting exceeds 64GB, page size / proc / meminfo Hugepagesize
vi /etc/security/limits.conf
* soft nofile 1024000
* hard nofile 1024000
* soft    nproc   unlimited
* hard    nproc   unlimited
* soft    core    unlimited
* hard    core    unlimited
* soft    memlock unlimited
* hard    memlock unlimited
rm -f /etc/security/limits.d/90-nproc.conf
\n "

echo  " ----- >>> ---- >>> User or database level custom parameters: "
psql --pset=pager=off -q -c 'select * from pg_db_role_setting'
echo  " Suggest: "
echo  "     Custom parameters need attention, the priority is higher than the database startup parameters and parameters in the configuration file, especially when troubleshooting. "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database Error Log Analysis | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Get error log information: "
cat *.csv | grep -E "^[0-9]" | grep -E "WARNING|ERROR|FATAL|PANIC" | awk -F "," '{print $12" , "$13" , "$14}'|sort|uniq -c|sort -rn
echo  " Suggest: "
echo  "     Refer to http://www.postgresql.org/docs/current/static/errcodes-appendix.html. "
echo -e "\n"

echo  " ----- >>> ---- >>> Get connection request: "
find . -name "*.csv" -type f -mtime -28 -exec grep "connection authorized" {} +|awk -F "," '{print $2,$3,$5}'|sed 's/\:[0-9]*//g'|sort|uniq -c|sort -n -r
echo  " Suggest: "
echo  "When there are many     connection requests, please consider using the connection pool at the application layer, or use the pgbouncer connection pool. "
echo -e "\n"

echo  " ----- >>> ---- >>> Get authentication failure: "
find . -name "*.csv" -type f -mtime -28 -exec grep "password authentication failed" {} +|awk -F "," '{print $2,$3,$5}'|sed 's/\:[0-9]*//g'|sort|uniq -c|sort -n -r
echo  " Suggest: "
echo  "     When there are many authentication failures, there may be users who are brute-forced. It is recommended to use the auth_delay plugin to prevent brute-forced cracking. "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database Slow SQL Log Analysis | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Slow query statistics: "
cat *.csv|awk -F "," '{print $1" "$2" "$3" "$8" "$14}' |grep "duration:"|grep -v "plan:"|awk '{print $1" "$4" "$5" "$6}'|sort|uniq -c|sort -rn
echo  " Suggest: "
echo  "     Output format (number of entries, date, user, database, QUERY, ms). "
echo  "     Slow queries reflect SQL that takes longer to execute than log_min_duration_statement. You can analyze the database or SQL statement for optimization space based on the actual situation. "
echo ""
echo  " ----- >>> ---- >>> the execution time of the first 10 slow query distributions, ms: "
cat *.csv|awk -F "," '{print $1" "$2" "$3" "$8" "$14}' |grep "duration:"|grep -v "plan:"|awk '{print $1" "$4" "$5" "$6" "$7" "$8}'|sort -k 6 -n|head -n 10
echo ""
echo  " ----- >>> ---- >>> slow query distribution execution time of the last 10, ms: "
cat *.csv|awk -F "," '{print $1" "$2" "$3" "$8" "$14}' |grep "duration:"|grep -v "plan:"|awk '{print $1" "$4" "$5" "$6" "$7" "$8}'|sort -k 6 -n|tail -n 10
echo -e "\n"

echo  " ----- >>> ---- >>> auto_explain Analysis Statistics: "
cat *.csv|awk -F "," '{print $1" "$2" "$3" "$8" "$14}' |grep "plan:"|grep "duration:"|awk '{print $1" "$4" "$5" "$6}'|sort|uniq -c|sort -rn
echo  " Suggest: "
echo  "     Output format (number of entries, date, user, database, QUERY). "
echo  "     slow query execution time to reflect over auto_explain.log_min_duration of SQL, you can analyze whether database or SQL statement has room for optimization, analysis csvlog in auto_explain output can be informed of the implementation plan at the statement timeout details of the actual situation. "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database Space Usage Analysis | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Output file system free space: "
df -m
echo  " Suggest: "
echo  "     Note that enough space is reserved for the database. "
echo -e "\n"

echo  " ----- >>> ---- >>> output tablespace corresponding directory: "
echo  $ PGDATA
ls -la $ PGDATA / pg_tblspc /
echo  " Suggest: "
echo  "     Note that if the tablespace is not a soft link, pay attention to whether it is deliberate. Normally it should be a soft link. "
echo -e "\n"

echo  " ----- >>> ---- >>> Output table space usage: "
psql --pset=pager=off -q -c 'select spcname,pg_tablespace_location(oid),pg_size_pretty(pg_tablespace_size(oid)) from pg_tablespace order by pg_tablespace_size(oid) desc'
echo  " Suggest: "
echo  "     remaining space Note checklist space where the file system (the default table space $ PGDATA the / base directory), IOPS distribution is uniform, the OS of the packet can be observed sysstat IO usage. "
echo -e "\n"

echo  " ----- >>> ---- >>> Output database usage: "
psql --pset=pager=off -q -c 'select datname,pg_size_pretty(pg_database_size(oid)) from pg_database order by pg_database_size(oid) desc'
echo  " Suggest: "
echo  "     Check the size of the database, if you need to clean up the historical data. "
echo -e "\n"

echo "----->>>---->>>  TOP 10 size对象: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),b.nspname,c.relname,c.relkind,pg_size_pretty(pg_relation_size(c.oid)),a.seq_scan,a.seq_tup_read,a.idx_scan,a.idx_tup_fetch,a.n_tup_ins,a.n_tup_upd,a.n_tup_del,a.n_tup_hot_upd,a.n_live_tup,a.n_dead_tup from pg_stat_all_tables a, pg_class c,pg_namespace b where c.relnamespace=b.oid and c.relkind=$$r$$ and a.relid=c.oid order by pg_relation_size(c.oid) desc limit 10'
done
echo  " Suggest: "
echo  "     Empirical value: If a single table exceeds 8GB, and this table needs to be frequently updated or deleted + inserted, it is recommended to reasonably split the table according to business logic to obtain better performance, and to facilitate the maintenance of the expanded index; Read the table, it is recommended to properly optimize the SQL statement. "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database Connection Analysis | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Current Activity: "
psql --pset=pager=off -q -c 'select now(),state,count(*) from pg_stat_activity group by 1,2'
echo  " Suggest: "
echo  "     If there are many active states, the database is busy. If there are many idle in transactions, the business logic design may be problematic. If there are many idles, the connection pool may be used, and the minimum number of connections to the connection pool may not be automatically recovered. "
echo -e "\n"

echo  " ----- >>> ---- >>> Total remaining connections: "
psql --pset=pager=off -q -c 'select max_conn,used,res_for_super,max_conn-used-res_for_super res_for_normal from (select count(*) used from pg_stat_activity) t1,(select setting::int res_for_super from pg_settings where name=$$superuser_reserved_connections$$) t2,(select setting::int max_conn from pg_settings where name=$$max_connections$$) t3'
echo  " Suggest: "
echo  "     Set up enough connections for superusers and ordinary users, so as not to log in to the database. "
echo -e "\n"

echo  " ----- >>> ---- >>> User connection limit: "
psql --pset=pager=off -q -c 'select a.rolname,a.rolconnlimit,b.connects from pg_authid a,(select usename,count(*) connects from pg_stat_activity group by usename) b where a.rolname=b.usename order by b.connects desc'
echo  " Suggest: "
echo  "     Set enough connections for the user, alter role ... CONNECTION LIMIT. "
echo -e "\n"

echo  " ----- >>> ---- >>> Database connection limit: "
psql --pset=pager=off -q -c 'select a.datname, a.datconnlimit, b.connects from pg_database a,(select datname,count(*) connects from pg_stat_activity group by datname) b where a.datname=b.datname order by b.connects desc'
echo  " Suggest: "
echo  "     Set enough connections for the database, alter database ... CONNECTION LIMIT. "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database Performance Analysis | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo "----->>>---->>>  TOP 5 SQL : total_cpu_time "
psql --pset=pager=off -q -x -c 'select c.rolname,b.datname,a.total_time/a.calls per_call_time,a.* from pg_stat_statements a,pg_database b,pg_authid c where a.userid=c.oid and a.dbid=b.oid order by a.total_time desc limit 5'
echo  " Suggest: "
echo  "     Check if SQL has room for optimization, and use the auto_explain plugin to observe whether the execution plan of LONG SQL is correct in csvlog. "
echo -e "\n"

echo  " ----- >>> ---- >>> Tables with more than 4 indexes and SIZE greater than 10MB: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(), t2.nspname, t1.relname, pg_size_pretty(pg_relation_size(t1.oid)), t3.idx_cnt from pg_class t1, pg_namespace t2, (select indrelid,count(*) idx_cnt from pg_index group by 1 having count(*)>4) t3 where t1.oid=t3.indrelid and t1.relnamespace=t2.oid and pg_relation_size(t1.oid)/1024/1024.0>10 order by t3.idx_cnt desc'
done
echo  " Suggest: "
echo  "     Too many indexes affect the addition, deletion, and modification of the table. It is recommended to check for unnecessary indexes. "
echo -e "\n"

echo  " ----- >>> ---- >>> Unused or fewer indexes since the last inspection: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),t2.schemaname,t2.relname,t2.indexrelname,t2.idx_scan,t2.idx_tup_read,t2.idx_tup_fetch,pg_size_pretty(pg_relation_size(indexrelid)) from pg_stat_all_tables t1,pg_stat_all_indexes t2 where t1.relid=t2.relid and t2.idx_scan<10 and t2.schemaname not in ($$pg_toast$$,$$pg_catalog$$) and indexrelid not in (select conindid from pg_constraint where contype in ($$p$$,$$u$$,$$f$$)) and pg_relation_size(indexrelid)>65536 order by pg_relation_size(indexrelid) desc'
done
echo  " Suggest: "
echo  "It is     recommended to delete unneeded indexes after confirmation with the application developer. "
echo -e "\n"

echo  " ----- >>> ---- >>> Database statistics, rollback ratio, hit ratio, data block read and write time, deadlock, replication conflict: "
psql --pset=pager=off -q -c 'select datname,round(100*(xact_rollback::numeric/(case when xact_commit > 0 then xact_commit else 1 end + xact_rollback)),2)||$$ %$$ rollback_ratio, round(100*(blks_hit::numeric/(case when blks_read>0 then blks_read else 1 end + blks_hit)),2)||$$ %$$ hit_ratio, blk_read_time, blk_write_time, conflicts, deadlocks from pg_stat_database'
echo  " Suggest: "
echo  "A     large rollback ratio indicates that there may be a problem with the business logic. A small hit ratio indicates that the shared_buffer needs to be increased. A long data block read and write time indicates that the IO performance of the block device needs to be improved. A higher number of deadlocks indicates a problem with the business logic and a copy conflict It means that the standby database may be running LONG SQL. "
echo -e "\n"

echo  " ----- >>> ---- >>> checkpoint, bgwriter stats: "
psql --pset=pager=off -q -x -c 'select * from pg_stat_bgwriter'
echo  " Suggest: "
echo  "     checkpoint_write_time mostly indicates that the checkpoint lasts a long time, and a lot of dirty pages are generated during the checkpoint. "
echo  "     checkpoint_sync_time represents the time when dirty pages in the shared buffer at the beginning of the checkpoint are synchronized to disk. If the time is too long and the database performs poorly at the checkpoint, consider improving the IOPS capability of the block device. "
echo  "     Too many buffers_backend_fsync indicates that you need to increase the shared buffer or reduce the bgwriter_delay parameter. "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database Spam Analysis | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Table index expansion check: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -x -c 'SELECT
  current_database() AS db, schemaname, tablename, reltuples::bigint AS tups, relpages::bigint AS pages, otta,
  ROUND(CASE WHEN otta=0 OR sml.relpages=0 OR sml.relpages=otta THEN 0.0 ELSE sml.relpages/otta::numeric END,1) AS tbloat,
  CASE WHEN relpages < otta THEN 0 ELSE relpages::bigint - otta END AS wastedpages,
  CASE WHEN relpages < otta THEN 0 ELSE bs*(sml.relpages-otta)::bigint END AS wastedbytes,
  CASE WHEN relpages < otta THEN $$0 bytes$$::text ELSE (bs*(relpages-otta))::bigint || $$ bytes$$ END AS wastedsize,
  iname, ituples::bigint AS itups, ipages::bigint AS ipages, iotta,
  ROUND(CASE WHEN iotta=0 OR ipages=0 OR ipages=iotta THEN 0.0 ELSE ipages/iotta::numeric END,1) AS ibloat,
  CASE WHEN ipages < iotta THEN 0 ELSE ipages::bigint - iotta END AS wastedipages,
  CASE WHEN ipages < iotta THEN 0 ELSE bs*(ipages-iotta) END AS wastedibytes,
  CASE WHEN ipages < iotta THEN $$0 bytes$$ ELSE (bs*(ipages-iotta))::bigint || $$ bytes$$ END AS wastedisize,
  CASE WHEN relpages < otta THEN
    CASE WHEN ipages < iotta THEN 0 ELSE bs*(ipages-iotta::bigint) END
    ELSE CASE WHEN ipages < iotta THEN bs*(relpages-otta::bigint)
      ELSE bs*(relpages-otta::bigint + ipages-iotta::bigint) END
  END AS totalwastedbytes
FROM (
  SELECT
    nn.nspname AS schemaname,
    cc.relname AS tablename,
    COALESCE(cc.reltuples,0) AS reltuples,
    COALESCE(cc.relpages,0) AS relpages,
    COALESCE(bs,0) AS bs,
    COALESCE(CEIL((cc.reltuples*((datahdr+ma-
      (CASE WHEN datahdr%ma=0 THEN ma ELSE datahdr%ma END))+nullhdr2+4))/(bs-20::float)),0) AS otta,
    COALESCE(c2.relname,$$?$$) AS iname, COALESCE(c2.reltuples,0) AS ituples, COALESCE(c2.relpages,0) AS ipages,
    COALESCE(CEIL((c2.reltuples*(datahdr-12))/(bs-20::float)),0) AS iotta -- very rough approximation, assumes all cols
  FROM
     pg_class cc
  JOIN pg_namespace nn ON cc.relnamespace = nn.oid AND nn.nspname <> $$information_schema$$
  LEFT JOIN
  (
    SELECT
      ma,bs,foo.nspname,foo.relname,
      (datawidth+(hdr+ma-(case when hdr%ma=0 THEN ma ELSE hdr%ma END)))::numeric AS datahdr,
      (maxfracsum*(nullhdr+ma-(case when nullhdr%ma=0 THEN ma ELSE nullhdr%ma END))) AS nullhdr2
    FROM (
      SELECT
        ns.nspname, tbl.relname, hdr, ma, bs,
        SUM((1-coalesce(null_frac,0))*coalesce(avg_width, 2048)) AS datawidth,
        MAX(coalesce(null_frac,0)) AS maxfracsum,
        hdr+(
          SELECT 1+count(*)/8
          FROM pg_stats s2
          WHERE null_frac<>0 AND s2.schemaname = ns.nspname AND s2.tablename = tbl.relname
        ) AS nullhdr
      FROM pg_attribute att 
      JOIN pg_class tbl ON att.attrelid = tbl.oid
      JOIN pg_namespace ns ON ns.oid = tbl.relnamespace 
      LEFT JOIN pg_stats s ON s.schemaname=ns.nspname
      AND s.tablename = tbl.relname
      AND s.inherited=false
      AND s.attname=att.attname,
      (
        SELECT
          (SELECT current_setting($$block_size$$)::numeric) AS bs,
            CASE WHEN SUBSTRING(SPLIT_PART(v, $$ $$, 2) FROM $$#"[0-9]+.[0-9]+#"%$$ for $$#$$)
              IN ($$8.0$$,$$8.1$$,$$8.2$$) THEN 27 ELSE 23 END AS hdr,
          CASE WHEN v ~ $$mingw32$$ OR v ~ $$64-bit$$ THEN 8 ELSE 4 END AS ma
        FROM (SELECT version() AS v) AS foo
      ) AS constants
      WHERE att.attnum > 0 AND tbl.relkind=$$r$$
      GROUP BY 1,2,3,4,5
    ) AS foo
  ) AS rs
  ON cc.relname = rs.relname AND nn.nspname = rs.nspname
  LEFT JOIN pg_index i ON indrelid = cc.oid
  LEFT JOIN pg_class c2 ON c2.oid = i.indexrelid
) AS sml order by wastedbytes desc limit 5'
done
echo  " Suggest: "
echo  "     Set the appropriate autovacuum_vacuum_scale_factor according to the number of bytes wasted. If large tables are frequently updated or deleted and inserted, it is recommended to set a smaller autovacuum_vacuum_scale_factor to reduce wasted space. "
echo  "     also need to open the autovacuum, based on memory size of the server, the number of the CPU core, provided sufficient autovacuum_work_mem or autovacuum_max_workers or maintenance_work_mem, and sufficiently small autovacuum_naptime. "
echo  "     It also needs to analyze whether the logical backup pg_dump is used for large databases, and whether there are often long SQL and long transactions in the system. These may cause bloat. "
echo  "     Use pg_reorg or vacuum full to reclaim bloated space. "
echo  "     Reference: http://blog.163.com/digoal@126/blog/static/1638770402015329115636287/ "
echo  "The     number of pages actually needed by the table evaluated by otta, and the number of pages actually needed by the index evaluated by iotta; "
echo  "     Bs database block size; "
echo  "     Tbloat table expansion multiple, ibloat index expansion multiple, how many data blocks wastedpages table wasted, how many data blocks wasted by wastedipages index; "
echo  "     How many bytes were wasted by the wastedbytes table, and how many bytes were wasted by the wastedibytes index; "
echo -e "\n"


echo  " ----- >>> ---- >>> Index inflation check: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -x -c 'SELECT
  current_database() AS db, schemaname, tablename, reltuples::bigint AS tups, relpages::bigint AS pages, otta,
  ROUND(CASE WHEN otta=0 OR sml.relpages=0 OR sml.relpages=otta THEN 0.0 ELSE sml.relpages/otta::numeric END,1) AS tbloat,
  CASE WHEN relpages < otta THEN 0 ELSE relpages::bigint - otta END AS wastedpages,
  CASE WHEN relpages < otta THEN 0 ELSE bs*(sml.relpages-otta)::bigint END AS wastedbytes,
  CASE WHEN relpages < otta THEN $$0 bytes$$::text ELSE (bs*(relpages-otta))::bigint || $$ bytes$$ END AS wastedsize,
  iname, ituples::bigint AS itups, ipages::bigint AS ipages, iotta,
  ROUND(CASE WHEN iotta=0 OR ipages=0 OR ipages=iotta THEN 0.0 ELSE ipages/iotta::numeric END,1) AS ibloat,
  CASE WHEN ipages < iotta THEN 0 ELSE ipages::bigint - iotta END AS wastedipages,
  CASE WHEN ipages < iotta THEN 0 ELSE bs*(ipages-iotta) END AS wastedibytes,
  CASE WHEN ipages < iotta THEN $$0 bytes$$ ELSE (bs*(ipages-iotta))::bigint || $$ bytes$$ END AS wastedisize,
  CASE WHEN relpages < otta THEN
    CASE WHEN ipages < iotta THEN 0 ELSE bs*(ipages-iotta::bigint) END
    ELSE CASE WHEN ipages < iotta THEN bs*(relpages-otta::bigint)
      ELSE bs*(relpages-otta::bigint + ipages-iotta::bigint) END
  END AS totalwastedbytes
FROM (
  SELECT
    nn.nspname AS schemaname,
    cc.relname AS tablename,
    COALESCE(cc.reltuples,0) AS reltuples,
    COALESCE(cc.relpages,0) AS relpages,
    COALESCE(bs,0) AS bs,
    COALESCE(CEIL((cc.reltuples*((datahdr+ma-
      (CASE WHEN datahdr%ma=0 THEN ma ELSE datahdr%ma END))+nullhdr2+4))/(bs-20::float)),0) AS otta,
    COALESCE(c2.relname,$$?$$) AS iname, COALESCE(c2.reltuples,0) AS ituples, COALESCE(c2.relpages,0) AS ipages,
    COALESCE(CEIL((c2.reltuples*(datahdr-12))/(bs-20::float)),0) AS iotta -- very rough approximation, assumes all cols
  FROM
     pg_class cc
  JOIN pg_namespace nn ON cc.relnamespace = nn.oid AND nn.nspname <> $$information_schema$$
  LEFT JOIN
  (
    SELECT
      ma,bs,foo.nspname,foo.relname,
      (datawidth+(hdr+ma-(case when hdr%ma=0 THEN ma ELSE hdr%ma END)))::numeric AS datahdr,
      (maxfracsum*(nullhdr+ma-(case when nullhdr%ma=0 THEN ma ELSE nullhdr%ma END))) AS nullhdr2
    FROM (
      SELECT
        ns.nspname, tbl.relname, hdr, ma, bs,
        SUM((1-coalesce(null_frac,0))*coalesce(avg_width, 2048)) AS datawidth,
        MAX(coalesce(null_frac,0)) AS maxfracsum,
        hdr+(
          SELECT 1+count(*)/8
          FROM pg_stats s2
          WHERE null_frac<>0 AND s2.schemaname = ns.nspname AND s2.tablename = tbl.relname
        ) AS nullhdr
      FROM pg_attribute att 
      JOIN pg_class tbl ON att.attrelid = tbl.oid
      JOIN pg_namespace ns ON ns.oid = tbl.relnamespace 
      LEFT JOIN pg_stats s ON s.schemaname=ns.nspname
      AND s.tablename = tbl.relname
      AND s.inherited=false
      AND s.attname=att.attname,
      (
        SELECT
          (SELECT current_setting($$block_size$$)::numeric) AS bs,
            CASE WHEN SUBSTRING(SPLIT_PART(v, $$ $$, 2) FROM $$#"[0-9]+.[0-9]+#"%$$ for $$#$$)
              IN ($$8.0$$,$$8.1$$,$$8.2$$) THEN 27 ELSE 23 END AS hdr,
          CASE WHEN v ~ $$mingw32$$ OR v ~ $$64-bit$$ THEN 8 ELSE 4 END AS ma
        FROM (SELECT version() AS v) AS foo
      ) AS constants
      WHERE att.attnum > 0 AND tbl.relkind=$$r$$
      GROUP BY 1,2,3,4,5
    ) AS foo
  ) AS rs
  ON cc.relname = rs.relname AND nn.nspname = rs.nspname
  LEFT JOIN pg_index i ON indrelid = cc.oid
  LEFT JOIN pg_class c2 ON c2.oid = i.indexrelid
) AS sml order by wastedibytes desc limit 5'
done
echo  " Suggest: "
echo  "     If the index is too bloated, it will affect performance. It is recommended to rebuild the index, create index CONCURRENTLY ... "
echo -e "\n"

echo  " ----- >>> ---- >>> Junk data: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),schemaname,relname,n_dead_tup from pg_stat_all_tables where n_live_tup>0 and n_dead_tup/n_live_tup>0.2 and schemaname not in ($$pg_toast$$,$$pg_catalog$$) order by n_dead_tup desc limit 5'
done
echo  " Suggest: "
echo  "     is usually too much garbage, probably because the process can not be recycled garbage, garbage or busy or does not wake up in time, or did not open autovacuum, or in a short time had a lot of garbage. "
echo  "     You can wait for autovacuum to process, or manually execute vacuum table. "
echo -e "\n"

echo  " ----- >>> ---- >>> Unreferenced Large Objects: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
vacuumlo -n $db -w
echo ""
done
echo  " Suggest: "
echo  "     If the large object is not referenced, it is recommended to delete it, otherwise it is similar to a memory leak. Using vacuumlo can delete large objects that are not referenced, for example: vacuumlo -l 1000 $ db -w. "
echo  "During     application development, pay attention to delete large objects that are not needed in time, use lo_unlink or the corresponding API of the driver. "
echo  "     Reference http://www.postgresql.org/docs/9.4/static/largeobjects.html "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database Age Analysis | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Database age: "
psql --pset=pager=off -q -c 'select datname,age(datfrozenxid),2^31-age(datfrozenxid) age_remain from pg_database order by age(datfrozenxid) desc'
echo  " Suggest: "
echo  "     The age of the database should normally be less than vacuum_freeze_table_age. If the remaining age is less than 500 million, human intervention is recommended. After the LONG SQL or transaction is killed, vacuum freeze is performed. "
echo -e "\n"

echo  " ----- >>> ---- >>> Table age: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),rolname,nspname,relkind,relname,age(relfrozenxid),2^31-age(relfrozenxid) age_remain from pg_authid t1 join pg_class t2 on t1.oid=t2.relowner join pg_namespace t3 on t2.relnamespace=t3.oid where t2.relkind in ($$t$$,$$r$$) order by age(relfrozenxid) desc limit 5'
done
echo  " Suggest: "
echo  "     The age of the table should normally be less than vacuum_freeze_table_age. If the remaining age is less than 500 million, human intervention is recommended. After the LONG SQL or transaction is killed, vacuum freeze is performed. "
echo -e "\n"

echo  " ----- >>> ---- >>> Long transaction, 2PC: "
psql --pset=pager=off -q -x -c 'select datname,usename,query,xact_start,now()-xact_start xact_duration,query_start,now()-query_start query_duration,state from pg_stat_activity where state<>$$idle$$ and (backend_xid is not null or backend_xmin is not null) and now()-xact_start > interval $$30 min$$ order by xact_start'
psql --pset=pager=off -q -x -c 'select name,statement,prepare_time,now()-prepare_time,parameter_types,from_sql from pg_prepared_statements where now()-prepare_time > interval $$30 min$$ order by prepare_time'
echo  " Suggest: "
echo  "     garbage generated in the process long affairs, can not be recovered, it is recommended not to run LONG SQL in the database, or stagger peak hours to run DML LONG SQL. 2PC transaction must remember off the end as soon as possible, otherwise it may cause database bloat. "
echo  "     Reference: http://blog.163.com/digoal@126/blog/static/1638770402015329115636287/ "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database XLOG, Stream Replication Status Analysis | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Whether to enable archiving, automatic garbage collection: "
psql --pset=pager=off -q -c 'select name,setting from pg_settings where name in ($$archive_mode$$,$$autovacuum$$,$$archive_command$$)'
echo  " Suggest: "
echo  "It is     recommended to enable automatic garbage collection and archive. "
echo -e "\n"

echo  " ----- >>> ---- >>> Archive Statistics: "
psql --pset=pager=off -q -c 'select pg_xlogfile_name(pg_current_xlog_location()) now_xlog, * from pg_stat_archiver'
echo  " Suggest: "
echo  "     If there is a large difference between the current XLOG file and the last failed XLOG file, it is recommended to investigate the cause of the archiving failure as soon as possible for repair, otherwise the pg_xlog directory may burst. "
echo -e "\n"

echo  " ----- >>> ---- >>> Stream replication statistics: "
psql --pset=pager=off -q -x -c 'select pg_xlog_location_diff(pg_current_xlog_location(),flush_location), * from pg_stat_replication'
echo  " Suggest: "
echo  "     Focus on the delay of stream replication. If the delay is very large, it is recommended to check the network bandwidth, as well as the performance of reading xlog locally and the performance of writing xlog remotely. "
echo -e "\n"

echo  " ----- >>> ---- >>> Stream Copy Slot: "
psql --pset=pager=off -q -c 'select pg_xlog_location_diff(pg_current_xlog_location(),restart_lsn), * from pg_replication_slots'
echo  " Suggest: "
echo  "     If restart_lsn and the current XLOG have a very large number of bytes, you need to check whether the subscribers of the slot can receive XLOG normally, or whether the subscribers are normal. If the slot data is not taken away for a long time, the pg_xlog directory may burst. "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Database Security or Potential Risk Analysis | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Password leak check: "
echo  "     Check ~ / .psql_history:   "
grep -i "password" ~/.psql_history|grep -i -E "role|group|user"
echo ""
echo  "     Check * .csv:   "
cat *.csv | grep -E "^[0-9]" | grep -i -r -E "role|group|user" |grep -i "password"|grep -i -E "create|alter"
echo ""
echo  "     Check $ PGDATA /recovery.*:   "
grep -i "password" ../recovery.*
echo ""
echo  "     Check pg_stat_statements:   "
psql --pset=pager=off -c 'select query from pg_stat_statements where (query ~* $$group$$ or query ~* $$user$$ or query ~* $$role$$) and query ~* $$password$$'
echo  "     Check pg_authid:   "
psql --pset=pager=off -q -c 'select * from pg_authid where rolpassword !~ $$^md5$$ or length(rolpassword)<>35'
echo  "     Check pg_user_mappings, pg_views:   "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -c 'select current_database(),* from pg_user_mappings where umoptions::text ~* $$password$$'
psql -d $db --pset=pager=off -c 'select current_database(),* from pg_views where definition ~* $$password$$ and definition ~* $$dblink$$'
done
echo  " Suggest: "
echo  "     If the above output shows that the password has been leaked, modify it as soon as possible, and prevent the password from being recorded in the above file by parameters (psql -n) (set log_statement = 'none'; set log_min_duration_statement = -1; set log_duration = off; set pg_stat_statements.track_utility = off;). "
echo  "The     plaintext password is not secure, it is recommended to use create | alter role ... encrypted password. "
echo  "     Password plaintext is not recommended in fdw, dblink based view. "
echo  "     Do not use a password in the configuration of recovery. *, it is not secure, you can use .pgpass to configure the password. "
echo -e "\n"

echo  " ----- >>> ---- >>> Simple Password Check: "
echo  "     1. Check if the existing password is simple, extract the password dictionary from crackdb library, check one by one:   "
echo  "     Check if md5 (' $ pwd ' || ' $ username ') matches pg_authid.rolpassword:   "
echo  "A     match indicates that the user used a simple password:   "
echo ""
echo  "     2. Prior check reference http://blog.163.com/digoal@126/blog/static/16387704020149852941586 "
echo -e "\n"

echo  " ----- >>> ---- >>> User password expiration time: "
psql --pset=pager=off -q -c 'select rolname,rolvaliduntil from pg_authid order by rolvaliduntil'
echo  " Suggest: "
echo  "After     expiration, the user will not be able to log in, remember to change the password, and extend the password expiration time to a certain time or unlimited time, alter role ... VALID UNTIL 'timestamp'. "
echo -e "\n"

echo  " ----- >>> ---- >>> SQL injection risk analysis: "
cat *.csv | grep -E "^[0-9]" | grep exec_simple_query |awk -F "," '{print $2" "$3" "$5" "$NF}'|sed 's/\:[0-9]*//g'|sort|uniq -c|sort -n -r
echo  " Suggest: "
echo  "     Calling exec_simple_query is risky, allowing multiple SQL packages to be called in one interface. It is recommended that programs use bind variables to avoid SQL injection risks, or that the terminal uses the SQL injection filtering plugin. "
echo -e "\n"

echo  " ----- >>> ---- >>> Rule security check on normal user objects: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -c 'select current_database(),a.schemaname,a.tablename,a.rulename,a.definition from pg_rules a,pg_namespace b,pg_class c,pg_authid d where a.schemaname=b.nspname and a.tablename=c.relname and d.oid=c.relowner and not d.rolsuper union all select current_database(),a.schemaname,a.viewname,a.viewowner,a.definition from pg_views a,pg_namespace b,pg_class c,pg_authid d where a.schemaname=b.nspname and a.viewname=c.relname and d.oid=c.relowner and not d.rolsuper'
done
echo  " Suggest: "
echo  "     Prevent ordinary users from setting traps in rules. Beware of dangerous security invoker function calls. Superusers may call these dangerous functions (in the role of invoice) by mistake after the rule is triggered. "
echo  "     Reference http://blog.163.com/digoal@126/blog/static/16387704020155131217736/ "
echo -e "\n"

echo  " ----- >>> ---- >>> Ordinary user-defined function security check: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -c 'select current_database(),b.rolname,c.nspname,a.proname from pg_proc a,pg_authid b,pg_namespace c where a.proowner=b.oid and a.pronamespace=c.oid and not b.rolsuper and not a.prosecdef'
done
echo  " Suggest: "
echo  "     Prevent ordinary users from setting traps in functions. Pay attention to dangerous security invoker function calls. Superusers may call these dangerous functions by mistake (invoker role) after triggering. "
echo  "     Reference http://blog.163.com/digoal@126/blog/static/16387704020155131217736/ "
echo -e "\n"

echo  " ----- >>> ---- >>> unlogged table and hash index: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),t3.rolname,t2.nspname,t1.relname from pg_class t1,pg_namespace t2,pg_authid t3 where t1.relnamespace=t2.oid and t1.relowner=t3.oid and t1.relpersistence=$$u$$'
psql -d $db --pset=pager=off -q -c 'select current_database(),pg_get_indexdef(oid) from pg_class where relkind=$$i$$ and pg_get_indexdef(oid) ~ $$USING hash$$'
done
echo  " Suggest: "
echo  "     Unlogged table and hash index do not record XLOG, and cannot be replicated to the standby node using streaming replication or log shipping. If some SQL is executed on the standby node, it may cause errors or no data. "
echo  "     Unlogged table and hash index cannot be repaired after CRASH of the database, it is not recommended. "
echo  "     PITR has no effect on unlogged table and hash index. "
echo -e "\n"

echo  " ----- >>> ---- >>> Sequence check with less than 10 million times remaining: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off <<EOF
create or replace function f(OUT v_datname name, OUT v_role name, OUT v_nspname name, OUT v_relname name, OUT v_times_remain int8) returns setof record as \$\$
declare
begin
  v_datname := current_database();
  for v_role,v_nspname,v_relname in select rolname,nspname,relname from pg_authid t1 , pg_class t2 , pg_namespace t3 where t1.oid=t2.relowner and t2.relnamespace=t3.oid and t2.relkind='S' 
  LOOP
    execute 'select (max_value-last_value)/increment_by from "'||v_nspname||'"."'||v_relname||'" where not is_cycled' into v_times_remain;
    return next;
  end loop;
end;
\$\$ language plpgsql;
select * from f() where v_times_remain is not null and v_times_remain < 10240000 order by v_times_remain limit 10;
EOF
done
echo  " Suggest: "
echo  "After the     remaining number of uses of the sequence is reached, it will not be available, an error will be reported, please pay attention to the developer. "
echo -e "\n"

echo  " ----- >>> ---- >>> trigger, event trigger: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),relname,tgname,proname,tgenabled from pg_trigger t1,pg_class t2,pg_proc t3 where t1.tgfoid=t3.oid and t1.tgrelid=t2.oid'
psql -d $db --pset=pager=off -q -c 'select current_database(),rolname,proname,evtname,evtevent,evtenabled,evttags from pg_event_trigger t1,pg_proc t2,pg_authid t3 where t1.evtfoid=t2.oid and t1.evtowner=t3.oid'
done
echo  " Suggest: "
echo  "     Please note the need for triggers and event triggers. "
echo -e "\n"

echo  " ----- >>> ---- >>> Check if a letter other than az 0-9 _ is used as the object name: "
psql --pset=pager=off -q -c 'select distinct datname from (select datname,regexp_split_to_table(datname,$$$$) word from pg_database) t where (not (ascii(word) >=97 and ascii(word) <=122)) and (not (ascii(word) >=48 and ascii(word) <=57)) and ascii(word)<>95'
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select current_database(),relname,relkind from (select relname,relkind,regexp_split_to_table(relname,$$$$) word from pg_class) t where (not (ascii(word) >=97 and ascii(word) <=122)) and (not (ascii(word) >=48 and ascii(word) <=57)) and ascii(word)<>95 group by 1,2,3'
psql -d $db --pset=pager=off -q -c 'select current_database(), typname from (select typname,regexp_split_to_table(typname,$$$$) word from pg_type) t where (not (ascii(word) >=97 and ascii(word) <=122)) and (not (ascii(word) >=48 and ascii(word) <=57)) and ascii(word)<>95 group by 1,2'
psql -d $db --pset=pager=off -q -c 'select current_database(), proname from (select proname,regexp_split_to_table(proname,$$$$) word from pg_proc where proname !~ $$^RI_FKey_$$) t where (not (ascii(word) >=97 and ascii(word) <=122)) and (not (ascii(word) >=48 and ascii(word) <=57)) and ascii(word)<>95 group by 1,2'
psql -d $db --pset=pager=off -q -c 'select current_database(),nspname,relname,attname from (select nspname,relname,attname,regexp_split_to_table(attname,$$$$) word from pg_class a,pg_attribute b,pg_namespace c where a.oid=b.attrelid and a.relnamespace=c.oid ) t where (not (ascii(word) >=97 and ascii(word) <=122)) and (not (ascii(word) >=48 and ascii(word) <=57)) and ascii(word)<>95 group by 1,2,3,4'
done
echo  " Suggest: "
echo  "It is     recommended to use only az, 0-9, _ for any identify (such as table name, column name, view name, function name, type name, database name, schema name, materialized view name, etc.). "
echo "    identify usage https://yq.aliyun.com/articles/52883 . "
echo "    https://www.postgresql.org/docs/9.5/static/sql-keywords-appendix.html . "
echo "    https://www.postgresql.org/docs/9.5/static/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS . "
echo -e "\n"

echo  " ----- >>> ---- >>> lock wait: "
psql -x --pset=pager=off <<EOF
with    
t_wait as    
(    
  select a.mode,a.locktype,a.database,a.relation,a.page,a.tuple,a.classid,a.granted,   
  a.objid,a.objsubid,a.pid,a.virtualtransaction,a.virtualxid,a.transactionid,a.fastpath,    
  b.state,b.query,b.xact_start,b.query_start,b.usename,b.datname,b.client_addr,b.client_port,b.application_name   
    from pg_locks a,pg_stat_activity b where a.pid=b.pid and not a.granted   
),   
t_run as   
(   
  select a.mode,a.locktype,a.database,a.relation,a.page,a.tuple,a.classid,a.granted,   
  a.objid,a.objsubid,a.pid,a.virtualtransaction,a.virtualxid,a.transactionid,a.fastpath,   
  b.state,b.query,b.xact_start,b.query_start,b.usename,b.datname,b.client_addr,b.client_port,b.application_name   
    from pg_locks a,pg_stat_activity b where a.pid=b.pid and a.granted   
),   
t_overlap as   
(   
  select r.* from t_wait w join t_run r on   
  (   
    r.locktype is not distinct from w.locktype and   
    r.database is not distinct from w.database and   
    r.relation is not distinct from w.relation and   
    r.page is not distinct from w.page and   
    r.tuple is not distinct from w.tuple and   
    r.virtualxid is not distinct from w.virtualxid and   
    r.transactionid is not distinct from w.transactionid and   
    r.classid is not distinct from w.classid and   
    r.objid is not distinct from w.objid and   
    r.objsubid is not distinct from w.objsubid and   
    r.pid <> w.pid   
  )    
),    
t_unionall as    
(    
  select r.* from t_overlap r    
  union all    
  select w.* from t_wait w    
)    
select locktype,datname,relation::regclass,page,tuple,virtualxid,transactionid::text,classid::regclass,objid,objsubid,   
string_agg(   
'Pid: '||case when pid is null then 'NULL' else pid::text end||chr(10)||   
'Lock_Granted: '||case when granted is null then 'NULL' else granted::text end||' , Mode: '||case when mode is null then 'NULL' else mode::text end||' , FastPath: '||case when fastpath is null then 'NULL' else fastpath::text end||' , VirtualTransaction: '||case when virtualtransaction is null then 'NULL' else virtualtransaction::text end||' , Session_State: '||case when state is null then 'NULL' else state::text end||chr(10)||   
'Username: '||case when usename is null then 'NULL' else usename::text end||' , Database: '||case when datname is null then 'NULL' else datname::text end||' , Client_Addr: '||case when client_addr is null then 'NULL' else client_addr::text end||' , Client_Port: '||case when client_port is null then 'NULL' else client_port::text end||' , Application_Name: '||case when application_name is null then 'NULL' else application_name::text end||chr(10)||    
'Xact_Start: '||case when xact_start is null then 'NULL' else xact_start::text end||' , Query_Start: '||case when query_start is null then 'NULL' else query_start::text end||' , Xact_Elapse: '||case when (now()-xact_start) is null then 'NULL' else (now()-xact_start)::text end||' , Query_Elapse: '||case when (now()-query_start) is null then 'NULL' else (now()-query_start)::text end||chr(10)||    
'SQL (Current SQL in Transaction): '||chr(10)||  
case when query is null then 'NULL' else query::text end,    
chr (10) || '--------' || chr (10)    
order by    
  (  case mode    
    when 'INVALID' then 0   
    when 'AccessShareLock' then 1   
    when 'RowShareLock' then 2   
    when 'RowExclusiveLock' then 3   
    when 'ShareUpdateExclusiveLock' then 4   
    when 'ShareLock' then 5   
    when 'ShareRowExclusiveLock' then 6   
    when 'ExclusiveLock' then 7   
    when 'AccessExclusiveLock' then 8   
    else 0   
  end  ) desc,   
  (case when granted then 0 else 1 end)  
) as lock_conflict  
from t_unionall   
group by   
locktype,datname,relation,page,tuple,virtualxid,transactionid::text,classid,objid,objsubid ;   
EOF
echo  " Suggest: "
echo  "     Lock waiting status, reflecting business logic problems or SQL performance problems, it is recommended to investigate the locked SQL. "
echo -e "\n"

echo  " ----- >>> ---- >>> Inheritance check: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -q -c 'select inhrelid::regclass,inhparent::regclass,inhseqno from pg_inherits order by 2,3'
done
echo  " Suggest: "
echo  "     If you use inheritance to implement the partition table, pay attention to the partition table of the trigger logic is normal, for the time pattern of the partition table partition plus the need for timely, modify the trigger function. "
echo  "It is     recommended that the permissions of the inherited table be unified. If the permissions are inconsistent, it may cause some users to have insufficient permissions when querying. "
echo -e "\n"


echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Reset statistics | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Reset statistics: "
for db in `psql --pset=pager=off -t -A -q -c 'select datname from pg_database where datname not in ($$template0$$, $$template1$$)'`
do
psql -d $db --pset=pager=off -c 'select pg_stat_reset()'
done
psql --pset=pager=off -c 'select pg_stat_reset_shared($$bgwriter$$)'
psql --pset=pager=off -c 'select pg_stat_reset_shared($$archiver$$)'

echo  " ----- >>> ---- >>> reset pg_stat_statements statistics: "
psql --pset=pager=off -q -A -c 'select pg_stat_statements_reset()'

}  # common function end


primary() {
echo  " ----- >>> ---- >>> Get recovery.done md5 value: "
md5sum $PGDATA/recovery.done
echo  " Suggest: "
echo  "     The active and standby md5 values are the same (a means to determine whether the contents of the active and standby configuration files are consistent, or use diff). "
echo -e "\n"

echo  " ----- >>> ---- >>> Get recovery.done configuration: "
grep '^\ *[a-z]' $PGDATA/recovery.done|awk -F "#" '{print $1}'
echo  " Suggest: "
echo  "     Do not configure a password in primary_conninfo, it is easy to leak. It is recommended to create a replication role for stream replication users, and configure pg_hba.conf to allow only the required source IP connections. "
echo -e "\n"
}  # primary function end


standby() {
echo  " ----- >>> ---- >>> Get recovery.conf md5 value: "
md5sum $PGDATA/recovery.conf
echo  " Suggest: "
echo  "     The active and standby md5 values are the same (a means to determine whether the contents of the active and standby configuration files are consistent, or use diff). "
echo -e "\n"

echo  " ----- >>> ---- >>> Get recovery.conf configuration: "
grep '^\ *[a-z]' $PGDATA/recovery.conf|awk -F "#" '{print $1}'
echo  " Suggest: "
echo  "     Do not configure a password in primary_conninfo, it is easy to leak. It is recommended to create a replication role for stream replication users, and configure pg_hba.conf to allow only the required source IP connections. "
echo -e "\n"
}  # standby function end


adds() {
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo  " | Additional Information | "
echo "|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++|"
echo ""

echo  " ----- >>> ---- >>> Appendix 1: `date -d "-1 day" +%Y-%m-%d` Statistics collected by operating system sysstat "
sar -A -f /var/log/sa/sa`date -d "-1 day" +%d`
echo -e "\n"

echo  " ----- >>> ---- >>> Other suggestions: "
echo  "     Other suggested inspections: "
echo  "         HA status is normal, such as checking HA programs, checking heartbeat table delay. "
echo "        sar io, load, ...... "
echo  "After the     inspection, clear the csv log "
}  # adds function end

common
adds
cd $pwd

#   Remarks
#   csv log analysis needs optimization
#   Some actions require root
