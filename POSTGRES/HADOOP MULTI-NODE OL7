+++++++++++++++++++++++++++++
+   HADOOP MULTI-NODE OL7   +
+++++++++++++++++++++++++++++

THE APACHE HADOOP FRAMEWORK CONSISTS OF THE FOLLOWING MODULES:
* HADOOP COMMON - CONTAINS THE COMMON LIBRARIES AND FILES NEEDED FOR ALL HADOOP MODULES.
* HADOOP DISTRIBUTED FILE SYSTEM (HDFS) - DISTRIBUTED FILE SYSTEM THAT STORES DATA ON MACHINES WITHIN THE CLUSTER, ON DEMAND, ALLOWING VERY LARGE BANDWIDTH ACROSS THE CLUSTER.
* HADOOP YARN - IT IS A RESOURCE MANAGEMENT PLATFORM RESPONSIBLE FOR THE MANAGEMENT OF COMPUTATIONAL RESOURCES IN THE CLUSTER, AS WELL AS RESOURCE SCHEDULING.
* HADOOP MAPREDUCE - PROGRAMMING MODEL FOR LARGE-SCALE PROCESSING.

#CREATE AND INSTALL MULTI-NODE HADOOP ON VIRTUALBOX (ORACLE LINUX 7)
#GUIDE : https://medium.com/@mr_cheng/instala%C3%A7%C3%A3o-do-apache-hadoop-3-2-1-multi-node-no-ubuntu-18-04-utilizando-oracle-virtualbox-f5aa90bc0b5a

#CONFIGURE IN ALL HADOOP MACHINES
#NETWORK 
vi /etc/hosts
192.168.56.119  hadoopmaster1
192.168.56.120  hadoopslave1
192.168.56.121  hadoopslave2

#DISABLE SELINUX AND STOP FIREWALL IN ALL NODES 
sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinux && setenforce 0
systemctl stop firewalld && systemctl disable firewalld

#ADD REPOSITORIES TO YUM 
vi /etc/yum.repos.d/oracle-linux-ol7.repo

[ol7_optional_developer]
name=Developer Preview of Oracle Linux $releasever Optional ($basearch)
baseurl=https://yum.oracle.com/repo/OracleLinux/OL7/optional/developer/$basearch/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
gpgcheck=1
enabled=1

[ol7_developer_EPEL]
name=Oracle Linux $releasever Development Packages ($basearch)
baseurl=https://yum.oracle.com/repo/OracleLinux/OL7/developer_EPEL/$basearch/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
gpgcheck=1
enabled=1

#LIST REPOSITORIES LIST
yum repolist

#INSTALL PRE-REQUIRES PACKAGES
yum install pdsh.x86_64 -y
yum install java-1.8.0-openjdk.x86_64 -y 

#CHECK JAVA VERSION
java -version

#DOWNLOAD HADOOP 3.2
wget https://mirrors.sonic.net/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz

#EXTRACT SOFTWARE
tar xzf hadoop-3.2.1.tar.gz

#RENAME FOLDER
mv hadoop-3.2.1 hadoop

#CONFIGURE JAVA_HOME ON HADOOP-ENV.SH
vi ~/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/jre

#MOVE HADOOP FOR /USR/LOCAL
mv hadoop /usr/local/hadoop

#CHANGE HOSTNAME
#FOR SLAVE MACHINES, SET THE HOSTNAME
hostnamectl 
hostnamectl set-hostname hadoopmaster1
hostnamectl --static 

#ADD VARIABLES /ETC/ENVIRONMENT
vi /etc/environment
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/hadoop/bin:/usr/local/hadoop/sbin"
JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.242.b08-0.el7_7.x86_64/jre"

#ADD USER HADOOPUSER
adduser hadoopuser
usermod -aG hadoopuser hadoopuser
chown hadoopuser:root -R /usr/local/hadoop/
chmod g+rwx -R /usr/local/hadoop/
id hadoopuser
uid=1001(hadoopuser) gid=1001(hadoopuser) groups=1001(hadoopuser)
su - hadoopuser

#GERENATE SSH KEY
ssh-keygen -t rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys

#VALIDADE LOCAL ACCESS
ssh localhost

#CONFIGURE ENVORIMENTS VARIABLES
vi .bashrc
export PDSH_RCMD_TYPE=ssh

#REBOOT master MACHINE
#REPEAT THE ABOVE CONFIGURATION ON SLAVE1 AND SLAVE2

#COPY SSH KEY AMONG THE THREE MACHINES
COPY hadoopuser@hadoopmaster1:/home/hadoopuser/.ssh/id_rsa.pub TO FILE /home/hadoopuser/.ssh/authorized_keys
COPY hadoopuser@hadoopslave1:/home/hadoopuser/.ssh/id_rsa.pub TO FILE /home/hadoopuser/.ssh/authorized_keys
COPY hadoopuser@hadoopslave2:/home/hadoopuser/.ssh/id_rsa.pub TO FILE /home/hadoopuser/.ssh/authorized_keys

## HADOOP MASTER ##

#IN HADOOP-MASTER, OPEN THE FILE CORE-SITE.XML AND ADD THE CONTENT AT THE END OF THE FILE:
vi /usr/local/hadoop/etc/hadoop/core-site.xml

<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://hadoopmaster1:9000</value>
</property>
</configuration>

# IN HADOOP-MASTER, OPEN THE FILE HDFS-SITE.XML:
vi /usr/local/hadoop/etc/hadoop/hdfs-site.xml

<configuration>
<property>
<name>dfs.namenode.name.dir</name><value>/usr/local/hadoop/data/nameNode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name><value>/usr/local/hadoop/data/dataNode</value>
</property>
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
</configuration>











