+++++++++++++++++++++++++++++++++++++++++++++++++
+    INSTALL PGPOOL II HIGH AVAILABLE ON OL7    +
+++++++++++++++++++++++++++++++++++++++++++++++++

#CLUSTER SYSTEM CONFIGURATION
MACHINE             HOSTNAME            IP              VIP
OL7_PGPOOL2_PRIM    pgpool2_prim        192.168.1.126   192.168.1.129      
OL7_PGPOOL2_STDB1   pgpool2_stdb1       192.168.1.127   192.168.1.129
OL7_PGPOOL2_STDB2   pgpool2_stdb2       192.168.1.128   192.168.1.129

#DISABLE SELINUX AND FIREWALLD

sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/config && setenforce 0
systemctl stop firewalld && systemctl disable firewalld

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE STATIC NETWORK
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=none
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=no
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=enp0s3
UUID=49c279da-7065-40f5-b914-176973db2a0a
DEVICE=enp0s3
ONBOOT=yes
IPADDR=192.168.1.126
PREFIX=24
GATEWAY=192.168.1.1

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE HOSTNAME
hostnamectl 
hostnamectl set-hostname pgpool2_prim
hostnamectl --static

#REPEAT CONFIGURATION FOR OTHER MACHINES

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE /ETC/HOSTS
vi /etc/hosts
192.168.1.126   pgpool2_prim
192.168.1.127   pgpool2_stdb1
192.168.1.128   pgpool2_stdb2

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE DNS
vi /etc/resolv.conf
nameserver 8.8.8.8
nameserver 8.8.4.4

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE AND INSTALL SOFTWARE POSTGRESQL
vi /etc/yum.repos.d/oracle-linux-ol7.repo

[ol7_optional_developer]
name=Developer Preview of Oracle Linux $releasever Optional ($basearch)
baseurl=https://yum.oracle.com/repo/OracleLinux/OL7/optional/developer/$basearch/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
gpgcheck=1
enabled=1

[ol7_developer_EPEL]
name=Oracle Linux $releasever Development Packages ($basearch)
baseurl=https://yum.oracle.com/repo/OracleLinux/OL7/developer_EPEL/$basearch/
gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-oracle
gpgcheck=1
enabled=1

yum install openssl-devel libtermcap-devel readline-devel gcc bison flex perl libconfig-devel kernel-devel rsync -y
yum install zlib jzlib zlib-devel -y
yum install wget -y
yum install libmemcached libmemcached-devel memcached.x86_64 memcached-devel libpqxx -y 
systemctl enable memcached
systemctl start memcached

#REPEAT CONFIGURATION FOR OTHER MACHINES

#GET SFW DATABASE POSTGRES
wget https://ftp.postgresql.org/pub/source/$VERSION/postgresql-$VERSION.tar.gz
tar xvfz postgresql-$VERSION.tar.gz
cd postgresql-$VERSION

#INSTALL SFW DATABASE POSTGRES
## install in default path /usr/local/pgsql using port 5432
./configure --with-openssl
example [./configure --prefix=$path/pgsql-$VERSION --with-openssl]
make
make install
cd contrib/
make
make install
ls -l /usr/local/pgsql/

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CREATE USER AND DIRECTORY
adduser postgres
passwd postgres
mkdir -p /dados/data
mkdir -p /postgres/wals
chown postgres:postgres -R /dados
chown postgres:postgres -R /postgres

#REPEAT CONFIGURATION FOR OTHER MACHINES

su - postgres
/usr/local/pgsql-11.4/bin/initdb --locale=pt_BR.UTF-8 -D /dados/data/

#CONFIGURE ONLY ON PRIMARY

#CREATE SERVICE POSTGRESQL
vi /usr/lib/systemd/system/postgresql.service
[Unit]
Description=PostgreSQL database server
Documentation=man:postgres(1)

[Service]
Type=notify
User=postgres
ExecStart=/usr/local/pgsql-11.4/bin/postgres -D /dados/data
ExecReload=/bin/kill -HUP $MAINPID
KillMode=mixed
KillSignal=SIGINT
TimeoutSec=0

[Install]
WantedBy=multi-user.target

systemctl daemon-reload
systemctl enable postgresql.service
systemctl status postgresql.service
systemctl start postgresql.service

#REPEAT CONFIGURATION FOR OTHER MACHINES

#INSTALL SFW DATABASE PGPOOL
yum install -y https://www.pgpool.net/yum/rpms/4.1/redhat/rhel-7-x86_64/pgpool-II-release-4.1-2.noarch.rpm
yum install -y pgpool-II-pg11*

#REPEAT CONFIGURATION FOR OTHER MACHINES

#CONFIGURE POSTGRES PRIMARY
vi /dados/data/postgresql.conf
listen_addresses = '*'
port = 5432
wal_level = replica
archive_mode = on
archive_command = 'gzip -c %p > /postgres/wals/%f.gz'
archive_timeout = 600
max_wal_senders = 10
wal_keep_segments = 32
hot_standby = on
wal_log_hints = on 

systemctl stop postgresql.service
systemctl start postgresql.service

#CREATE USER FOR REPLICATION PRIMARY
su - postgres
/usr/local/pgsql-11.4/bin/psql -U postgres -p 5432
SET password_encryption = 'scram-sha-256';
CREATE ROLE pgpool WITH LOGIN;
CREATE ROLE repl WITH REPLICATION LOGIN;
\password pgpool
\password repl
\password postgres
GRANT pg_monitor TO pgpool;
\q
exit

#CONFIGURE PG_HBA PRIMARY

vi /dados/data/pg_hba.conf
host    all             all             192.168.1.0/24          scram-sha-256
host    replication     all             192.168.1.0/24          scram-sha-256
su - postgres -c "/usr/local/pgsql-11.4/bin/pg_ctl reload -D /dados/data"

#TO PERFORM THE AUTOMATIVE FAILOVER IT IS NECESSARY TO EXCHANGE SSH KEYS USER ROOT AND POSTGRES USER
ssh-keygen -t rsa
cd .ssh/
ssh-copy-id -i id_rsa.pub root@192.168.1.126
ssh-copy-id -i id_rsa.pub root@192.168.1.127
ssh-copy-id -i id_rsa.pub root@192.168.1.128
ssh-copy-id -i id_rsa.pub postgres@192.168.1.126
ssh-copy-id -i id_rsa.pub postgres@192.168.1.127
ssh-copy-id -i id_rsa.pub postgres@192.168.1.128
su - postgres
ssh-keygen -t rsa
.ssh/
ssh-copy-id -i id_rsa.pub postgres@192.168.1.126
ssh-copy-id -i id_rsa.pub postgres@192.168.1.127
ssh-copy-id -i id_rsa.pub postgres@192.168.1.128

#CONFIGURE .PGPASS FILE
vi /dados/.pgpass
pgpool2_prim:5432:replication:repl:<repl user password>
pgpool2_stdb1:5432:replication:repl:<repl user passowrd>
pgpool2_stdb2:5432:replication:repl:<repl user passowrd>
pgpool2_prim:5432:postgres:postgres:<postgres user passowrd>
pgpool2_stdb1:5432:postgres:postgres:<postgres user passowrd>
pgpool2_stdb2:5432:postgres:postgres:<postgres user passowrd>
chmod 600 /dados/.pgpass

#REPEAT CONFIGURATION FOR OTHER MACHINES

#PGPOOL II CONFIGURATION FILES
cp -vf /etc/pgpool-II/pgpool.conf.sample-stream /etc/pgpool-II/pgpool.conf
vi /etc/pgpool-II/pgpool.conf
listen_addresses = '*'
sr_check_user = 'pgpool'
sr_check_password = '<pgpool password>'
health_check_period = 5
health_check_timeout = 30
health_check_user = 'pgpool'
health_check_password = '<pgpool password>'
health_check_max_retries = 3
backend_hostname0 = 'pgpool2_prim'
backend_port0 = 5432
backend_weight0 = 1
backend_data_directory0 = '/dados/data'
backend_flag0 = 'ALLOW_TO_FAILOVER'
backend_hostname1 = 'pgpool2_stdb1'
backend_port1 = 5432
backend_weight1 = 1
backend_data_directory1 = '/dados/data'
backend_flag1 = 'ALLOW_TO_FAILOVER'
backend_hostname2 = 'pgpool2_stdb2'
backend_port2 = 5432
backend_weight2 = 1
backend_data_directory2 = '/dados/data'
backend_flag2 = 'ALLOW_TO_FAILOVER'
backend_application_name0 = 'pgpool2_prim'
backend_application_name1 = 'pgpool2_stdb1'
backend_application_name2 = 'pgpool2_stdb2'
failover_command = '/etc/pgpool-II/failover.sh %d %h %p %D %m %H %M %P %r %R %N %S'
follow_master_command = '/etc/pgpool-II/follow_master.sh %d %h %p %D %m %H %M %P %r %R'
recovery_user = 'postgres'
recovery_password = '<postgres password>'
recovery_1st_stage_command = 'recovery_1st_stage'
enable_pool_hba = on
use_watchdog = on
delegate_IP = '192.168.1.129'
if_up_cmd = '/usr/bin/sudo /sbin/ip addr add $_IP_$/24 dev enp0s8 label enp0s8:0'
if_down_cmd = '/usr/bin/sudo /sbin/ip addr del $_IP_$/24 dev enp0s8'
arping_cmd = '/usr/bin/sudo /usr/sbin/arping -U $_IP_$ -w 1 -I enp0s8'
log_destination = 'syslog'
syslog_facility = 'LOCAL1'
########################################################################
#NOTE SPECIFY THE hOSTNAME AND PORT NUMBER OF EACH PGPOOL-II server.   #
#FOR PRIMARY                                                           #
wd_hostname = 'pgpool2_prim'                                           #
wd_port = 9000                                                         #
########################################################################
########################################################################
#NOTE SPECIFY THE hOSTNAME AND PORT NUMBER OF EACH PGPOOL-II server.   #
#FOR STANDBY1                                                          #
wd_hostname = 'pgpool2_stdb1'                                          #
wd_port = 9000                                                         #
########################################################################
########################################################################
#NOTE SPECIFY THE hOSTNAME AND PORT NUMBER OF EACH PGPOOL-II server.   #
#FOR STANDBY2                                                          #
wd_hostname = 'pgpool2_stdb2'                                          #
wd_port = 9000                                                         #
########################################################################
#####################################################################################################################################
#SPECIFY THE HOSTNAME, PGPOOL-II PORT NUMBER, AND WATCHDOG PORT NUMBER OF MONITORED PGPOOL-II SERVERS ON EACH PGPOOL-II SERVER.     #
#FOR PRIMARY                                                                                                                        #
other_pgpool_hostname0 = 'pgpool2_stdb1'                                                                                            #
other_pgpool_port0 = 9999                                                                                                           #
other_wd_port0 = 9000                                                                                                               #
other_pgpool_hostname1 = 'pgpool2_stdb2'                                                                                            #
other_pgpool_port1 = 9999                                                                                                           #
other_wd_port1 = 9000                                                                                                               #
#####################################################################################################################################
#####################################################################################################################################
#SPECIFY THE HOSTNAME, PGPOOL-II PORT NUMBER, AND WATCHDOG PORT NUMBER OF MONITORED PGPOOL-II SERVERS ON EACH PGPOOL-II SERVER.     #
#FOR STANDBY1                                                                                                                       #
other_pgpool_hostname0 = 'pgpool2_prim'                                                                                             #
other_pgpool_port0 = 9999                                                                                                           #
other_wd_port0 = 9000                                                                                                               #
other_pgpool_hostname1 = 'pgpool2_stdb2'                                                                                            #
other_pgpool_port1 = 9999                                                                                                           #
other_wd_port1 = 9000                                                                                                               #
#####################################################################################################################################
#####################################################################################################################################
#SPECIFY THE HOSTNAME, PGPOOL-II PORT NUMBER, AND WATCHDOG PORT NUMBER OF MONITORED PGPOOL-II SERVERS ON EACH PGPOOL-II SERVER.     #
#FOR STANDBY2                                                                                                                       #
other_pgpool_hostname0 = 'pgpool2_prim'                                                                                             #
other_pgpool_port0 = 9999                                                                                                           #
other_wd_port0 = 9000                                                                                                               #
other_pgpool_hostname1 = 'pgpool2_stdb1'                                                                                            #
other_pgpool_port1 = 9999                                                                                                           #
other_wd_port1 = 9000                                                                                                               #
#####################################################################################################################################
#########################################################################################
#SPECIFY THE HOSTNAME AND PORT NUMBER OF DESTINATION FOR SENDING HEARTBEAT SIGNAL       #
#FOR PRIMARY                                                                            #
heartbeat_destination0 = 'pgpool2_stdb1'                                                #
heartbeat_destination_port0 = 9694                                                      #
heartbeat_device0 = ''                                                                  #
heartbeat_destination1 = 'pgpool2_stdb2'                                                #
heartbeat_destination_port1 = 9694                                                      #
heartbeat_device1 = ''                                                                  #
#########################################################################################
#########################################################################################
#SPECIFY THE HOSTNAME AND PORT NUMBER OF DESTINATION FOR SENDING HEARTBEAT SIGNAL       #
#FOR STANDBY1                                                                           #
heartbeat_destination0 = 'pgpool2_prim'                                                 #
heartbeat_destination_port0 = 9694                                                      #
heartbeat_device0 = ''                                                                  #
heartbeat_destination1 = 'pgpool2_stdb2'                                                #
heartbeat_destination_port1 = 9694                                                      #
heartbeat_device1 = ''                                                                  #
#########################################################################################
#########################################################################################
#SPECIFY THE HOSTNAME AND PORT NUMBER OF DESTINATION FOR SENDING HEARTBEAT SIGNAL       #
#FOR STANDBY2                                                                           #
heartbeat_destination0 = 'pgpool2_prim'                                                 #
heartbeat_destination_port0 = 9694                                                      #
heartbeat_device0 = ''                                                                  #
heartbeat_destination1 = 'pgpool2_stdb1'                                                #
heartbeat_destination_port1 = 9694                                                      #
heartbeat_device1 = ''                                                                  #
#########################################################################################
                                                                                                                                    
#FAILOVER CONFIGURATION FILE
cp -vf /etc/pgpool-II/follow_master.sh.sample /etc/pgpool-II/follow_master.sh
cp -vf /etc/pgpool-II/failover.sh.sample /etc/pgpool-II/failover.sh
vi /etc/pgpool-II/failover.sh
#!/bin/bash
# This script is run by failover_command.

set -o xtrace
exec > >(logger -i -p local1.info) 2>&1

# Special values:
#   %d = failed node id
#   %h = failed node hostname
#   %p = failed node port number
#   %D = failed node database cluster path
#   %m = new master node id
#   %H = new master node hostname
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %N = old primary node hostname
#   %S = old primary node port number
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"
OLD_PRIMARY_NODE_HOST="${11}"
OLD_PRIMARY_NODE_PORT="${12}"

PGHOME=/usr/local/pgsql-11.4


logger -i -p local1.info failover.sh: start: failed_node_id=$FAILED_NODE_ID old_primary_node_id=$OLD_PRIMARY_NODE_ID failed_host=$FAILED_NODE_HOST new_master_host=$NEW_MASTER_NODE_HOST

## If there's no master node anymore, skip failover.
if [ $NEW_MASTER_NODE_ID -lt 0 ]; then
    logger -i -p local1.info failover.sh: All nodes are down. Skipping failover.
        exit 0
fi

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info failover.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## If Standby node is down, skip failover.
if [ $FAILED_NODE_ID -ne $OLD_PRIMARY_NODE_ID ]; then
    logger -i -p local1.info failover.sh: Standby node is down. Skipping failover.

    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$OLD_PRIMARY_NODE_HOST -i ~/.ssh/id_rsa_pgpool "
        ${PGHOME}/bin/psql -p $OLD_PRIMARY_NODE_PORT -c \"SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')\"
    "

    if [ $? -ne 0 ]; then
        logger -i -p local1.error failover.sh: drop replication slot "${FAILED_NODE_HOST}" failed
        exit 1
    fi

    exit 0
fi

## Promote Standby node.
logger -i -p local1.info failover.sh: Primary node is down, promote standby node ${NEW_MASTER_NODE_HOST}.

ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
    postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -D ${NEW_MASTER_NODE_PGDATA} -w promote

if [ $? -ne 0 ]; then
    logger -i -p local1.error failover.sh: new_master_host=$NEW_MASTER_NODE_HOST promote failed
    exit 1
fi

logger -i -p local1.info failover.sh: end: new_master_node_id=$NEW_MASTER_NODE_ID started as the primary node
exit 0

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

vi /etc/pgpool-II/follow_master.sh
#!/bin/bash
# This script is run after failover_command to synchronize the Standby with the new Primary.
# First try pg_rewind. If pg_rewind failed, use pg_basebackup.

set -o xtrace
exec > >(logger -i -p local1.info) 2>&1

# Special values:
#   %d = failed node id
#   %h = failed node hostname
#   %p = failed node port number
#   %D = failed node database cluster path
#   %m = new master node id
#   %H = new master node hostname
#   %M = old master node id
#   %P = old primary node id
#   %r = new master port number
#   %R = new master database cluster path
#   %N = old primary node hostname
#   %S = old primary node port number
#   %% = '%' character

FAILED_NODE_ID="$1"
FAILED_NODE_HOST="$2"
FAILED_NODE_PORT="$3"
FAILED_NODE_PGDATA="$4"
NEW_MASTER_NODE_ID="$5"
NEW_MASTER_NODE_HOST="$6"
OLD_MASTER_NODE_ID="$7"
OLD_PRIMARY_NODE_ID="$8"
NEW_MASTER_NODE_PORT="$9"
NEW_MASTER_NODE_PGDATA="${10}"

PGHOME=/usr/local/pgsql-11.4
ARCHIVEDIR=/postgres/wals
REPLUSER=repl
PCP_USER=pgpool
PGPOOL_PATH=/usr/bin
PCP_PORT=9898

logger -i -p local1.info follow_master.sh: start: Standby node ${FAILED_NODE_ID}

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info follow_master.sh: passwrodless SSH to postgres@${NEW_MASTER_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Get PostgreSQL major version
PGVERSION=`${PGHOME}/bin/initdb -V | awk '{print $3}' | sed 's/\..*//' | sed 's/\([0-9]*\)[a-zA-Z].*/\1/'`

if [ $PGVERSION -ge 12 ]; then
RECOVERYCONF=${FAILED_NODE_PGDATA}/myrecovery.conf
else
RECOVERYCONF=${FAILED_NODE_PGDATA}/recovery.conf
fi

## Check the status of Standby
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ${PGHOME}/bin/pg_ctl -w -D ${FAILED_NODE_PGDATA} status


## If Standby is running, synchronize it with the new Primary.
if [ $? -eq 0 ]; then

    logger -i -p local1.info follow_master.sh: pg_rewind for $FAILED_NODE_ID

    # Create replication slot "${FAILED_NODE_HOST}"
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
        ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c \"SELECT pg_create_physical_replication_slot('${FAILED_NODE_HOST}');\"
    "

    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "

        set -o errexit

        ${PGHOME}/bin/pg_ctl -w -m f -D ${FAILED_NODE_PGDATA} stop

        cat > ${RECOVERYCONF} << EOT
primary_conninfo = 'host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT} user=${REPLUSER} application_name=${FAILED_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${FAILED_NODE_HOST}'
EOT

        if [ ${PGVERSION} -ge 12 ]; then
            touch ${FAILED_NODE_PGDATA}/standby.signal
        else
            echo \"standby_mode = 'on'\" >> ${RECOVERYCONF}
        fi

        ${PGHOME}/bin/pg_rewind -D ${FAILED_NODE_PGDATA} --source-server=\"user=postgres host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT}\"

    "

    if [ $? -ne 0 ]; then
        logger -i -p local1.error follow_master.sh: end: pg_rewind failed. Try pg_basebackup.

        ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "

            set -o errexit

            # Execute pg_basebackup
            rm -rf ${FAILED_NODE_PGDATA}
            rm -rf ${ARCHIVEDIR}/*
            ${PGHOME}/bin/pg_basebackup -h ${NEW_MASTER_NODE_HOST} -U $REPLUSER -p ${NEW_MASTER_NODE_PORT} -D ${FAILED_NODE_PGDATA} -X stream

            if [ ${PGVERSION} -ge 12 ]; then
                sed -i -e \"\\\$ainclude_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'\" \
                       -e \"/^include_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'/d\" ${FAILED_NODE_PGDATA}/postgresql.conf
            fi

            cat > ${RECOVERYCONF} << EOT
primary_conninfo = 'host=${NEW_MASTER_NODE_HOST} port=${NEW_MASTER_NODE_PORT} user=${REPLUSER} application_name=${FAILED_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${NEW_MASTER_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${FAILED_NODE_HOST}'
EOT

            if [ ${PGVERSION} -ge 12 ]; then
                touch ${FAILED_NODE_PGDATA}/standby.signal
            else
                echo \"standby_mode = 'on'\" >> ${RECOVERYCONF}
            fi
        "

        if [ $? -ne 0 ]; then
            # drop replication slot
            ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool "
                ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c \"SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')\"
            "

            logger -i -p local1.error follow_master.sh: end: pg_basebackup failed
            exit 1
        fi
    fi

    # start Standby node on ${FAILED_NODE_HOST}
    ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            postgres@${FAILED_NODE_HOST} -i ~/.ssh/id_rsa_pgpool $PGHOME/bin/pg_ctl -l /dev/null -w -D ${FAILED_NODE_PGDATA} start

    # If start Standby successfully, attach this node
    if [ $? -eq 0 ]; then

        # Run pcp_attact_node to attach Standby node to Pgpool-II.
        ${PGPOOL_PATH}/pcp_attach_node -w -h localhost -U $PCP_USER -p ${PCP_PORT} -n ${FAILED_NODE_ID}

        if [ $? -ne 0 ]; then
                logger -i -p local1.error follow_master.sh: end: pcp_attach_node failed
                exit 1
        fi

    # If start Standby failed, drop replication slot "${FAILED_NODE_HOST}"
    else

        ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${NEW_MASTER_NODE_HOST} -i ~/.ssh/id_rsa_pgpool \
        ${PGHOME}/bin/psql -p ${NEW_MASTER_NODE_PORT} -c "SELECT pg_drop_replication_slot('${FAILED_NODE_HOST}')"

        logger -i -p local1.error follow_master.sh: end: follow master command failed
        exit 1
    fi

else
    logger -i -p local1.info follow_master.sh: failed_nod_id=${FAILED_NODE_ID} is not running. skipping follow master command
    exit 0
fi

logger -i -p local1.info follow_master.sh: end: follow master command complete
exit 0

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

#ONLINE RECOVERY CONFIGURATION FILES
cp /etc/pgpool-II/recovery_1st_stage.sample /dados/data/recovery_1st_stage
cp /etc/pgpool-II/pgpool_remote_start.sample /dados/data/pgpool_remote_start
chown postgres:postgres /dados/data/{recovery_1st_stage,pgpool_remote_start}

vi /dados/data/recovery_1st_stage
#!/bin/bash
# This script is executed by "recovery_1st_stage" to recovery a Standby node.

set -o xtrace
exec > >(logger -i -p local1.info) 2>&1

PRIMARY_NODE_PGDATA="$1"
DEST_NODE_HOST="$2"
DEST_NODE_PGDATA="$3"
PRIMARY_NODE_PORT="$4"
DEST_NODE_ID="$5"
DEST_NODE_PORT="$6"

PRIMARY_NODE_HOST=$(hostname)
PGHOME=/usr/local/pgsql-11.4
ARCHIVEDIR=/postgres/wals
REPLUSER=repl

logger -i -p local1.info recovery_1st_stage: start: pg_basebackup for Standby node $DEST_NODE_ID

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${DEST_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info recovery_1st_stage: passwrodless SSH to postgres@${DEST_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Get PostgreSQL major version
PGVERSION=`${PGHOME}/bin/initdb -V | awk '{print $3}' | sed 's/\..*//' | sed 's/\([0-9]*\)[a-zA-Z].*/\1/'`
if [ $PGVERSION -ge 12 ]; then
    RECOVERYCONF=${DEST_NODE_PGDATA}/myrecovery.conf
else
    RECOVERYCONF=${DEST_NODE_PGDATA}/recovery.conf
fi

## Create replication slot "${DEST_NODE_HOST}"
${PGHOME}/bin/psql -p ${PRIMARY_NODE_PORT} << EOQ
SELECT pg_create_physical_replication_slot('${DEST_NODE_HOST}');
EOQ

## Execute pg_basebackup to recovery Standby node
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST -i ~/.ssh/id_rsa_pgpool "

    set -o errexit

    rm -rf $DEST_NODE_PGDATA
    rm -rf $ARCHIVEDIR/*

    ${PGHOME}/bin/pg_basebackup -h $PRIMARY_NODE_HOST -U $REPLUSER -p $PRIMARY_NODE_PORT -D $DEST_NODE_PGDATA -X stream

    if [ ${PGVERSION} -ge 12 ]; then
        sed -i -e \"\\\$ainclude_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'\" \
               -e \"/^include_if_exists = '$(echo ${RECOVERYCONF} | sed -e 's/\//\\\//g')'/d\" ${DEST_NODE_PGDATA}/postgresql.conf
    fi

    cat > ${RECOVERYCONF} << EOT
primary_conninfo = 'host=${PRIMARY_NODE_HOST} port=${PRIMARY_NODE_PORT} user=${REPLUSER} application_name=${DEST_NODE_HOST} passfile=''/var/lib/pgsql/.pgpass'''
recovery_target_timeline = 'latest'
restore_command = 'scp ${PRIMARY_NODE_HOST}:${ARCHIVEDIR}/%f %p'
primary_slot_name = '${DEST_NODE_HOST}'
EOT

    if [ ${PGVERSION} -ge 12 ]; then
        touch ${DEST_NODE_PGDATA}/standby.signal
    else
        echo \"standby_mode = 'on'\" >> ${RECOVERYCONF}
    fi

    sed -i \"s/#*port = .*/port = ${DEST_NODE_PORT}/\" ${DEST_NODE_PGDATA}/postgresql.conf
"

if [ $? -ne 0 ]; then

    ${PGHOME}/bin/psql -p ${PRIMARY_NODE_PORT} << EOQ
SELECT pg_drop_replication_slot('${DEST_NODE_HOST}');
EOQ

    logger -i -p local1.error recovery_1st_stage: end: pg_basebackup failed. online recovery failed
    exit 1
fi

logger -i -p local1.info recovery_1st_stage: end: recovery_1st_stage complete
exit 0

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

vi /dados/data/pgpool_remote_start

#!/bin/bash
# This script is run after recovery_1st_stage to start Standby node.

set -o xtrace
exec > >(logger -i -p local1.info) 2>&1

DEST_NODE_HOST="$1"
DEST_NODE_PGDATA="$2"

PGHOME=/usr/local/pgsql-11.4

logger -i -p local1.info pgpool_remote_start: start: remote start Standby node $DEST_NODE_HOST

## Test passwrodless SSH
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@${DEST_NODE_HOST} -i ~/.ssh/id_rsa_pgpool ls /tmp > /dev/null

if [ $? -ne 0 ]; then
    logger -i -p local1.info pgpool_remote_start: passwrodless SSH to postgres@${DEST_NODE_HOST} failed. Please setup passwrodless SSH.
    exit 1
fi

## Start Standby node
ssh -T -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null postgres@$DEST_NODE_HOST -i ~/.ssh/id_rsa_pgpool "
    $PGHOME/bin/pg_ctl -l /dev/null -w -D $DEST_NODE_PGDATA start
"

if [ $? -ne 0 ]; then
    logger -i -p local1.error pgpool_remote_start: $DEST_NODE_HOST PostgreSQL start failed.
    exit 1
fi

logger -i -p local1.info pgpool_remote_start: end: $DEST_NODE_HOST PostgreSQL started successfully.
exit 0

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

cp -vf /usr/pgsql-11/share/extension/* /usr/local/pgsql-11.4/share/extension/
cp -Rvf /usr/pgsql-11/lib/* /usr/local/pgsql-11.4/lib/
su - postgres
/usr/local/pgsql-11.4/bin/psql -U postgres -c "CREATE EXTENSION pgpool_recovery"
CREATE EXTENSION
exit

#CONFIGURE POOL_PGHBA FILE
vi /etc/pgpool-II/pool_hba.conf
host    all         pgpool      192.168.1.0/24        scram-sha-256
host    all         postgres    192.168.1.0/24        scram-sha-256

#CLIENTE AUTHENTICATION CONFIGURATION
su - postgres
echo '<pgpool password>' > ~/.pgpoolkey
chmod 600 ~/.pgpoolkey
pg_enc -m -k ~/.pgpoolkey -u pgpool -p
db password: [pgpool user's password]
pg_enc -m -k ~/.pgpoolkey -u postgres -p
db password: [postgres user's passowrd]
cat /etc/pgpool-II/pool_passwd
pgpool:AESheq2ZMZjynddMWk5sKP/Rw==
postgres:AESHs/pWL5rtXy2IwuzroHfqg==
exit

#WATCHDOG CONFIGURATION
visudo
postgres ALL=NOPASSWD: /sbin/ip
postgres ALL=NOPASSWD: /usr/sbin/arping

#CONFIGURE SERVICE PGPOOL
vi /etc/sysconfig/pgpool
OPTS=" -d -n"
#OPTS=" -n"

#LOG PGPOOL CONFIGURATION
mkdir /var/log/pgpool-II
touch /var/log/pgpool-II/pgpool.log
vi /etc/rsyslog.conf
*.info;mail.none;authpriv.none;cron.none;LOCAL1.none    /var/log/messages
LOCAL1.*                                                /var/log/pgpool-II/pgpool.log
vi /etc/logrotate.d/syslog
/var/log/pgpool-II/pgpool.log
/var/log/cron
/var/log/maillog
/var/log/messages
/var/log/secure
/var/log/spooler
{
    missingok
    sharedscripts
    postrotate
        /bin/kill -HUP `cat /var/run/syslogd.pid 2> /dev/null` 2> /dev/null || true
    endscript
}

systemctl restart rsyslog

#PCP COMMAND CONFIGURATION
echo 'pgpool:'`pg_md5 PCP passowrd` >> /etc/pgpool-II/pcp.conf
echo 'localhost:9898:pgpool:pgpool' > ~/.pcppass
chmod 600 ~/.pcppass

#START SERVICE 
systemctl start pgpool.service

#STOP SERVICE
systemctl stop pgpool.service

#HOW TO USE
#FIRST START PGPOOL ON PRIMARY
systemctl start pgpool.service

#SET UP POSTGRESQL STANDBY SERVER
pcp_recovery_node -h 192.168.1.129 -p 9898 -U pgpool -n 1
 Password: 
 pcp_recovery_node -- Command Successful

pcp_recovery_node -h 192.168.1.129 -p 9898 -U pgpool -n 2
 Password: 
 pcp_recovery_node -- Command Successful

#VERIFY THAT SERVER2 AND SERVER3 ARE STARTED AS POSTGRESQL STANDBY SERVER
psql -h 192.168.137.150 -p 9999 -U pgpool postgres -c "show pool_nodes"
  Password for user pgpool
  node_id | hostname      | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state |
  --------+---------------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+
  0       | pgpool2_prim  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        |
  1       | pgpool2_stdb1 | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  |
  2       | pgpool2_stdb2 | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  |

#SWITCHING ACTIVE/STANDBY WATCHDOG
#CONFIRM THE WATCHDOG STATUS BY USING PCP_WATCHDOG_INFO. THE PGPOOL-II SERVER WHICH IS STARTED FIRST RUN AS MASTER.
pcp_watchdog_info -h 192.168.1.129 -p 9898 -U pgpool
Password: 
    3 YES server1:9999 Linux pgpool2_prim pgpool2_prim

    pgpool2_prim:9999 Linux pgpool2_prim pgpool2_prim 9999 9000 4 MASTER  #The Pgpool-II server started first becames "MASTER".
    pgpool2_stdb1:9999 Linux pgpool2_stdb1 pgpool2_stdb1 9999 9000 7 STANDBY #run as standby
    pgpool2_stdb2:9999 Linux pgpool2_stdb2 pgpool2_stdb2 9999 9000 7 STANDBY #run as standby
    
-- RUN SWITCH
-- STOP PGPOOL SERVICE PRIMARY
#STOP ACTIVE SERVER SERVER1, THEN SERVER2 OR SERVER3 WILL BE PROMOTED TO ACTIVE SERVER. 
TO STOP SERVER1, WE CAN STOP PGPOOL-II SERVICE OR SHUTDOWN THE WHOLE SYSTEM. HERE, WE STOP PGPOOL-II SERVICE.
systemctl stop pgpool.service
pcp_watchdog_info -p 9898 -h 192.168.1.129 -U pgpool
    Password: 
    3 YES pgpool2_stdb1:9999 Linux pgpool2_stdb1 pgpool2_stdb1

    pgpool2_stdb1:9999 Linux pgpool2_stdb1 pgpool2_stdb1 9999 9000 4 MASTER     #pgpool2_stdb1 is promoted to MASTER
    pgpool2_prim:9999 Linux pgpool2_prim pgpool2_prim 9999 9000 10 SHUTDOWN  #pgpool2_prim is stopped
    pgpool2_stdb2:9999 Linux pgpool2_stdb2 pgpool2_stdb2 9999 9000 7 STANDBY    #pgpool2_stdb2 runs as STANDBY
    
#START PGPOOL-II (SERVER1) WHICH WE HAVE STOPPED AGAIN, AND VERIFY THAT SERVER1 RUNS AS A STANDBY.
-- START PGPOOL SERVICE PRIMARY
systemctl start pgpool.service

    pcp_watchdog_info -p 9898 -h 192.168.1.129 -U pgpool
    Password: 
    3 YES pgpool2_stdb1:9999 Linux pgpool2_stdb1 pgpool2_stdb1

    pgpool2_stdb1:9999 Linux pgpool2_stdb1 pgpool2_stdb1 9999 9000 4 MASTER
    pgpool2_prim:9999 Linux pgpool2_prim pgpool2_prim 9999 9000 10 STANDBY
    pgpool2_stdb2:9999 Linux pgpool2_stdb2 pgpool2_stdb2 9999 9000 7 STANDBY
   
#FAILOVER
#FIRST, USE PSQL TO CONNECT TO POSTGRESQL VIA VIRTUAL IP, AND VERIFY THE BACKEND INFORMATION.
psql -h 192.168.1.129 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
  node_id | hostname      | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state |
  --------+---------------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+
  0       | pgpool2_prim  | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        |
  1       | pgpool2_stdb1 | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  |
  2       | pgpool2_stdb2 | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  |

#NEXT, STOP PRIMARY POSTGRESQL SERVER SERVER1, AND VERIFY AUTOMATIC FAILOVER.
su - postgre
/usr/local/pgsql-11.4/bin/pg_ctl -D /dados/data -m immediate stop
#AFTER STOPPING POSTGRESQL ON SERVER1, FAILOVER OCCURS AND POSTGRESQL ON SERVER2 BECOMES NEW PRIMARY DB.
psql -h 192.168.1.129 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
  node_id | hostname      | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state |
  --------+---------------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+
  0       | pgpool2_prim  | 5432 | down   | 0.333333  | standby | 0          | false             | 0                 |                   |                        |
  1       | pgpool2_stdb1 | 5432 | up     | 0.333333  | primary | 0          | true              | 0                 |                   |                        |
  2       | pgpool2_stdb2 | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  |

#SERVER3 IS RUNNING AS STANDBY OF NEW PRIMARY SERVER2.
    psql -h pgpool2_stdb2 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
    pg_is_in_recovery 
    -------------------
    t

    psql -h pgpool2_stdb1 -p 5432 -U pgpool postgres -c "select pg_is_in_recovery()"
    pg_is_in_recovery 
    -------------------
    f

    psql -h pgpool2_stdb1 -p 5432 -U pgpool postgres -c "select * from pg_stat_replication" -x
    -[ RECORD 1 ]----+------------------------------
    pid              | 11059
    usesysid         | 16392
    usename          | repl
    application_name | pgpool2_stdb2
    client_addr      | 192.168.1.128
    client_hostname  | 
    client_port      | 48694
    backend_xmin     | 
    state            | streaming
    sent_lsn         | 0/75000148
    write_lsn        | 0/75000148
    flush_lsn        | 0/75000148
    replay_lsn       | 0/75000148
    write_lag        | 
    flush_lag        | 
    replay_lag       | 
    sync_priority    | 0
    sync_state       | async

#ONLINE RECOVERY
HERE, WE USE PGPOOL-II ONLINE RECOVERY FUNCTIONALITY TO RESTORE pgpool2_prim (OLD PRIMARY SERVER) AS A STANDBY. 
BEFORE RESTORING THE OLD PRIMARY SERVER, PLEASE ENSURE THAT RECOVERY_1ST_STAGE AND PGPOOL_REMOTE_START SCRIPTS EXIST IN DATABASE CLUSTER DIRECTORY OF CURRENT PRIMARY SERVER pgpool2_stdb1.

pcp_recovery_node -h 192.168.1.129 -p 9898 -U pgpool -n 0
    Password: 
    pcp_recovery_node -- Command Successful

#THEN VERIFY THAT SERVER1 IS STARTED AS A STANDBY.
psql -h 192.168.1.129 -p 9999 -U pgpool postgres -c "show pool_nodes"
    Password for user pgpool:
  node_id | hostname      | port | status | lb_weight |  role   | select_cnt | load_balance_node | replication_delay | replication_state | replication_sync_state |
  --------+---------------+------+--------+-----------+---------+------------+-------------------+-------------------+-------------------+------------------------+
  0       | pgpool2_prim  | 5432 | up     | 0.333333  | standby | 0          | false             | 0                 | streaming         | async                  |
  1       | pgpool2_stdb1 | 5432 | up     | 0.333333  | primary | 0          | false             | 0                 |                   |                        |
  2       | pgpool2_stdb2 | 5432 | up     | 0.333333  | standby | 0          | true              | 0                 | streaming         | async                  |

#END

















